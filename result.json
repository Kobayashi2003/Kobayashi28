{"traceEvents": [{"ph": "M", "pid": 32672, "tid": 32672, "name": "process_name", "args": {"name": "MainProcess"}}, {"ph": "M", "pid": 32672, "tid": 22768, "name": "thread_name", "args": {"name": "MainThread"}}, {"pid": 32672, "tid": 22768, "ts": 1072608737603.399, "ph": "X", "cat": "fee", "dur": 4.3, "name": "_asyncio._get_running_loop"}, {"pid": 32672, "tid": 22768, "ts": 1072608737610.9, "ph": "X", "cat": "fee", "dur": 0.1, "name": "builtins.isinstance"}, {"pid": 32672, "tid": 22768, "ts": 1072608737611.2, "ph": "X", "cat": "fee", "dur": 0.1, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072608737611.8, "ph": "X", "cat": "fee", "dur": 0.199, "name": "set.add"}, {"pid": 32672, "tid": 22768, "ts": 1072608737609.0, "ph": "X", "cat": "fee", "dur": 3.1, "name": "iscoroutine (D:\\Program\\anaconda3\\lib\\asyncio\\coroutines.py:177)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737616.1, "ph": "X", "cat": "fee", "dur": 0.299, "name": "str.rpartition"}, {"pid": 32672, "tid": 22768, "ts": 1072608737615.6, "ph": "X", "cat": "fee", "dur": 0.999, "name": "parent (<frozen importlib._bootstrap>:404)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737617.7, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.isinstance"}, {"pid": 32672, "tid": 22768, "ts": 1072608737618.099, "ph": "X", "cat": "fee", "dur": 0.4, "name": "builtins.hasattr"}, {"pid": 32672, "tid": 22768, "ts": 1072608737617.4, "ph": "X", "cat": "fee", "dur": 1.399, "name": "_handle_fromlist (<frozen importlib._bootstrap>:1053)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737620.2, "ph": "X", "cat": "fee", "dur": 5.299, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:642)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737625.9, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_thread.lock.__exit__"}, {"pid": 32672, "tid": 22768, "ts": 1072608737614.0, "ph": "X", "cat": "fee", "dur": 12.2, "name": "_init_event_loop_policy (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:743)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737613.3, "ph": "X", "cat": "fee", "dur": 13.0, "name": "get_event_loop_policy (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:751)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737631.7, "ph": "X", "cat": "fee", "dur": 3.599, "name": "_overlapped.CreateIoCompletionPort"}, {"pid": 32672, "tid": 22768, "ts": 1072608737636.4, "ph": "X", "cat": "fee", "dur": 2.099, "name": "__init__ (D:\\Program\\anaconda3\\lib\\_weakrefset.py:37)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737639.599, "ph": "X", "cat": "fee", "dur": 0.7, "name": "__init__ (D:\\Program\\anaconda3\\lib\\_weakrefset.py:37)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737630.3, "ph": "X", "cat": "fee", "dur": 10.299, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:413)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737646.9, "ph": "X", "cat": "fee", "dur": 6.399, "name": "time.get_clock_info"}, {"pid": 32672, "tid": 22768, "ts": 1072608737657.9, "ph": "X", "cat": "fee", "dur": 0.099, "name": "builtins.isinstance"}, {"pid": 32672, "tid": 22768, "ts": 1072608737657.6, "ph": "X", "cat": "fee", "dur": 0.5, "name": "check_str (D:\\Program\\anaconda3\\lib\\os.py:741)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737658.3, "ph": "X", "cat": "fee", "dur": 0.2, "name": "str.upper"}, {"pid": 32672, "tid": 22768, "ts": 1072608737657.399, "ph": "X", "cat": "fee", "dur": 1.12, "name": "encodekey (D:\\Program\\anaconda3\\lib\\os.py:747)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737656.9, "ph": "X", "cat": "fee", "dur": 3.4, "name": "__getitem__ (D:\\Program\\anaconda3\\lib\\os.py:675)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737656.4, "ph": "X", "cat": "fee", "dur": 4.699, "name": "get (D:\\Program\\anaconda3\\lib\\_collections_abc.py:821)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737654.999, "ph": "X", "cat": "fee", "dur": 6.5, "name": "_is_debug_mode (D:\\Program\\anaconda3\\lib\\asyncio\\coroutines.py:18)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737662.999, "ph": "X", "cat": "fee", "dur": 0.3, "name": "is_running (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:692)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737661.8, "ph": "X", "cat": "fee", "dur": 1.7, "name": "set_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1927)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737665.5, "ph": "X", "cat": "fee", "dur": 1.099, "name": "__init__ (D:\\Program\\anaconda3\\lib\\_weakrefset.py:37)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737643.6, "ph": "X", "cat": "fee", "dur": 24.7, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:391)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737674.5, "ph": "X", "cat": "fee", "dur": 0.299, "name": "_thread.RLock.acquire"}, {"pid": 32672, "tid": 22768, "ts": 1072608737673.399, "ph": "X", "cat": "fee", "dur": 1.5, "name": "_acquireLock (D:\\Program\\anaconda3\\lib\\logging\\__init__.py:219)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737676.3, "ph": "X", "cat": "fee", "dur": 1.199, "name": "disable (D:\\Program\\anaconda3\\lib\\logging\\__init__.py:1307)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737678.0, "ph": "X", "cat": "fee", "dur": 1.9, "name": "getEffectiveLevel (D:\\Program\\anaconda3\\lib\\logging\\__init__.py:1710)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737680.699, "ph": "X", "cat": "fee", "dur": 0.6, "name": "_thread.RLock.release"}, {"pid": 32672, "tid": 22768, "ts": 1072608737680.5, "ph": "X", "cat": "fee", "dur": 0.899, "name": "_releaseLock (D:\\Program\\anaconda3\\lib\\logging\\__init__.py:228)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737671.1, "ph": "X", "cat": "fee", "dur": 10.6, "name": "isEnabledFor (D:\\Program\\anaconda3\\lib\\logging\\__init__.py:1724)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737670.299, "ph": "X", "cat": "fee", "dur": 11.5, "name": "debug (D:\\Program\\anaconda3\\lib\\logging\\__init__.py:1455)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737684.999, "ph": "X", "cat": "fee", "dur": 0.3, "name": "set_loop (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:434)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737692.3, "ph": "X", "cat": "fee", "dur": 37.899, "name": "__init__ (D:\\Program\\anaconda3\\lib\\socket.py:220)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737731.2, "ph": "X", "cat": "fee", "dur": 110.8, "name": "socket.bind"}, {"pid": 32672, "tid": 22768, "ts": 1072608737843.4, "ph": "X", "cat": "fee", "dur": 51.299, "name": "socket.listen"}, {"pid": 32672, "tid": 22768, "ts": 1072608737895.3, "ph": "X", "cat": "fee", "dur": 8.1, "name": "socket.getsockname"}, {"pid": 32672, "tid": 22768, "ts": 1072608737905.4, "ph": "X", "cat": "fee", "dur": 18.699, "name": "__init__ (D:\\Program\\anaconda3\\lib\\socket.py:220)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737924.7, "ph": "X", "cat": "fee", "dur": 2.5, "name": "socket.setblocking"}, {"pid": 32672, "tid": 22768, "ts": 1072608737928.199, "ph": "X", "cat": "fee", "dur": 267.6, "name": "socket.connect"}, {"pid": 32672, "tid": 22768, "ts": 1072608738202.3, "ph": "X", "cat": "fee", "dur": 3.0, "name": "socket.setblocking"}, {"pid": 32672, "tid": 22768, "ts": 1072608738207.8, "ph": "X", "cat": "fee", "dur": 32.3, "name": "socket._accept"}, {"pid": 32672, "tid": 22768, "ts": 1072608738247.2, "ph": "X", "cat": "fee", "dur": 1.1, "name": "__new__ (D:\\Program\\anaconda3\\lib\\enum.py:678)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738245.699, "ph": "X", "cat": "fee", "dur": 2.9, "name": "__call__ (D:\\Program\\anaconda3\\lib\\enum.py:359)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738244.7, "ph": "X", "cat": "fee", "dur": 4.2, "name": "_intenum_converter (D:\\Program\\anaconda3\\lib\\socket.py:99)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738241.399, "ph": "X", "cat": "fee", "dur": 7.7, "name": "family (D:\\Program\\anaconda3\\lib\\socket.py:514)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738250.7, "ph": "X", "cat": "fee", "dur": 0.4, "name": "__new__ (D:\\Program\\anaconda3\\lib\\enum.py:678)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738250.1, "ph": "X", "cat": "fee", "dur": 1.099, "name": "__call__ (D:\\Program\\anaconda3\\lib\\enum.py:359)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738249.9, "ph": "X", "cat": "fee", "dur": 1.319, "name": "_intenum_converter (D:\\Program\\anaconda3\\lib\\socket.py:99)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738249.5, "ph": "X", "cat": "fee", "dur": 1.9, "name": "type (D:\\Program\\anaconda3\\lib\\socket.py:520)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738252.6, "ph": "X", "cat": "fee", "dur": 5.2, "name": "__init__ (D:\\Program\\anaconda3\\lib\\socket.py:220)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738258.5, "ph": "X", "cat": "fee", "dur": 1.599, "name": "_socket.getdefaulttimeout"}, {"pid": 32672, "tid": 22768, "ts": 1072608738260.499, "ph": "X", "cat": "fee", "dur": 0.02, "name": "socket.gettimeout"}, {"pid": 32672, "tid": 22768, "ts": 1072608738207.2, "ph": "X", "cat": "fee", "dur": 53.5, "name": "accept (D:\\Program\\anaconda3\\lib\\socket.py:286)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738264.099, "ph": "X", "cat": "fee", "dur": 6.3, "name": "socket.close"}, {"pid": 32672, "tid": 22768, "ts": 1072608738263.499, "ph": "X", "cat": "fee", "dur": 7.0, "name": "_real_close (D:\\Program\\anaconda3\\lib\\socket.py:494)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738262.5, "ph": "X", "cat": "fee", "dur": 8.2, "name": "close (D:\\Program\\anaconda3\\lib\\socket.py:498)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737687.0, "ph": "X", "cat": "fee", "dur": 583.899, "name": "socketpair (D:\\Program\\anaconda3\\lib\\socket.py:615)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738274.8, "ph": "X", "cat": "fee", "dur": 0.8, "name": "socket.setblocking"}, {"pid": 32672, "tid": 22768, "ts": 1072608738275.799, "ph": "X", "cat": "fee", "dur": 0.6, "name": "socket.setblocking"}, {"pid": 32672, "tid": 22768, "ts": 1072608737685.699, "ph": "X", "cat": "fee", "dur": 591.4, "name": "_make_self_pipe (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:765)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738279.3, "ph": "X", "cat": "fee", "dur": 0.199, "name": "_thread.get_ident"}, {"pid": 32672, "tid": 22768, "ts": 1072608738278.299, "ph": "X", "cat": "fee", "dur": 1.6, "name": "current_thread (D:\\Program\\anaconda3\\lib\\threading.py:1430)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738280.6, "ph": "X", "cat": "fee", "dur": 0.299, "name": "main_thread (D:\\Program\\anaconda3\\lib\\threading.py:1574)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738282.3, "ph": "X", "cat": "fee", "dur": 0.1, "name": "socket.fileno"}, {"pid": 32672, "tid": 22768, "ts": 1072608738282.5, "ph": "X", "cat": "fee", "dur": 4.9, "name": "_signal.set_wakeup_fd"}, {"pid": 32672, "tid": 22768, "ts": 1072608737641.9, "ph": "X", "cat": "fee", "dur": 645.699, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:628)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737628.2, "ph": "X", "cat": "fee", "dur": 659.899, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:312)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737626.699, "ph": "X", "cat": "fee", "dur": 661.7, "name": "new_event_loop (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:682)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737612.8, "ph": "X", "cat": "fee", "dur": 676.1, "name": "new_event_loop (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:796)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738291.4, "ph": "X", "cat": "fee", "dur": 0.2, "name": "get_event_loop_policy (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:751)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738294.7, "ph": "X", "cat": "fee", "dur": 0.299, "name": "builtins.isinstance"}, {"pid": 32672, "tid": 22768, "ts": 1072608738292.5, "ph": "X", "cat": "fee", "dur": 2.9, "name": "set_event_loop (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:676)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738290.9, "ph": "X", "cat": "fee", "dur": 4.599, "name": "set_event_loop (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:791)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738296.9, "ph": "X", "cat": "fee", "dur": 0.399, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738298.1, "ph": "X", "cat": "fee", "dur": 0.2, "name": "is_running (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:692)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738298.6, "ph": "X", "cat": "fee", "dur": 1.1, "name": "_asyncio._get_running_loop"}, {"pid": 32672, "tid": 22768, "ts": 1072608738297.8, "ph": "X", "cat": "fee", "dur": 2.099, "name": "_check_running (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:582)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738301.9, "ph": "X", "cat": "fee", "dur": 1.499, "name": "builtins.hasattr"}, {"pid": 32672, "tid": 22768, "ts": 1072608738301.3, "ph": "X", "cat": "fee", "dur": 2.199, "name": "isfuture (D:\\Program\\anaconda3\\lib\\asyncio\\base_futures.py:14)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738305.7, "ph": "X", "cat": "fee", "dur": 0.3, "name": "builtins.hasattr"}, {"pid": 32672, "tid": 22768, "ts": 1072608738305.5, "ph": "X", "cat": "fee", "dur": 0.6, "name": "isfuture (D:\\Program\\anaconda3\\lib\\asyncio\\base_futures.py:14)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738306.9, "ph": "X", "cat": "fee", "dur": 0.6, "name": "iscoroutine (D:\\Program\\anaconda3\\lib\\asyncio\\coroutines.py:177)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738308.6, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738315.6, "ph": "X", "cat": "fee", "dur": 0.099, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738321.6, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738324.899, "ph": "X", "cat": "fee", "dur": 0.02, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738323.4, "ph": "X", "cat": "fee", "dur": 3.0, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:31)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738327.5, "ph": "X", "cat": "fee", "dur": 0.1, "name": "collections.deque.append"}, {"pid": 32672, "tid": 22768, "ts": 1072608738322.1, "ph": "X", "cat": "fee", "dur": 5.7, "name": "_call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:772)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738321.199, "ph": "X", "cat": "fee", "dur": 6.8, "name": "call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:743)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738331.3, "ph": "X", "cat": "fee", "dur": 0.1, "name": "set.add"}, {"pid": 32672, "tid": 22768, "ts": 1072608738330.1, "ph": "X", "cat": "fee", "dur": 1.4, "name": "add (D:\\Program\\anaconda3\\lib\\_weakrefset.py:86)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738308.299, "ph": "X", "cat": "fee", "dur": 24.7, "name": "create_task (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:431)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738305.2, "ph": "X", "cat": "fee", "dur": 28.499, "name": "_ensure_future (D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py:618)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738304.8, "ph": "X", "cat": "fee", "dur": 29.0, "name": "ensure_future (D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py:610)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738334.6, "ph": "X", "cat": "fee", "dur": 0.299, "name": "_asyncio.Task.add_done_callback"}, {"pid": 32672, "tid": 22768, "ts": 1072608738336.8, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738338.0, "ph": "X", "cat": "fee", "dur": 0.3, "name": "_contextvars.copy_context"}, {"pid": 32672, "tid": 22768, "ts": 1072608738338.8, "ph": "X", "cat": "fee", "dur": 0.1, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738337.4, "ph": "X", "cat": "fee", "dur": 1.6, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:31)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738339.3, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.append"}, {"pid": 32672, "tid": 22768, "ts": 1072608738337.1, "ph": "X", "cat": "fee", "dur": 2.299, "name": "_call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:772)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738336.599, "ph": "X", "cat": "fee", "dur": 2.9, "name": "call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:743)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738340.7, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738341.1, "ph": "X", "cat": "fee", "dur": 0.02, "name": "is_running (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:692)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738341.3, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio._get_running_loop"}, {"pid": 32672, "tid": 22768, "ts": 1072608738340.899, "ph": "X", "cat": "fee", "dur": 0.6, "name": "_check_running (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:582)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738342.0, "ph": "X", "cat": "fee", "dur": 0.6, "name": "_set_coroutine_origin_tracking (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1909)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738343.0, "ph": "X", "cat": "fee", "dur": 0.7, "name": "sys.get_asyncgen_hooks"}, {"pid": 32672, "tid": 22768, "ts": 1072608738343.9, "ph": "X", "cat": "fee", "dur": 0.099, "name": "_thread.get_ident"}, {"pid": 32672, "tid": 22768, "ts": 1072608738345.3, "ph": "X", "cat": "fee", "dur": 0.9, "name": "sys.set_asyncgen_hooks"}, {"pid": 32672, "tid": 22768, "ts": 1072608738346.399, "ph": "X", "cat": "fee", "dur": 0.3, "name": "_asyncio._set_running_loop"}, {"pid": 32672, "tid": 22768, "ts": 1072608738347.6, "ph": "X", "cat": "fee", "dur": 0.099, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072608738352.3, "ph": "X", "cat": "fee", "dur": 1.499, "name": "math.ceil"}, {"pid": 32672, "tid": 22768, "ts": 1072608738354.8, "ph": "X", "cat": "fee", "dur": 1.7, "name": "_overlapped.GetQueuedCompletionStatus"}, {"pid": 32672, "tid": 22768, "ts": 1072608738357.8, "ph": "X", "cat": "fee", "dur": 0.1, "name": "list.clear"}, {"pid": 32672, "tid": 22768, "ts": 1072608738350.8, "ph": "X", "cat": "fee", "dur": 7.2, "name": "_poll (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:779)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738349.3, "ph": "X", "cat": "fee", "dur": 9.2, "name": "select (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:437)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738359.1, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_process_events (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:861)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738360.4, "ph": "X", "cat": "fee", "dur": 1.699, "name": "time.monotonic"}, {"pid": 32672, "tid": 22768, "ts": 1072608738359.6, "ph": "X", "cat": "fee", "dur": 2.6, "name": "time (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:696)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738362.8, "ph": "X", "cat": "fee", "dur": 0.3, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072608738364.299, "ph": "X", "cat": "fee", "dur": 0.1, "name": "collections.deque.popleft"}, {"pid": 32672, "tid": 22768, "ts": 1072608738368.2, "ph": "X", "cat": "fee", "dur": 20.3, "name": "time.strftime"}, {"pid": 32672, "tid": 22768, "ts": 1072608738389.0, "ph": "X", "cat": "fee", "dur": 143.9, "name": "builtins.print"}, {"pid": 32672, "tid": 22768, "ts": 1072608738539.199, "ph": "X", "cat": "fee", "dur": 1.1, "name": "_asyncio.get_running_loop"}, {"pid": 32672, "tid": 22768, "ts": 1072608738547.799, "ph": "X", "cat": "fee", "dur": 0.4, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738543.9, "ph": "X", "cat": "fee", "dur": 4.8, "name": "create_future (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:427)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738551.4, "ph": "X", "cat": "fee", "dur": 0.399, "name": "time.monotonic"}, {"pid": 32672, "tid": 22768, "ts": 1072608738550.7, "ph": "X", "cat": "fee", "dur": 1.2, "name": "time (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:696)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738553.999, "ph": "X", "cat": "fee", "dur": 0.4, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738559.3, "ph": "X", "cat": "fee", "dur": 0.5, "name": "_contextvars.copy_context"}, {"pid": 32672, "tid": 22768, "ts": 1072608738561.999, "ph": "X", "cat": "fee", "dur": 0.1, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738558.6, "ph": "X", "cat": "fee", "dur": 4.7, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:31)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738556.9, "ph": "X", "cat": "fee", "dur": 7.4, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:103)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738565.5, "ph": "X", "cat": "fee", "dur": 0.399, "name": "_heapq.heappush"}, {"pid": 32672, "tid": 22768, "ts": 1072608738553.6, "ph": "X", "cat": "fee", "dur": 12.5, "name": "call_at (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:727)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738550.1, "ph": "X", "cat": "fee", "dur": 16.5, "name": "call_later (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:705)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738537.9, "ph": "X", "cat": "fee", "dur": 29.699, "name": "sleep (D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py:593)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738535.4, "ph": "X", "cat": "fee", "dur": 32.299, "name": "say_after (d:\\Program\\Code\\1_Programing_Language\\Python\\Learning\\Fluent_Python\\example2.py:4)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738367.8, "ph": "X", "cat": "fee", "dur": 199.999, "name": "main (d:\\Program\\Code\\1_Programing_Language\\Python\\Learning\\Fluent_Python\\example2.py:9)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738366.8, "ph": "X", "cat": "fee", "dur": 202.0, "name": "_contextvars.Context.run"}, {"pid": 32672, "tid": 22768, "ts": 1072608738365.499, "ph": "X", "cat": "fee", "dur": 203.7, "name": "_run (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:78)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738570.1, "ph": "X", "cat": "fee", "dur": 0.2, "name": "collections.deque.popleft"}, {"pid": 32672, "tid": 22768, "ts": 1072608738576.0, "ph": "X", "cat": "fee", "dur": 1.299, "name": "__contains__ (D:\\Program\\anaconda3\\lib\\_weakrefset.py:75)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738578.9, "ph": "X", "cat": "fee", "dur": 0.2, "name": "set.add"}, {"pid": 32672, "tid": 22768, "ts": 1072608738578.3, "ph": "X", "cat": "fee", "dur": 0.899, "name": "add (D:\\Program\\anaconda3\\lib\\_weakrefset.py:86)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738580.7, "ph": "X", "cat": "fee", "dur": 0.299, "name": "socket.fileno"}, {"pid": 32672, "tid": 22768, "ts": 1072608738581.1, "ph": "X", "cat": "fee", "dur": 3.4, "name": "_overlapped.CreateIoCompletionPort"}, {"pid": 32672, "tid": 22768, "ts": 1072608738574.7, "ph": "X", "cat": "fee", "dur": 9.9, "name": "_register_with_iocp (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:722)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738589.7, "ph": "X", "cat": "fee", "dur": 0.1, "name": "builtins.isinstance"}, {"pid": 32672, "tid": 22768, "ts": 1072608738590.4, "ph": "X", "cat": "fee", "dur": 0.1, "name": "socket.fileno"}, {"pid": 32672, "tid": 22768, "ts": 1072608738590.7, "ph": "X", "cat": "fee", "dur": 9.8, "name": "_overlapped.Overlapped.WSARecv"}, {"pid": 32672, "tid": 22768, "ts": 1072608738602.1, "ph": "X", "cat": "fee", "dur": 0.399, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:423)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738606.199, "ph": "X", "cat": "fee", "dur": 0.1, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738604.6, "ph": "X", "cat": "fee", "dur": 3.499, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:54)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738601.5, "ph": "X", "cat": "fee", "dur": 8.5, "name": "_register (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:732)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738574.0, "ph": "X", "cat": "fee", "dur": 36.3, "name": "recv (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:453)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738611.5, "ph": "X", "cat": "fee", "dur": 0.199, "name": "_OverlappedFuture.add_done_callback"}, {"pid": 32672, "tid": 22768, "ts": 1072608738572.6, "ph": "X", "cat": "fee", "dur": 39.2, "name": "_loop_self_reading (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:772)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738571.799, "ph": "X", "cat": "fee", "dur": 40.2, "name": "_contextvars.Context.run"}, {"pid": 32672, "tid": 22768, "ts": 1072608738571.0, "ph": "X", "cat": "fee", "dur": 41.1, "name": "_run (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:78)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738347.3, "ph": "X", "cat": "fee", "dur": 265.699, "name": "_run_once (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1829)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738614.1, "ph": "X", "cat": "fee", "dur": 0.199, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072608738616.2, "ph": "X", "cat": "fee", "dur": 0.1, "name": "time.monotonic"}, {"pid": 32672, "tid": 22768, "ts": 1072608738615.9, "ph": "X", "cat": "fee", "dur": 0.419, "name": "time (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:696)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738616.7, "ph": "X", "cat": "fee", "dur": 1.1, "name": "builtins.max"}, {"pid": 32672, "tid": 22768, "ts": 1072608738618.0, "ph": "X", "cat": "fee", "dur": 0.499, "name": "builtins.min"}, {"pid": 32672, "tid": 22768, "ts": 1072608738621.0, "ph": "X", "cat": "fee", "dur": 0.4, "name": "math.ceil"}, {"pid": 32672, "tid": 22768, "ts": 1072608738621.9, "ph": "X", "cat": "fee", "dur": 1004449.9, "name": "_overlapped.GetQueuedCompletionStatus"}, {"pid": 32672, "tid": 22768, "ts": 1072609743081.6, "ph": "X", "cat": "fee", "dur": 0.499, "name": "list.clear"}, {"pid": 32672, "tid": 22768, "ts": 1072608738619.6, "ph": "X", "cat": "fee", "dur": 1004463.099, "name": "_poll (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:779)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738619.0, "ph": "X", "cat": "fee", "dur": 1004467.899, "name": "select (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:437)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743090.5, "ph": "X", "cat": "fee", "dur": 0.6, "name": "_process_events (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:861)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743093.999, "ph": "X", "cat": "fee", "dur": 2.1, "name": "time.monotonic"}, {"pid": 32672, "tid": 22768, "ts": 1072609743092.0, "ph": "X", "cat": "fee", "dur": 4.199, "name": "time (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:696)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743117.8, "ph": "X", "cat": "fee", "dur": 0.899, "name": "_heapq.heappop"}, {"pid": 32672, "tid": 22768, "ts": 1072609743120.8, "ph": "X", "cat": "fee", "dur": 0.5, "name": "collections.deque.append"}, {"pid": 32672, "tid": 22768, "ts": 1072609743122.1, "ph": "X", "cat": "fee", "dur": 0.999, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072609743133.6, "ph": "X", "cat": "fee", "dur": 0.1, "name": "collections.deque.popleft"}, {"pid": 32672, "tid": 22768, "ts": 1072609743144.8, "ph": "X", "cat": "fee", "dur": 0.299, "name": "_asyncio.Future.cancelled"}, {"pid": 32672, "tid": 22768, "ts": 1072609743150.9, "ph": "X", "cat": "fee", "dur": 0.6, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743157.5, "ph": "X", "cat": "fee", "dur": 0.399, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743155.3, "ph": "X", "cat": "fee", "dur": 2.899, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:31)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743159.2, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.append"}, {"pid": 32672, "tid": 22768, "ts": 1072609743152.5, "ph": "X", "cat": "fee", "dur": 6.9, "name": "_call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:772)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743150.2, "ph": "X", "cat": "fee", "dur": 9.499, "name": "call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:743)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743145.7, "ph": "X", "cat": "fee", "dur": 14.5, "name": "_asyncio.Future.set_result"}, {"pid": 32672, "tid": 22768, "ts": 1072609743143.6, "ph": "X", "cat": "fee", "dur": 16.799, "name": "_set_result_unless_cancelled (D:\\Program\\anaconda3\\lib\\asyncio\\futures.py:309)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743142.1, "ph": "X", "cat": "fee", "dur": 18.6, "name": "_contextvars.Context.run"}, {"pid": 32672, "tid": 22768, "ts": 1072609743136.399, "ph": "X", "cat": "fee", "dur": 24.7, "name": "_run (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:78)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738613.8, "ph": "X", "cat": "fee", "dur": 1004547.699, "name": "_run_once (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1829)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743162.799, "ph": "X", "cat": "fee", "dur": 0.1, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072609743166.4, "ph": "X", "cat": "fee", "dur": 0.6, "name": "math.ceil"}, {"pid": 32672, "tid": 22768, "ts": 1072609743168.8, "ph": "X", "cat": "fee", "dur": 3.899, "name": "_overlapped.GetQueuedCompletionStatus"}, {"pid": 32672, "tid": 22768, "ts": 1072609743173.4, "ph": "X", "cat": "fee", "dur": 0.02, "name": "list.clear"}, {"pid": 32672, "tid": 22768, "ts": 1072609743164.599, "ph": "X", "cat": "fee", "dur": 9.0, "name": "_poll (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:779)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743164.3, "ph": "X", "cat": "fee", "dur": 9.8, "name": "select (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:437)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743174.399, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_process_events (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:861)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743174.9, "ph": "X", "cat": "fee", "dur": 0.1, "name": "time.monotonic"}, {"pid": 32672, "tid": 22768, "ts": 1072609743174.7, "ph": "X", "cat": "fee", "dur": 0.399, "name": "time (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:696)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743175.699, "ph": "X", "cat": "fee", "dur": 0.1, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072609743176.8, "ph": "X", "cat": "fee", "dur": 0.099, "name": "collections.deque.popleft"}, {"pid": 32672, "tid": 22768, "ts": 1072609743191.2, "ph": "X", "cat": "fee", "dur": 0.3, "name": "_timer_handle_cancelled (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1824)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743195.7, "ph": "X", "cat": "fee", "dur": 0.1, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743195.0, "ph": "X", "cat": "fee", "dur": 1.1, "name": "cancel (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:64)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743189.9, "ph": "X", "cat": "fee", "dur": 6.699, "name": "cancel (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:148)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743183.4, "ph": "X", "cat": "fee", "dur": 13.3, "name": "sleep (D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py:593)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743198.399, "ph": "X", "cat": "fee", "dur": 210.7, "name": "builtins.print"}, {"pid": 32672, "tid": 22768, "ts": 1072609743182.7, "ph": "X", "cat": "fee", "dur": 227.9, "name": "say_after (d:\\Program\\Code\\1_Programing_Language\\Python\\Learning\\Fluent_Python\\example2.py:4)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743418.5, "ph": "X", "cat": "fee", "dur": 0.8, "name": "_asyncio.get_running_loop"}, {"pid": 32672, "tid": 22768, "ts": 1072609743424.9, "ph": "X", "cat": "fee", "dur": 0.399, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743420.6, "ph": "X", "cat": "fee", "dur": 5.199, "name": "create_future (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:427)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743429.0, "ph": "X", "cat": "fee", "dur": 0.5, "name": "time.monotonic"}, {"pid": 32672, "tid": 22768, "ts": 1072609743428.6, "ph": "X", "cat": "fee", "dur": 0.919, "name": "time (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:696)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743432.3, "ph": "X", "cat": "fee", "dur": 0.5, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743437.8, "ph": "X", "cat": "fee", "dur": 0.899, "name": "_contextvars.copy_context"}, {"pid": 32672, "tid": 22768, "ts": 1072609743441.1, "ph": "X", "cat": "fee", "dur": 0.1, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743436.699, "ph": "X", "cat": "fee", "dur": 4.8, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:31)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743434.4, "ph": "X", "cat": "fee", "dur": 7.799, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:103)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743443.5, "ph": "X", "cat": "fee", "dur": 12.199, "name": "_heapq.heappush"}, {"pid": 32672, "tid": 22768, "ts": 1072609743431.799, "ph": "X", "cat": "fee", "dur": 24.4, "name": "call_at (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:727)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743427.499, "ph": "X", "cat": "fee", "dur": 29.5, "name": "call_later (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:705)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743417.1, "ph": "X", "cat": "fee", "dur": 41.0, "name": "sleep (D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py:593)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743414.599, "ph": "X", "cat": "fee", "dur": 43.7, "name": "say_after (d:\\Program\\Code\\1_Programing_Language\\Python\\Learning\\Fluent_Python\\example2.py:4)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743182.2, "ph": "X", "cat": "fee", "dur": 276.2, "name": "main (d:\\Program\\Code\\1_Programing_Language\\Python\\Learning\\Fluent_Python\\example2.py:9)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743178.0, "ph": "X", "cat": "fee", "dur": 283.799, "name": "_contextvars.Context.run"}, {"pid": 32672, "tid": 22768, "ts": 1072609743177.3, "ph": "X", "cat": "fee", "dur": 285.0, "name": "_run (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:78)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743162.4, "ph": "X", "cat": "fee", "dur": 301.5, "name": "_run_once (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1829)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743465.499, "ph": "X", "cat": "fee", "dur": 0.3, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072609743477.1, "ph": "X", "cat": "fee", "dur": 0.399, "name": "time.monotonic"}, {"pid": 32672, "tid": 22768, "ts": 1072609743476.599, "ph": "X", "cat": "fee", "dur": 1.1, "name": "time (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:696)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743478.399, "ph": "X", "cat": "fee", "dur": 2.7, "name": "builtins.max"}, {"pid": 32672, "tid": 22768, "ts": 1072609743481.6, "ph": "X", "cat": "fee", "dur": 0.6, "name": "builtins.min"}, {"pid": 32672, "tid": 22768, "ts": 1072609743486.399, "ph": "X", "cat": "fee", "dur": 0.5, "name": "math.ceil"}, {"pid": 32672, "tid": 22768, "ts": 1072609743487.9, "ph": "X", "cat": "fee", "dur": 2002072.5, "name": "_overlapped.GetQueuedCompletionStatus"}, {"pid": 32672, "tid": 22768, "ts": 1072611745569.1, "ph": "X", "cat": "fee", "dur": 0.5, "name": "list.clear"}, {"pid": 32672, "tid": 22768, "ts": 1072609743483.5, "ph": "X", "cat": "fee", "dur": 2002086.7, "name": "_poll (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:779)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743483.0, "ph": "X", "cat": "fee", "dur": 2002091.8, "name": "select (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:437)"}, {"pid": 32672, "tid": 22768, "ts": 1072611745579.2, "ph": "X", "cat": "fee", "dur": 0.6, "name": "_process_events (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:861)"}, {"pid": 32672, "tid": 22768, "ts": 1072611745582.6, "ph": "X", "cat": "fee", "dur": 1.4, "name": "time.monotonic"}, {"pid": 32672, "tid": 22768, "ts": 1072611745580.9, "ph": "X", "cat": "fee", "dur": 3.2, "name": "time (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:696)"}, {"pid": 32672, "tid": 22768, "ts": 1072611745589.699, "ph": "X", "cat": "fee", "dur": 0.7, "name": "_heapq.heappop"}, {"pid": 32672, "tid": 22768, "ts": 1072611745592.199, "ph": "X", "cat": "fee", "dur": 0.4, "name": "collections.deque.append"}, {"pid": 32672, "tid": 22768, "ts": 1072611745593.6, "ph": "X", "cat": "fee", "dur": 0.7, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072611745599.4, "ph": "X", "cat": "fee", "dur": 0.199, "name": "collections.deque.popleft"}, {"pid": 32672, "tid": 22768, "ts": 1072611745607.2, "ph": "X", "cat": "fee", "dur": 0.299, "name": "_asyncio.Future.cancelled"}, {"pid": 32672, "tid": 22768, "ts": 1072611745612.9, "ph": "X", "cat": "fee", "dur": 0.499, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072611745627.3, "ph": "X", "cat": "fee", "dur": 0.3, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072611745624.699, "ph": "X", "cat": "fee", "dur": 3.4, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:31)"}, {"pid": 32672, "tid": 22768, "ts": 1072611745628.9, "ph": "X", "cat": "fee", "dur": 0.099, "name": "collections.deque.append"}, {"pid": 32672, "tid": 22768, "ts": 1072611745614.1, "ph": "X", "cat": "fee", "dur": 14.999, "name": "_call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:772)"}, {"pid": 32672, "tid": 22768, "ts": 1072611745611.4, "ph": "X", "cat": "fee", "dur": 17.9, "name": "call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:743)"}, {"pid": 32672, "tid": 22768, "ts": 1072611745608.4, "ph": "X", "cat": "fee", "dur": 21.5, "name": "_asyncio.Future.set_result"}, {"pid": 32672, "tid": 22768, "ts": 1072611745606.4, "ph": "X", "cat": "fee", "dur": 23.6, "name": "_set_result_unless_cancelled (D:\\Program\\anaconda3\\lib\\asyncio\\futures.py:309)"}, {"pid": 32672, "tid": 22768, "ts": 1072611745605.5, "ph": "X", "cat": "fee", "dur": 24.799, "name": "_contextvars.Context.run"}, {"pid": 32672, "tid": 22768, "ts": 1072611745601.6, "ph": "X", "cat": "fee", "dur": 29.1, "name": "_run (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:78)"}, {"pid": 32672, "tid": 22768, "ts": 1072609743465.1, "ph": "X", "cat": "fee", "dur": 2002166.0, "name": "_run_once (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1829)"}, {"pid": 32672, "tid": 22768, "ts": 1072611745632.3, "ph": "X", "cat": "fee", "dur": 0.1, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072611745636.7, "ph": "X", "cat": "fee", "dur": 0.899, "name": "math.ceil"}, {"pid": 32672, "tid": 22768, "ts": 1072611745638.799, "ph": "X", "cat": "fee", "dur": 4.8, "name": "_overlapped.GetQueuedCompletionStatus"}, {"pid": 32672, "tid": 22768, "ts": 1072611745644.3, "ph": "X", "cat": "fee", "dur": 0.02, "name": "list.clear"}, {"pid": 32672, "tid": 22768, "ts": 1072611745634.8, "ph": "X", "cat": "fee", "dur": 9.7, "name": "_poll (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:779)"}, {"pid": 32672, "tid": 22768, "ts": 1072611745634.3, "ph": "X", "cat": "fee", "dur": 10.8, "name": "select (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:437)"}, {"pid": 32672, "tid": 22768, "ts": 1072611745645.4, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_process_events (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:861)"}, {"pid": 32672, "tid": 22768, "ts": 1072611745646.0, "ph": "X", "cat": "fee", "dur": 0.199, "name": "time.monotonic"}, {"pid": 32672, "tid": 22768, "ts": 1072611745645.8, "ph": "X", "cat": "fee", "dur": 0.5, "name": "time (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:696)"}, {"pid": 32672, "tid": 22768, "ts": 1072611745646.799, "ph": "X", "cat": "fee", "dur": 0.1, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072611745647.7, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.popleft"}, {"pid": 32672, "tid": 22768, "ts": 1072611745655.2, "ph": "X", "cat": "fee", "dur": 0.199, "name": "_timer_handle_cancelled (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1824)"}, {"pid": 32672, "tid": 22768, "ts": 1072611745659.2, "ph": "X", "cat": "fee", "dur": 0.2, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072611745658.7, "ph": "X", "cat": "fee", "dur": 0.999, "name": "cancel (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:64)"}, {"pid": 32672, "tid": 22768, "ts": 1072611745654.3, "ph": "X", "cat": "fee", "dur": 9.699, "name": "cancel (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:148)"}, {"pid": 32672, "tid": 22768, "ts": 1072611745652.8, "ph": "X", "cat": "fee", "dur": 11.299, "name": "sleep (D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py:593)"}, {"pid": 32672, "tid": 22768, "ts": 1072611745666.3, "ph": "X", "cat": "fee", "dur": 182.4, "name": "builtins.print"}, {"pid": 32672, "tid": 22768, "ts": 1072611745652.1, "ph": "X", "cat": "fee", "dur": 197.7, "name": "say_after (d:\\Program\\Code\\1_Programing_Language\\Python\\Learning\\Fluent_Python\\example2.py:4)"}, {"pid": 32672, "tid": 22768, "ts": 1072611745852.599, "ph": "X", "cat": "fee", "dur": 17.1, "name": "time.strftime"}, {"pid": 32672, "tid": 22768, "ts": 1072611745870.9, "ph": "X", "cat": "fee", "dur": 201.199, "name": "builtins.print"}, {"pid": 32672, "tid": 22768, "ts": 1072611745651.6, "ph": "X", "cat": "fee", "dur": 422.199, "name": "main (d:\\Program\\Code\\1_Programing_Language\\Python\\Learning\\Fluent_Python\\example2.py:9)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746081.7, "ph": "X", "cat": "fee", "dur": 0.9, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746090.5, "ph": "X", "cat": "fee", "dur": 0.2, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746087.2, "ph": "X", "cat": "fee", "dur": 3.9, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:31)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746092.699, "ph": "X", "cat": "fee", "dur": 0.3, "name": "collections.deque.append"}, {"pid": 32672, "tid": 22768, "ts": 1072611746083.3, "ph": "X", "cat": "fee", "dur": 63.599, "name": "_call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:772)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746080.1, "ph": "X", "cat": "fee", "dur": 67.799, "name": "call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:743)"}, {"pid": 32672, "tid": 22768, "ts": 1072611745648.8, "ph": "X", "cat": "fee", "dur": 500.9, "name": "_contextvars.Context.run"}, {"pid": 32672, "tid": 22768, "ts": 1072611745648.099, "ph": "X", "cat": "fee", "dur": 502.3, "name": "_run (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:78)"}, {"pid": 32672, "tid": 22768, "ts": 1072611745632.0, "ph": "X", "cat": "fee", "dur": 520.999, "name": "_run_once (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1829)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746155.8, "ph": "X", "cat": "fee", "dur": 0.399, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072611746161.8, "ph": "X", "cat": "fee", "dur": 1.1, "name": "math.ceil"}, {"pid": 32672, "tid": 22768, "ts": 1072611746164.1, "ph": "X", "cat": "fee", "dur": 3.0, "name": "_overlapped.GetQueuedCompletionStatus"}, {"pid": 32672, "tid": 22768, "ts": 1072611746168.4, "ph": "X", "cat": "fee", "dur": 0.2, "name": "list.clear"}, {"pid": 32672, "tid": 22768, "ts": 1072611746160.0, "ph": "X", "cat": "fee", "dur": 8.799, "name": "_poll (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:779)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746158.999, "ph": "X", "cat": "fee", "dur": 11.0, "name": "select (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:437)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746170.8, "ph": "X", "cat": "fee", "dur": 0.2, "name": "_process_events (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:861)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746172.1, "ph": "X", "cat": "fee", "dur": 0.5, "name": "time.monotonic"}, {"pid": 32672, "tid": 22768, "ts": 1072611746171.5, "ph": "X", "cat": "fee", "dur": 1.2, "name": "time (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:696)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746173.3, "ph": "X", "cat": "fee", "dur": 0.2, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072611746175.4, "ph": "X", "cat": "fee", "dur": 0.199, "name": "collections.deque.popleft"}, {"pid": 32672, "tid": 22768, "ts": 1072611746180.2, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_asyncio.Task.cancelled"}, {"pid": 32672, "tid": 22768, "ts": 1072611746180.8, "ph": "X", "cat": "fee", "dur": 0.2, "name": "_asyncio.Task.exception"}, {"pid": 32672, "tid": 22768, "ts": 1072611746181.699, "ph": "X", "cat": "fee", "dur": 1.2, "name": "builtins.isinstance"}, {"pid": 32672, "tid": 22768, "ts": 1072611746188.7, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_asyncio.Task.get_loop"}, {"pid": 32672, "tid": 22768, "ts": 1072611746187.5, "ph": "X", "cat": "fee", "dur": 1.4, "name": "_get_loop (D:\\Program\\anaconda3\\lib\\asyncio\\futures.py:297)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746190.7, "ph": "X", "cat": "fee", "dur": 0.4, "name": "stop (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:651)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746179.199, "ph": "X", "cat": "fee", "dur": 12.0, "name": "_run_until_complete_cb (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:184)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746177.7, "ph": "X", "cat": "fee", "dur": 14.2, "name": "_contextvars.Context.run"}, {"pid": 32672, "tid": 22768, "ts": 1072611746176.199, "ph": "X", "cat": "fee", "dur": 15.9, "name": "_run (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:78)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746154.3, "ph": "X", "cat": "fee", "dur": 38.399, "name": "_run_once (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1829)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746194.599, "ph": "X", "cat": "fee", "dur": 1.2, "name": "_asyncio._set_running_loop"}, {"pid": 32672, "tid": 22768, "ts": 1072611746196.399, "ph": "X", "cat": "fee", "dur": 0.9, "name": "_set_coroutine_origin_tracking (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1909)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746199.8, "ph": "X", "cat": "fee", "dur": 2.7, "name": "sys.set_asyncgen_hooks"}, {"pid": 32672, "tid": 22768, "ts": 1072608738340.299, "ph": "X", "cat": "fee", "dur": 3007862.3, "name": "run_forever (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:589)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746309.0, "ph": "X", "cat": "fee", "dur": 8.399, "name": "_overlapped.Overlapped.cancel"}, {"pid": 32672, "tid": 22768, "ts": 1072611746235.099, "ph": "X", "cat": "fee", "dur": 84.8, "name": "_cancel_overlapped (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:67)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746344.7, "ph": "X", "cat": "fee", "dur": 0.399, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746349.5, "ph": "X", "cat": "fee", "dur": 0.2, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746347.499, "ph": "X", "cat": "fee", "dur": 2.5, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:31)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746350.8, "ph": "X", "cat": "fee", "dur": 0.2, "name": "collections.deque.append"}, {"pid": 32672, "tid": 22768, "ts": 1072611746345.5, "ph": "X", "cat": "fee", "dur": 8.199, "name": "_call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:772)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746344.1, "ph": "X", "cat": "fee", "dur": 9.8, "name": "call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:743)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746338.1, "ph": "X", "cat": "fee", "dur": 16.3, "name": "_OverlappedFuture.cancel"}, {"pid": 32672, "tid": 22768, "ts": 1072611746234.499, "ph": "X", "cat": "fee", "dur": 120.1, "name": "cancel (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:83)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746356.699, "ph": "X", "cat": "fee", "dur": 0.4, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:423)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746358.1, "ph": "X", "cat": "fee", "dur": 0.2, "name": "list.append"}, {"pid": 32672, "tid": 22768, "ts": 1072611746356.3, "ph": "X", "cat": "fee", "dur": 2.1, "name": "_unregister (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:764)"}, {"pid": 32672, "tid": 22768, "ts": 1072608738335.2, "ph": "X", "cat": "fee", "dur": 3008023.499, "name": "run_forever (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:317)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746360.0, "ph": "X", "cat": "fee", "dur": 0.2, "name": "_asyncio.Task.remove_done_callback"}, {"pid": 32672, "tid": 22768, "ts": 1072611746360.7, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_asyncio.Task.done"}, {"pid": 32672, "tid": 22768, "ts": 1072611746361.7, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_asyncio.Task.result"}, {"pid": 32672, "tid": 22768, "ts": 1072608738296.3, "ph": "X", "cat": "fee", "dur": 3008065.6, "name": "run_until_complete (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:613)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746368.5, "ph": "X", "cat": "fee", "dur": 0.6, "name": "set.discard"}, {"pid": 32672, "tid": 22768, "ts": 1072611746365.299, "ph": "X", "cat": "fee", "dur": 3.9, "name": "_remove (D:\\Program\\anaconda3\\lib\\_weakrefset.py:39)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746375.9, "ph": "X", "cat": "fee", "dur": 4.4, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072611746380.6, "ph": "X", "cat": "fee", "dur": 0.099, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072611746375.5, "ph": "X", "cat": "fee", "dur": 5.5, "name": "__len__ (D:\\Program\\anaconda3\\lib\\_weakrefset.py:72)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746382.9, "ph": "X", "cat": "fee", "dur": 2.099, "name": "__init__ (D:\\Program\\anaconda3\\lib\\_weakrefset.py:17)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746386.5, "ph": "X", "cat": "fee", "dur": 0.2, "name": "set.add"}, {"pid": 32672, "tid": 22768, "ts": 1072611746385.8, "ph": "X", "cat": "fee", "dur": 0.999, "name": "__enter__ (D:\\Program\\anaconda3\\lib\\_weakrefset.py:21)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746388.5, "ph": "X", "cat": "fee", "dur": 0.099, "name": "set.remove"}, {"pid": 32672, "tid": 22768, "ts": 1072611746390.1, "ph": "X", "cat": "fee", "dur": 0.3, "name": "list.pop"}, {"pid": 32672, "tid": 22768, "ts": 1072611746389.1, "ph": "X", "cat": "fee", "dur": 3.299, "name": "_commit_removals (D:\\Program\\anaconda3\\lib\\_weakrefset.py:53)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746387.999, "ph": "X", "cat": "fee", "dur": 4.6, "name": "__exit__ (D:\\Program\\anaconda3\\lib\\_weakrefset.py:27)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746381.5, "ph": "X", "cat": "fee", "dur": 11.6, "name": "__iter__ (D:\\Program\\anaconda3\\lib\\_weakrefset.py:63)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746395.8, "ph": "X", "cat": "fee", "dur": 0.6, "name": "<setcomp> (D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py:61)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746372.9, "ph": "X", "cat": "fee", "dur": 24.1, "name": "all_tasks (D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py:42)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746371.3, "ph": "X", "cat": "fee", "dur": 26.099, "name": "_cancel_all_tasks (D:\\Program\\anaconda3\\lib\\asyncio\\runners.py:55)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746399.3, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746400.1, "ph": "X", "cat": "fee", "dur": 0.199, "name": "is_running (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:692)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746401.4, "ph": "X", "cat": "fee", "dur": 0.299, "name": "_asyncio._get_running_loop"}, {"pid": 32672, "tid": 22768, "ts": 1072611746399.699, "ph": "X", "cat": "fee", "dur": 2.1, "name": "_check_running (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:582)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746403.6, "ph": "X", "cat": "fee", "dur": 1.8, "name": "builtins.hasattr"}, {"pid": 32672, "tid": 22768, "ts": 1072611746402.4, "ph": "X", "cat": "fee", "dur": 3.1, "name": "isfuture (D:\\Program\\anaconda3\\lib\\asyncio\\base_futures.py:14)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746407.699, "ph": "X", "cat": "fee", "dur": 0.3, "name": "builtins.hasattr"}, {"pid": 32672, "tid": 22768, "ts": 1072611746407.6, "ph": "X", "cat": "fee", "dur": 0.419, "name": "isfuture (D:\\Program\\anaconda3\\lib\\asyncio\\base_futures.py:14)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746409.6, "ph": "X", "cat": "fee", "dur": 0.8, "name": "iscoroutine (D:\\Program\\anaconda3\\lib\\asyncio\\coroutines.py:177)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746411.9, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746413.799, "ph": "X", "cat": "fee", "dur": 0.1, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746427.299, "ph": "X", "cat": "fee", "dur": 0.7, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746432.299, "ph": "X", "cat": "fee", "dur": 0.2, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746430.399, "ph": "X", "cat": "fee", "dur": 2.4, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:31)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746433.8, "ph": "X", "cat": "fee", "dur": 0.299, "name": "collections.deque.append"}, {"pid": 32672, "tid": 22768, "ts": 1072611746428.7, "ph": "X", "cat": "fee", "dur": 5.6, "name": "_call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:772)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746426.5, "ph": "X", "cat": "fee", "dur": 8.1, "name": "call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:743)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746437.199, "ph": "X", "cat": "fee", "dur": 0.3, "name": "set.add"}, {"pid": 32672, "tid": 22768, "ts": 1072611746435.5, "ph": "X", "cat": "fee", "dur": 5.1, "name": "add (D:\\Program\\anaconda3\\lib\\_weakrefset.py:86)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746411.5, "ph": "X", "cat": "fee", "dur": 30.2, "name": "create_task (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:431)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746407.2, "ph": "X", "cat": "fee", "dur": 34.899, "name": "_ensure_future (D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py:618)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746406.9, "ph": "X", "cat": "fee", "dur": 35.499, "name": "ensure_future (D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py:610)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746443.2, "ph": "X", "cat": "fee", "dur": 0.199, "name": "_asyncio.Task.add_done_callback"}, {"pid": 32672, "tid": 22768, "ts": 1072611746445.0, "ph": "X", "cat": "fee", "dur": 0.099, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746446.3, "ph": "X", "cat": "fee", "dur": 0.399, "name": "_contextvars.copy_context"}, {"pid": 32672, "tid": 22768, "ts": 1072611746447.2, "ph": "X", "cat": "fee", "dur": 0.02, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746445.7, "ph": "X", "cat": "fee", "dur": 1.699, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:31)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746447.8, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.append"}, {"pid": 32672, "tid": 22768, "ts": 1072611746445.4, "ph": "X", "cat": "fee", "dur": 2.5, "name": "_call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:772)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746444.8, "ph": "X", "cat": "fee", "dur": 3.3, "name": "call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:743)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746450.2, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746450.699, "ph": "X", "cat": "fee", "dur": 0.1, "name": "is_running (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:692)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746451.1, "ph": "X", "cat": "fee", "dur": 0.199, "name": "_asyncio._get_running_loop"}, {"pid": 32672, "tid": 22768, "ts": 1072611746450.4, "ph": "X", "cat": "fee", "dur": 1.1, "name": "_check_running (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:582)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746451.8, "ph": "X", "cat": "fee", "dur": 0.799, "name": "_set_coroutine_origin_tracking (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1909)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746453.2, "ph": "X", "cat": "fee", "dur": 0.9, "name": "sys.get_asyncgen_hooks"}, {"pid": 32672, "tid": 22768, "ts": 1072611746454.999, "ph": "X", "cat": "fee", "dur": 0.6, "name": "_thread.get_ident"}, {"pid": 32672, "tid": 22768, "ts": 1072611746456.9, "ph": "X", "cat": "fee", "dur": 1.699, "name": "sys.set_asyncgen_hooks"}, {"pid": 32672, "tid": 22768, "ts": 1072611746458.8, "ph": "X", "cat": "fee", "dur": 0.499, "name": "_asyncio._set_running_loop"}, {"pid": 32672, "tid": 22768, "ts": 1072611746459.8, "ph": "X", "cat": "fee", "dur": 0.199, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072611746462.899, "ph": "X", "cat": "fee", "dur": 0.4, "name": "math.ceil"}, {"pid": 32672, "tid": 22768, "ts": 1072611746464.0, "ph": "X", "cat": "fee", "dur": 17.6, "name": "_overlapped.GetQueuedCompletionStatus"}, {"pid": 32672, "tid": 22768, "ts": 1072611746482.4, "ph": "X", "cat": "fee", "dur": 0.9, "name": "dict.pop"}, {"pid": 32672, "tid": 22768, "ts": 1072611746484.399, "ph": "X", "cat": "fee", "dur": 0.9, "name": "__contains__ (D:\\Program\\anaconda3\\lib\\_weakrefset.py:75)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746486.299, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_OverlappedFuture.done"}, {"pid": 32672, "tid": 22768, "ts": 1072611746486.7, "ph": "X", "cat": "fee", "dur": 0.5, "name": "_overlapped.GetQueuedCompletionStatus"}, {"pid": 32672, "tid": 22768, "ts": 1072611746488.5, "ph": "X", "cat": "fee", "dur": 0.1, "name": "dict.pop"}, {"pid": 32672, "tid": 22768, "ts": 1072611746489.1, "ph": "X", "cat": "fee", "dur": 0.2, "name": "list.clear"}, {"pid": 32672, "tid": 22768, "ts": 1072611746461.6, "ph": "X", "cat": "fee", "dur": 27.799, "name": "_poll (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:779)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746461.0, "ph": "X", "cat": "fee", "dur": 32.5, "name": "select (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:437)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746493.9, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_process_events (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:861)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746494.7, "ph": "X", "cat": "fee", "dur": 0.199, "name": "time.monotonic"}, {"pid": 32672, "tid": 22768, "ts": 1072611746494.4, "ph": "X", "cat": "fee", "dur": 0.6, "name": "time (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:696)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746495.6, "ph": "X", "cat": "fee", "dur": 0.2, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072611746497.2, "ph": "X", "cat": "fee", "dur": 0.099, "name": "collections.deque.popleft"}, {"pid": 32672, "tid": 22768, "ts": 1072611746500.0, "ph": "X", "cat": "fee", "dur": 0.7, "name": "_OverlappedFuture.result"}, {"pid": 32672, "tid": 22768, "ts": 1072611746499.4, "ph": "X", "cat": "fee", "dur": 4.2, "name": "_loop_self_reading (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:772)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746499.0, "ph": "X", "cat": "fee", "dur": 4.9, "name": "_contextvars.Context.run"}, {"pid": 32672, "tid": 22768, "ts": 1072611746497.899, "ph": "X", "cat": "fee", "dur": 6.2, "name": "_run (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:78)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746504.4, "ph": "X", "cat": "fee", "dur": 0.1, "name": "collections.deque.popleft"}, {"pid": 32672, "tid": 22768, "ts": 1072611746511.7, "ph": "X", "cat": "fee", "dur": 0.1, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072611746512.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072611746511.4, "ph": "X", "cat": "fee", "dur": 0.799, "name": "__len__ (D:\\Program\\anaconda3\\lib\\_weakrefset.py:72)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746511.2, "ph": "X", "cat": "fee", "dur": 1.2, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072611746510.4, "ph": "X", "cat": "fee", "dur": 2.2, "name": "shutdown_asyncgens (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:535)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746514.499, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746515.799, "ph": "X", "cat": "fee", "dur": 0.02, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746515.199, "ph": "X", "cat": "fee", "dur": 0.8, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:31)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746516.3, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.append"}, {"pid": 32672, "tid": 22768, "ts": 1072611746514.9, "ph": "X", "cat": "fee", "dur": 1.499, "name": "_call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:772)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746514.4, "ph": "X", "cat": "fee", "dur": 2.199, "name": "call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:743)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746505.8, "ph": "X", "cat": "fee", "dur": 11.199, "name": "_contextvars.Context.run"}, {"pid": 32672, "tid": 22768, "ts": 1072611746505.4, "ph": "X", "cat": "fee", "dur": 11.7, "name": "_run (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:78)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746517.2, "ph": "X", "cat": "fee", "dur": 0.1, "name": "collections.deque.popleft"}, {"pid": 32672, "tid": 22768, "ts": 1072611746519.499, "ph": "X", "cat": "fee", "dur": 0.7, "name": "__contains__ (D:\\Program\\anaconda3\\lib\\_weakrefset.py:75)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746519.2, "ph": "X", "cat": "fee", "dur": 1.2, "name": "_register_with_iocp (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:722)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746536.599, "ph": "X", "cat": "fee", "dur": 0.1, "name": "builtins.isinstance"}, {"pid": 32672, "tid": 22768, "ts": 1072611746538.0, "ph": "X", "cat": "fee", "dur": 0.7, "name": "socket.fileno"}, {"pid": 32672, "tid": 22768, "ts": 1072611746538.9, "ph": "X", "cat": "fee", "dur": 9.5, "name": "_overlapped.Overlapped.WSARecv"}, {"pid": 32672, "tid": 22768, "ts": 1072611746549.599, "ph": "X", "cat": "fee", "dur": 0.2, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:423)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746553.1, "ph": "X", "cat": "fee", "dur": 0.099, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746551.3, "ph": "X", "cat": "fee", "dur": 3.299, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:54)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746549.4, "ph": "X", "cat": "fee", "dur": 6.599, "name": "_register (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:732)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746518.6, "ph": "X", "cat": "fee", "dur": 37.6, "name": "recv (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:453)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746556.8, "ph": "X", "cat": "fee", "dur": 0.099, "name": "_OverlappedFuture.add_done_callback"}, {"pid": 32672, "tid": 22768, "ts": 1072611746518.1, "ph": "X", "cat": "fee", "dur": 38.899, "name": "_loop_self_reading (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:772)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746517.9, "ph": "X", "cat": "fee", "dur": 39.2, "name": "_contextvars.Context.run"}, {"pid": 32672, "tid": 22768, "ts": 1072611746517.7, "ph": "X", "cat": "fee", "dur": 39.5, "name": "_run (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:78)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746459.5, "ph": "X", "cat": "fee", "dur": 98.1, "name": "_run_once (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1829)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746558.099, "ph": "X", "cat": "fee", "dur": 0.1, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072611746559.2, "ph": "X", "cat": "fee", "dur": 0.02, "name": "math.ceil"}, {"pid": 32672, "tid": 22768, "ts": 1072611746559.5, "ph": "X", "cat": "fee", "dur": 0.499, "name": "_overlapped.GetQueuedCompletionStatus"}, {"pid": 32672, "tid": 22768, "ts": 1072611746560.4, "ph": "X", "cat": "fee", "dur": 0.1, "name": "list.clear"}, {"pid": 32672, "tid": 22768, "ts": 1072611746558.799, "ph": "X", "cat": "fee", "dur": 1.8, "name": "_poll (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:779)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746558.6, "ph": "X", "cat": "fee", "dur": 2.299, "name": "select (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:437)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746561.1, "ph": "X", "cat": "fee", "dur": 0.099, "name": "_process_events (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:861)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746561.5, "ph": "X", "cat": "fee", "dur": 0.1, "name": "time.monotonic"}, {"pid": 32672, "tid": 22768, "ts": 1072611746561.4, "ph": "X", "cat": "fee", "dur": 0.3, "name": "time (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:696)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746562.0, "ph": "X", "cat": "fee", "dur": 0.1, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072611746562.6, "ph": "X", "cat": "fee", "dur": 0.1, "name": "collections.deque.popleft"}, {"pid": 32672, "tid": 22768, "ts": 1072611746563.699, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_asyncio.Task.cancelled"}, {"pid": 32672, "tid": 22768, "ts": 1072611746564.0, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_asyncio.Task.exception"}, {"pid": 32672, "tid": 22768, "ts": 1072611746564.5, "ph": "X", "cat": "fee", "dur": 0.399, "name": "builtins.isinstance"}, {"pid": 32672, "tid": 22768, "ts": 1072611746566.4, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.get_loop"}, {"pid": 32672, "tid": 22768, "ts": 1072611746565.6, "ph": "X", "cat": "fee", "dur": 0.9, "name": "_get_loop (D:\\Program\\anaconda3\\lib\\asyncio\\futures.py:297)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746567.0, "ph": "X", "cat": "fee", "dur": 0.2, "name": "stop (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:651)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746563.5, "ph": "X", "cat": "fee", "dur": 3.799, "name": "_run_until_complete_cb (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:184)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746563.3, "ph": "X", "cat": "fee", "dur": 4.099, "name": "_contextvars.Context.run"}, {"pid": 32672, "tid": 22768, "ts": 1072611746562.9, "ph": "X", "cat": "fee", "dur": 4.519, "name": "_run (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:78)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746558.0, "ph": "X", "cat": "fee", "dur": 9.699, "name": "_run_once (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1829)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746568.2, "ph": "X", "cat": "fee", "dur": 0.2, "name": "_asyncio._set_running_loop"}, {"pid": 32672, "tid": 22768, "ts": 1072611746569.5, "ph": "X", "cat": "fee", "dur": 0.299, "name": "_set_coroutine_origin_tracking (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1909)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746570.5, "ph": "X", "cat": "fee", "dur": 0.3, "name": "sys.set_asyncgen_hooks"}, {"pid": 32672, "tid": 22768, "ts": 1072611746450.0, "ph": "X", "cat": "fee", "dur": 120.9, "name": "run_forever (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:589)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746572.2, "ph": "X", "cat": "fee", "dur": 2.599, "name": "_overlapped.Overlapped.cancel"}, {"pid": 32672, "tid": 22768, "ts": 1072611746571.9, "ph": "X", "cat": "fee", "dur": 3.2, "name": "_cancel_overlapped (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:67)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746576.2, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746577.3, "ph": "X", "cat": "fee", "dur": 0.02, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746576.8, "ph": "X", "cat": "fee", "dur": 0.7, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:31)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746577.7, "ph": "X", "cat": "fee", "dur": 0.099, "name": "collections.deque.append"}, {"pid": 32672, "tid": 22768, "ts": 1072611746576.5, "ph": "X", "cat": "fee", "dur": 1.399, "name": "_call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:772)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746576.1, "ph": "X", "cat": "fee", "dur": 1.9, "name": "call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:743)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746575.6, "ph": "X", "cat": "fee", "dur": 2.6, "name": "_OverlappedFuture.cancel"}, {"pid": 32672, "tid": 22768, "ts": 1072611746571.699, "ph": "X", "cat": "fee", "dur": 6.6, "name": "cancel (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:83)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746578.7, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:423)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746578.9, "ph": "X", "cat": "fee", "dur": 0.199, "name": "list.append"}, {"pid": 32672, "tid": 22768, "ts": 1072611746578.6, "ph": "X", "cat": "fee", "dur": 0.52, "name": "_unregister (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:764)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746443.899, "ph": "X", "cat": "fee", "dur": 135.5, "name": "run_forever (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:317)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746579.7, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_asyncio.Task.remove_done_callback"}, {"pid": 32672, "tid": 22768, "ts": 1072611746579.9, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_asyncio.Task.done"}, {"pid": 32672, "tid": 22768, "ts": 1072611746580.1, "ph": "X", "cat": "fee", "dur": 0.099, "name": "_asyncio.Task.result"}, {"pid": 32672, "tid": 22768, "ts": 1072611746399.2, "ph": "X", "cat": "fee", "dur": 181.099, "name": "run_until_complete (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:613)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746581.4, "ph": "X", "cat": "fee", "dur": 0.099, "name": "set.discard"}, {"pid": 32672, "tid": 22768, "ts": 1072611746580.7, "ph": "X", "cat": "fee", "dur": 0.899, "name": "_remove (D:\\Program\\anaconda3\\lib\\_weakrefset.py:39)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746583.8, "ph": "X", "cat": "fee", "dur": 0.099, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746584.2, "ph": "X", "cat": "fee", "dur": 0.2, "name": "is_running (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:692)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746584.499, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_asyncio._get_running_loop"}, {"pid": 32672, "tid": 22768, "ts": 1072611746584.1, "ph": "X", "cat": "fee", "dur": 0.6, "name": "_check_running (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:582)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746585.4, "ph": "X", "cat": "fee", "dur": 1.3, "name": "builtins.hasattr"}, {"pid": 32672, "tid": 22768, "ts": 1072611746585.0, "ph": "X", "cat": "fee", "dur": 1.9, "name": "isfuture (D:\\Program\\anaconda3\\lib\\asyncio\\base_futures.py:14)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746588.0, "ph": "X", "cat": "fee", "dur": 0.4, "name": "builtins.hasattr"}, {"pid": 32672, "tid": 22768, "ts": 1072611746587.9, "ph": "X", "cat": "fee", "dur": 0.52, "name": "isfuture (D:\\Program\\anaconda3\\lib\\asyncio\\base_futures.py:14)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746589.3, "ph": "X", "cat": "fee", "dur": 0.499, "name": "iscoroutine (D:\\Program\\anaconda3\\lib\\asyncio\\coroutines.py:177)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746590.3, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746591.0, "ph": "X", "cat": "fee", "dur": 0.1, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746592.1, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746593.0, "ph": "X", "cat": "fee", "dur": 0.099, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746592.7, "ph": "X", "cat": "fee", "dur": 0.6, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:31)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746593.5, "ph": "X", "cat": "fee", "dur": 0.1, "name": "collections.deque.append"}, {"pid": 32672, "tid": 22768, "ts": 1072611746592.499, "ph": "X", "cat": "fee", "dur": 1.2, "name": "_call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:772)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746592.0, "ph": "X", "cat": "fee", "dur": 1.799, "name": "call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:743)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746594.6, "ph": "X", "cat": "fee", "dur": 0.1, "name": "set.add"}, {"pid": 32672, "tid": 22768, "ts": 1072611746594.1, "ph": "X", "cat": "fee", "dur": 0.7, "name": "add (D:\\Program\\anaconda3\\lib\\_weakrefset.py:86)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746590.2, "ph": "X", "cat": "fee", "dur": 4.9, "name": "create_task (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:431)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746587.7, "ph": "X", "cat": "fee", "dur": 7.5, "name": "_ensure_future (D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py:618)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746587.5, "ph": "X", "cat": "fee", "dur": 7.799, "name": "ensure_future (D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py:610)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746595.7, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_asyncio.Task.add_done_callback"}, {"pid": 32672, "tid": 22768, "ts": 1072611746596.5, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746600.8, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_contextvars.copy_context"}, {"pid": 32672, "tid": 22768, "ts": 1072611746601.2, "ph": "X", "cat": "fee", "dur": 0.1, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746600.6, "ph": "X", "cat": "fee", "dur": 0.8, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:31)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746601.6, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.append"}, {"pid": 32672, "tid": 22768, "ts": 1072611746600.4, "ph": "X", "cat": "fee", "dur": 1.299, "name": "_call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:772)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746596.4, "ph": "X", "cat": "fee", "dur": 5.399, "name": "call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:743)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746602.399, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746602.7, "ph": "X", "cat": "fee", "dur": 0.1, "name": "is_running (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:692)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746602.9, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio._get_running_loop"}, {"pid": 32672, "tid": 22768, "ts": 1072611746602.6, "ph": "X", "cat": "fee", "dur": 0.399, "name": "_check_running (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:582)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746603.3, "ph": "X", "cat": "fee", "dur": 0.2, "name": "_set_coroutine_origin_tracking (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1909)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746603.599, "ph": "X", "cat": "fee", "dur": 0.1, "name": "sys.get_asyncgen_hooks"}, {"pid": 32672, "tid": 22768, "ts": 1072611746603.8, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_thread.get_ident"}, {"pid": 32672, "tid": 22768, "ts": 1072611746604.3, "ph": "X", "cat": "fee", "dur": 0.4, "name": "sys.set_asyncgen_hooks"}, {"pid": 32672, "tid": 22768, "ts": 1072611746604.9, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_asyncio._set_running_loop"}, {"pid": 32672, "tid": 22768, "ts": 1072611746605.2, "ph": "X", "cat": "fee", "dur": 0.1, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072611746605.999, "ph": "X", "cat": "fee", "dur": 0.1, "name": "math.ceil"}, {"pid": 32672, "tid": 22768, "ts": 1072611746606.3, "ph": "X", "cat": "fee", "dur": 0.8, "name": "_overlapped.GetQueuedCompletionStatus"}, {"pid": 32672, "tid": 22768, "ts": 1072611746607.4, "ph": "X", "cat": "fee", "dur": 0.2, "name": "dict.pop"}, {"pid": 32672, "tid": 22768, "ts": 1072611746607.8, "ph": "X", "cat": "fee", "dur": 0.399, "name": "__contains__ (D:\\Program\\anaconda3\\lib\\_weakrefset.py:75)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746608.499, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_OverlappedFuture.done"}, {"pid": 32672, "tid": 22768, "ts": 1072611746608.8, "ph": "X", "cat": "fee", "dur": 0.299, "name": "_overlapped.GetQueuedCompletionStatus"}, {"pid": 32672, "tid": 22768, "ts": 1072611746609.5, "ph": "X", "cat": "fee", "dur": 0.1, "name": "dict.pop"}, {"pid": 32672, "tid": 22768, "ts": 1072611746609.699, "ph": "X", "cat": "fee", "dur": 0.1, "name": "list.clear"}, {"pid": 32672, "tid": 22768, "ts": 1072611746605.8, "ph": "X", "cat": "fee", "dur": 4.1, "name": "_poll (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:779)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746605.6, "ph": "X", "cat": "fee", "dur": 4.699, "name": "select (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:437)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746610.5, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_process_events (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:861)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746610.899, "ph": "X", "cat": "fee", "dur": 0.02, "name": "time.monotonic"}, {"pid": 32672, "tid": 22768, "ts": 1072611746610.7, "ph": "X", "cat": "fee", "dur": 0.299, "name": "time (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:696)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746611.3, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072611746611.7, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.popleft"}, {"pid": 32672, "tid": 22768, "ts": 1072611746612.6, "ph": "X", "cat": "fee", "dur": 0.199, "name": "_OverlappedFuture.result"}, {"pid": 32672, "tid": 22768, "ts": 1072611746612.4, "ph": "X", "cat": "fee", "dur": 0.8, "name": "_loop_self_reading (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:772)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746612.3, "ph": "X", "cat": "fee", "dur": 1.0, "name": "_contextvars.Context.run"}, {"pid": 32672, "tid": 22768, "ts": 1072611746612.0, "ph": "X", "cat": "fee", "dur": 1.399, "name": "_run (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:78)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746613.5, "ph": "X", "cat": "fee", "dur": 0.1, "name": "collections.deque.popleft"}, {"pid": 32672, "tid": 22768, "ts": 1072611746614.4, "ph": "X", "cat": "fee", "dur": 0.4, "name": "shutdown_default_executor (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:560)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746615.199, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746616.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746615.6, "ph": "X", "cat": "fee", "dur": 0.6, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:31)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746616.4, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.append"}, {"pid": 32672, "tid": 22768, "ts": 1072611746615.4, "ph": "X", "cat": "fee", "dur": 1.099, "name": "_call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:772)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746615.0, "ph": "X", "cat": "fee", "dur": 1.6, "name": "call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:743)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746614.2, "ph": "X", "cat": "fee", "dur": 2.6, "name": "_contextvars.Context.run"}, {"pid": 32672, "tid": 22768, "ts": 1072611746613.999, "ph": "X", "cat": "fee", "dur": 2.9, "name": "_run (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:78)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746617.099, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.popleft"}, {"pid": 32672, "tid": 22768, "ts": 1072611746619.2, "ph": "X", "cat": "fee", "dur": 0.2, "name": "__contains__ (D:\\Program\\anaconda3\\lib\\_weakrefset.py:75)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746618.999, "ph": "X", "cat": "fee", "dur": 0.5, "name": "_register_with_iocp (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:722)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746620.0, "ph": "X", "cat": "fee", "dur": 0.02, "name": "builtins.isinstance"}, {"pid": 32672, "tid": 22768, "ts": 1072611746620.3, "ph": "X", "cat": "fee", "dur": 0.02, "name": "socket.fileno"}, {"pid": 32672, "tid": 22768, "ts": 1072611746620.5, "ph": "X", "cat": "fee", "dur": 1.499, "name": "_overlapped.Overlapped.WSARecv"}, {"pid": 32672, "tid": 22768, "ts": 1072611746622.599, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:423)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746623.5, "ph": "X", "cat": "fee", "dur": 0.1, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746623.0, "ph": "X", "cat": "fee", "dur": 0.899, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:54)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746622.4, "ph": "X", "cat": "fee", "dur": 2.0, "name": "_register (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:732)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746618.899, "ph": "X", "cat": "fee", "dur": 5.7, "name": "recv (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:453)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746624.9, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_OverlappedFuture.add_done_callback"}, {"pid": 32672, "tid": 22768, "ts": 1072611746618.5, "ph": "X", "cat": "fee", "dur": 6.52, "name": "_loop_self_reading (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:772)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746618.4, "ph": "X", "cat": "fee", "dur": 6.699, "name": "_contextvars.Context.run"}, {"pid": 32672, "tid": 22768, "ts": 1072611746618.2, "ph": "X", "cat": "fee", "dur": 6.999, "name": "_run (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:78)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746605.1, "ph": "X", "cat": "fee", "dur": 20.4, "name": "_run_once (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1829)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746625.8, "ph": "X", "cat": "fee", "dur": 0.1, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072611746626.4, "ph": "X", "cat": "fee", "dur": 0.1, "name": "math.ceil"}, {"pid": 32672, "tid": 22768, "ts": 1072611746626.6, "ph": "X", "cat": "fee", "dur": 0.4, "name": "_overlapped.GetQueuedCompletionStatus"}, {"pid": 32672, "tid": 22768, "ts": 1072611746627.3, "ph": "X", "cat": "fee", "dur": 0.02, "name": "list.clear"}, {"pid": 32672, "tid": 22768, "ts": 1072611746626.299, "ph": "X", "cat": "fee", "dur": 1.1, "name": "_poll (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:779)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746626.1, "ph": "X", "cat": "fee", "dur": 1.6, "name": "select (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:437)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746627.8, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_process_events (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:861)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746628.199, "ph": "X", "cat": "fee", "dur": 0.02, "name": "time.monotonic"}, {"pid": 32672, "tid": 22768, "ts": 1072611746628.0, "ph": "X", "cat": "fee", "dur": 0.239, "name": "time (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:696)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746628.5, "ph": "X", "cat": "fee", "dur": 0.1, "name": "builtins.len"}, {"pid": 32672, "tid": 22768, "ts": 1072611746628.799, "ph": "X", "cat": "fee", "dur": 0.1, "name": "collections.deque.popleft"}, {"pid": 32672, "tid": 22768, "ts": 1072611746629.5, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_asyncio.Task.cancelled"}, {"pid": 32672, "tid": 22768, "ts": 1072611746629.7, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_asyncio.Task.exception"}, {"pid": 32672, "tid": 22768, "ts": 1072611746629.9, "ph": "X", "cat": "fee", "dur": 0.199, "name": "builtins.isinstance"}, {"pid": 32672, "tid": 22768, "ts": 1072611746630.5, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.get_loop"}, {"pid": 32672, "tid": 22768, "ts": 1072611746630.3, "ph": "X", "cat": "fee", "dur": 0.299, "name": "_get_loop (D:\\Program\\anaconda3\\lib\\asyncio\\futures.py:297)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746630.7, "ph": "X", "cat": "fee", "dur": 0.1, "name": "stop (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:651)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746629.399, "ph": "X", "cat": "fee", "dur": 1.5, "name": "_run_until_complete_cb (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:184)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746629.3, "ph": "X", "cat": "fee", "dur": 1.699, "name": "_contextvars.Context.run"}, {"pid": 32672, "tid": 22768, "ts": 1072611746629.1, "ph": "X", "cat": "fee", "dur": 1.92, "name": "_run (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:78)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746625.699, "ph": "X", "cat": "fee", "dur": 5.6, "name": "_run_once (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1829)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746631.6, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio._set_running_loop"}, {"pid": 32672, "tid": 22768, "ts": 1072611746631.799, "ph": "X", "cat": "fee", "dur": 0.2, "name": "_set_coroutine_origin_tracking (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1909)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746632.2, "ph": "X", "cat": "fee", "dur": 0.1, "name": "sys.set_asyncgen_hooks"}, {"pid": 32672, "tid": 22768, "ts": 1072611746602.299, "ph": "X", "cat": "fee", "dur": 30.1, "name": "run_forever (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:589)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746633.0, "ph": "X", "cat": "fee", "dur": 0.899, "name": "_overlapped.Overlapped.cancel"}, {"pid": 32672, "tid": 22768, "ts": 1072611746632.9, "ph": "X", "cat": "fee", "dur": 1.199, "name": "_cancel_overlapped (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:67)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746634.899, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746635.7, "ph": "X", "cat": "fee", "dur": 0.1, "name": "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746635.4, "ph": "X", "cat": "fee", "dur": 0.499, "name": "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:31)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746636.099, "ph": "X", "cat": "fee", "dur": 0.02, "name": "collections.deque.append"}, {"pid": 32672, "tid": 22768, "ts": 1072611746635.2, "ph": "X", "cat": "fee", "dur": 1.0, "name": "_call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:772)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746634.8, "ph": "X", "cat": "fee", "dur": 2.4, "name": "call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:743)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746634.5, "ph": "X", "cat": "fee", "dur": 2.799, "name": "_OverlappedFuture.cancel"}, {"pid": 32672, "tid": 22768, "ts": 1072611746632.7, "ph": "X", "cat": "fee", "dur": 4.699, "name": "cancel (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:83)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746637.7, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:423)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746637.999, "ph": "X", "cat": "fee", "dur": 0.02, "name": "list.append"}, {"pid": 32672, "tid": 22768, "ts": 1072611746637.6, "ph": "X", "cat": "fee", "dur": 0.5, "name": "_unregister (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:764)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746596.0, "ph": "X", "cat": "fee", "dur": 42.3, "name": "run_forever (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:317)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746638.4, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_asyncio.Task.remove_done_callback"}, {"pid": 32672, "tid": 22768, "ts": 1072611746638.599, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.done"}, {"pid": 32672, "tid": 22768, "ts": 1072611746638.8, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_asyncio.Task.result"}, {"pid": 32672, "tid": 22768, "ts": 1072611746583.7, "ph": "X", "cat": "fee", "dur": 55.2, "name": "run_until_complete (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:613)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746639.3, "ph": "X", "cat": "fee", "dur": 0.1, "name": "set.discard"}, {"pid": 32672, "tid": 22768, "ts": 1072611746639.0, "ph": "X", "cat": "fee", "dur": 0.42, "name": "_remove (D:\\Program\\anaconda3\\lib\\_weakrefset.py:39)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746640.6, "ph": "X", "cat": "fee", "dur": 0.399, "name": "get_event_loop_policy (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:751)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746641.5, "ph": "X", "cat": "fee", "dur": 1.3, "name": "set_event_loop (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:676)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746640.2, "ph": "X", "cat": "fee", "dur": 2.8, "name": "set_event_loop (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:791)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746644.5, "ph": "X", "cat": "fee", "dur": 0.1, "name": "is_running (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:692)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746645.0, "ph": "X", "cat": "fee", "dur": 0.2, "name": "is_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:682)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746646.8, "ph": "X", "cat": "fee", "dur": 0.1, "name": "_thread.get_ident"}, {"pid": 32672, "tid": 22768, "ts": 1072611746645.999, "ph": "X", "cat": "fee", "dur": 1.5, "name": "current_thread (D:\\Program\\anaconda3\\lib\\threading.py:1430)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746648.0, "ph": "X", "cat": "fee", "dur": 0.399, "name": "main_thread (D:\\Program\\anaconda3\\lib\\threading.py:1574)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746649.3, "ph": "X", "cat": "fee", "dur": 0.6, "name": "_signal.set_wakeup_fd"}, {"pid": 32672, "tid": 22768, "ts": 1072611746652.2, "ph": "X", "cat": "fee", "dur": 0.3, "name": "dict.values"}, {"pid": 32672, "tid": 22768, "ts": 1072611746653.4, "ph": "X", "cat": "fee", "dur": 0.1, "name": "dict.clear"}, {"pid": 32672, "tid": 22768, "ts": 1072611746651.2, "ph": "X", "cat": "fee", "dur": 2.32, "name": "_stop_accept_futures (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:865)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746657.2, "ph": "X", "cat": "fee", "dur": 60.4, "name": "socket.close"}, {"pid": 32672, "tid": 22768, "ts": 1072611746656.6, "ph": "X", "cat": "fee", "dur": 61.199, "name": "_real_close (D:\\Program\\anaconda3\\lib\\socket.py:494)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746654.9, "ph": "X", "cat": "fee", "dur": 63.1, "name": "close (D:\\Program\\anaconda3\\lib\\socket.py:498)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746718.9, "ph": "X", "cat": "fee", "dur": 59.699, "name": "socket.close"}, {"pid": 32672, "tid": 22768, "ts": 1072611746718.8, "ph": "X", "cat": "fee", "dur": 59.9, "name": "_real_close (D:\\Program\\anaconda3\\lib\\socket.py:494)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746718.5, "ph": "X", "cat": "fee", "dur": 60.3, "name": "close (D:\\Program\\anaconda3\\lib\\socket.py:498)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746654.1, "ph": "X", "cat": "fee", "dur": 125.8, "name": "_close_self_pipe (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:755)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746783.2, "ph": "X", "cat": "fee", "dur": 0.299, "name": "dict.items"}, {"pid": 32672, "tid": 22768, "ts": 1072611746785.399, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_OverlappedFuture.cancelled"}, {"pid": 32672, "tid": 22768, "ts": 1072611746785.9, "ph": "X", "cat": "fee", "dur": 0.099, "name": "time.monotonic"}, {"pid": 32672, "tid": 22768, "ts": 1072611746786.4, "ph": "X", "cat": "fee", "dur": 0.1, "name": "time.monotonic"}, {"pid": 32672, "tid": 22768, "ts": 1072611746787.4, "ph": "X", "cat": "fee", "dur": 0.2, "name": "math.ceil"}, {"pid": 32672, "tid": 22768, "ts": 1072611746787.9, "ph": "X", "cat": "fee", "dur": 4.199, "name": "_overlapped.GetQueuedCompletionStatus"}, {"pid": 32672, "tid": 22768, "ts": 1072611746792.5, "ph": "X", "cat": "fee", "dur": 0.199, "name": "dict.pop"}, {"pid": 32672, "tid": 22768, "ts": 1072611746793.1, "ph": "X", "cat": "fee", "dur": 0.6, "name": "__contains__ (D:\\Program\\anaconda3\\lib\\_weakrefset.py:75)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746793.999, "ph": "X", "cat": "fee", "dur": 0.02, "name": "_OverlappedFuture.done"}, {"pid": 32672, "tid": 22768, "ts": 1072611746794.2, "ph": "X", "cat": "fee", "dur": 0.5, "name": "_overlapped.GetQueuedCompletionStatus"}, {"pid": 32672, "tid": 22768, "ts": 1072611746795.199, "ph": "X", "cat": "fee", "dur": 0.02, "name": "dict.pop"}, {"pid": 32672, "tid": 22768, "ts": 1072611746795.4, "ph": "X", "cat": "fee", "dur": 0.1, "name": "list.clear"}, {"pid": 32672, "tid": 22768, "ts": 1072611746786.9, "ph": "X", "cat": "fee", "dur": 8.7, "name": "_poll (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:779)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746796.9, "ph": "X", "cat": "fee", "dur": 2.599, "name": "_winapi.CloseHandle"}, {"pid": 32672, "tid": 22768, "ts": 1072611746781.8, "ph": "X", "cat": "fee", "dur": 17.8, "name": "close (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:842)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746802.2, "ph": "X", "cat": "fee", "dur": 0.1, "name": "set.discard"}, {"pid": 32672, "tid": 22768, "ts": 1072611746801.7, "ph": "X", "cat": "fee", "dur": 0.7, "name": "_remove (D:\\Program\\anaconda3\\lib\\_weakrefset.py:39)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746804.8, "ph": "X", "cat": "fee", "dur": 0.1, "name": "close (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:842)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746804.6, "ph": "X", "cat": "fee", "dur": 0.5, "name": "__del__ (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:889)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746807.9, "ph": "X", "cat": "fee", "dur": 0.1, "name": "is_running (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:692)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746808.8, "ph": "X", "cat": "fee", "dur": 1.3, "name": "collections.deque.clear"}, {"pid": 32672, "tid": 22768, "ts": 1072611746810.3, "ph": "X", "cat": "fee", "dur": 0.02, "name": "list.clear"}, {"pid": 32672, "tid": 22768, "ts": 1072611746807.2, "ph": "X", "cat": "fee", "dur": 3.4, "name": "close (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:659)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746644.3, "ph": "X", "cat": "fee", "dur": 166.4, "name": "close (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:679)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737602.099, "ph": "X", "cat": "fee", "dur": 3009208.9, "name": "run (D:\\Program\\anaconda3\\lib\\asyncio\\runners.py:8)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746811.9, "ph": "X", "cat": "fee", "dur": 0.1, "name": "is_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:682)"}, {"pid": 32672, "tid": 22768, "ts": 1072611746811.6, "ph": "X", "cat": "fee", "dur": 0.499, "name": "__del__ (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:686)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737598.6, "ph": "X", "cat": "fee", "dur": 3009214.7, "name": "<module> (d:\\Program\\Code\\1_Programing_Language\\Python\\Learning\\Fluent_Python\\example2.py:1)"}, {"pid": 32672, "tid": 22768, "ts": 1072608737597.899, "ph": "X", "cat": "fee", "dur": 3009216.5, "name": "builtins.exec"}], "viztracer_metadata": {"overflow": false, "version": "0.15.6"}, "file_info": {"files": {"D:\\Program\\anaconda3\\lib\\asyncio\\coroutines.py": ["__all__ = 'coroutine', 'iscoroutinefunction', 'iscoroutine'\n\nimport collections.abc\nimport functools\nimport inspect\nimport os\nimport sys\nimport traceback\nimport types\nimport warnings\n\nfrom . import base_futures\nfrom . import constants\nfrom . import format_helpers\nfrom .log import logger\n\n\ndef _is_debug_mode():\n    # If you set _DEBUG to true, @coroutine will wrap the resulting\n    # generator objects in a CoroWrapper instance (defined below).  That\n    # instance will log a message when the generator is never iterated\n    # over, which may happen when you forget to use \"await\" or \"yield from\"\n    # with a coroutine call.\n    # Note that the value of the _DEBUG flag is taken\n    # when the decorator is used, so to be of any use it must be set\n    # before you define your coroutines.  A downside of using this feature\n    # is that tracebacks show entries for the CoroWrapper.__next__ method\n    # when _DEBUG is true.\n    return sys.flags.dev_mode or (not sys.flags.ignore_environment and\n                                  bool(os.environ.get('PYTHONASYNCIODEBUG')))\n\n\n_DEBUG = _is_debug_mode()\n\n\nclass CoroWrapper:\n    # Wrapper for coroutine object in _DEBUG mode.\n\n    def __init__(self, gen, func=None):\n        assert inspect.isgenerator(gen) or inspect.iscoroutine(gen), gen\n        self.gen = gen\n        self.func = func  # Used to unwrap @coroutine decorator\n        self._source_traceback = format_helpers.extract_stack(sys._getframe(1))\n        self.__name__ = getattr(gen, '__name__', None)\n        self.__qualname__ = getattr(gen, '__qualname__', None)\n\n    def __repr__(self):\n        coro_repr = _format_coroutine(self)\n        if self._source_traceback:\n            frame = self._source_traceback[-1]\n            coro_repr += f', created at {frame[0]}:{frame[1]}'\n\n        return f'<{self.__class__.__name__} {coro_repr}>'\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return self.gen.send(None)\n\n    def send(self, value):\n        return self.gen.send(value)\n\n    def throw(self, type, value=None, traceback=None):\n        return self.gen.throw(type, value, traceback)\n\n    def close(self):\n        return self.gen.close()\n\n    @property\n    def gi_frame(self):\n        return self.gen.gi_frame\n\n    @property\n    def gi_running(self):\n        return self.gen.gi_running\n\n    @property\n    def gi_code(self):\n        return self.gen.gi_code\n\n    def __await__(self):\n        return self\n\n    @property\n    def gi_yieldfrom(self):\n        return self.gen.gi_yieldfrom\n\n    def __del__(self):\n        # Be careful accessing self.gen.frame -- self.gen might not exist.\n        gen = getattr(self, 'gen', None)\n        frame = getattr(gen, 'gi_frame', None)\n        if frame is not None and frame.f_lasti == -1:\n            msg = f'{self!r} was never yielded from'\n            tb = getattr(self, '_source_traceback', ())\n            if tb:\n                tb = ''.join(traceback.format_list(tb))\n                msg += (f'\\nCoroutine object created at '\n                        f'(most recent call last, truncated to '\n                        f'{constants.DEBUG_STACK_DEPTH} last lines):\\n')\n                msg += tb.rstrip()\n            logger.error(msg)\n\n\ndef coroutine(func):\n    \"\"\"Decorator to mark coroutines.\n\n    If the coroutine is not yielded from before it is destroyed,\n    an error message is logged.\n    \"\"\"\n    warnings.warn('\"@coroutine\" decorator is deprecated since Python 3.8, use \"async def\" instead',\n                  DeprecationWarning,\n                  stacklevel=2)\n    if inspect.iscoroutinefunction(func):\n        # In Python 3.5 that's all we need to do for coroutines\n        # defined with \"async def\".\n        return func\n\n    if inspect.isgeneratorfunction(func):\n        coro = func\n    else:\n        @functools.wraps(func)\n        def coro(*args, **kw):\n            res = func(*args, **kw)\n            if (base_futures.isfuture(res) or inspect.isgenerator(res) or\n                    isinstance(res, CoroWrapper)):\n                res = yield from res\n            else:\n                # If 'res' is an awaitable, run it.\n                try:\n                    await_meth = res.__await__\n                except AttributeError:\n                    pass\n                else:\n                    if isinstance(res, collections.abc.Awaitable):\n                        res = yield from await_meth()\n            return res\n\n    coro = types.coroutine(coro)\n    if not _DEBUG:\n        wrapper = coro\n    else:\n        @functools.wraps(func)\n        def wrapper(*args, **kwds):\n            w = CoroWrapper(coro(*args, **kwds), func=func)\n            if w._source_traceback:\n                del w._source_traceback[-1]\n            # Python < 3.5 does not implement __qualname__\n            # on generator objects, so we set it manually.\n            # We use getattr as some callables (such as\n            # functools.partial may lack __qualname__).\n            w.__name__ = getattr(func, '__name__', None)\n            w.__qualname__ = getattr(func, '__qualname__', None)\n            return w\n\n    wrapper._is_coroutine = _is_coroutine  # For iscoroutinefunction().\n    return wrapper\n\n\n# A marker for iscoroutinefunction.\n_is_coroutine = object()\n\n\ndef iscoroutinefunction(func):\n    \"\"\"Return True if func is a decorated coroutine function.\"\"\"\n    return (inspect.iscoroutinefunction(func) or\n            getattr(func, '_is_coroutine', None) is _is_coroutine)\n\n\n# Prioritize native coroutine check to speed-up\n# asyncio.iscoroutine.\n_COROUTINE_TYPES = (types.CoroutineType, types.GeneratorType,\n                    collections.abc.Coroutine, CoroWrapper)\n_iscoroutine_typecache = set()\n\n\ndef iscoroutine(obj):\n    \"\"\"Return True if obj is a coroutine object.\"\"\"\n    if type(obj) in _iscoroutine_typecache:\n        return True\n\n    if isinstance(obj, _COROUTINE_TYPES):\n        # Just in case we don't want to cache more than 100\n        # positive types.  That shouldn't ever happen, unless\n        # someone stressing the system on purpose.\n        if len(_iscoroutine_typecache) < 100:\n            _iscoroutine_typecache.add(type(obj))\n        return True\n    else:\n        return False\n\n\ndef _format_coroutine(coro):\n    assert iscoroutine(coro)\n\n    is_corowrapper = isinstance(coro, CoroWrapper)\n\n    def get_name(coro):\n        # Coroutines compiled with Cython sometimes don't have\n        # proper __qualname__ or __name__.  While that is a bug\n        # in Cython, asyncio shouldn't crash with an AttributeError\n        # in its __repr__ functions.\n        if is_corowrapper:\n            return format_helpers._format_callback(coro.func, (), {})\n\n        if hasattr(coro, '__qualname__') and coro.__qualname__:\n            coro_name = coro.__qualname__\n        elif hasattr(coro, '__name__') and coro.__name__:\n            coro_name = coro.__name__\n        else:\n            # Stop masking Cython bugs, expose them in a friendly way.\n            coro_name = f'<{type(coro).__name__} without __name__>'\n        return f'{coro_name}()'\n\n    def is_running(coro):\n        try:\n            return coro.cr_running\n        except AttributeError:\n            try:\n                return coro.gi_running\n            except AttributeError:\n                return False\n\n    coro_code = None\n    if hasattr(coro, 'cr_code') and coro.cr_code:\n        coro_code = coro.cr_code\n    elif hasattr(coro, 'gi_code') and coro.gi_code:\n        coro_code = coro.gi_code\n\n    coro_name = get_name(coro)\n\n    if not coro_code:\n        # Built-in types might not have __qualname__ or __name__.\n        if is_running(coro):\n            return f'{coro_name} running'\n        else:\n            return coro_name\n\n    coro_frame = None\n    if hasattr(coro, 'gi_frame') and coro.gi_frame:\n        coro_frame = coro.gi_frame\n    elif hasattr(coro, 'cr_frame') and coro.cr_frame:\n        coro_frame = coro.cr_frame\n\n    # If Cython's coroutine has a fake code object without proper\n    # co_filename -- expose that.\n    filename = coro_code.co_filename or '<empty co_filename>'\n\n    lineno = 0\n    if (is_corowrapper and\n            coro.func is not None and\n            not inspect.isgeneratorfunction(coro.func)):\n        source = format_helpers._get_function_source(coro.func)\n        if source is not None:\n            filename, lineno = source\n        if coro_frame is None:\n            coro_repr = f'{coro_name} done, defined at {filename}:{lineno}'\n        else:\n            coro_repr = f'{coro_name} running, defined at {filename}:{lineno}'\n\n    elif coro_frame is not None:\n        lineno = coro_frame.f_lineno\n        coro_repr = f'{coro_name} running at {filename}:{lineno}'\n\n    else:\n        lineno = coro_code.co_firstlineno\n        coro_repr = f'{coro_name} done, defined at {filename}:{lineno}'\n\n    return coro_repr\n", 269], "D:\\Program\\anaconda3\\lib\\asyncio\\events.py": ["\"\"\"Event loop and event loop policy.\"\"\"\n\n__all__ = (\n    'AbstractEventLoopPolicy',\n    'AbstractEventLoop', 'AbstractServer',\n    'Handle', 'TimerHandle',\n    'get_event_loop_policy', 'set_event_loop_policy',\n    'get_event_loop', 'set_event_loop', 'new_event_loop',\n    'get_child_watcher', 'set_child_watcher',\n    '_set_running_loop', 'get_running_loop',\n    '_get_running_loop',\n)\n\nimport contextvars\nimport os\nimport socket\nimport subprocess\nimport sys\nimport threading\n\nfrom . import format_helpers\n\n\nclass Handle:\n    \"\"\"Object returned by callback registration methods.\"\"\"\n\n    __slots__ = ('_callback', '_args', '_cancelled', '_loop',\n                 '_source_traceback', '_repr', '__weakref__',\n                 '_context')\n\n    def __init__(self, callback, args, loop, context=None):\n        if context is None:\n            context = contextvars.copy_context()\n        self._context = context\n        self._loop = loop\n        self._callback = callback\n        self._args = args\n        self._cancelled = False\n        self._repr = None\n        if self._loop.get_debug():\n            self._source_traceback = format_helpers.extract_stack(\n                sys._getframe(1))\n        else:\n            self._source_traceback = None\n\n    def _repr_info(self):\n        info = [self.__class__.__name__]\n        if self._cancelled:\n            info.append('cancelled')\n        if self._callback is not None:\n            info.append(format_helpers._format_callback_source(\n                self._callback, self._args))\n        if self._source_traceback:\n            frame = self._source_traceback[-1]\n            info.append(f'created at {frame[0]}:{frame[1]}')\n        return info\n\n    def __repr__(self):\n        if self._repr is not None:\n            return self._repr\n        info = self._repr_info()\n        return '<{}>'.format(' '.join(info))\n\n    def cancel(self):\n        if not self._cancelled:\n            self._cancelled = True\n            if self._loop.get_debug():\n                # Keep a representation in debug mode to keep callback and\n                # parameters. For example, to log the warning\n                # \"Executing <Handle...> took 2.5 second\"\n                self._repr = repr(self)\n            self._callback = None\n            self._args = None\n\n    def cancelled(self):\n        return self._cancelled\n\n    def _run(self):\n        try:\n            self._context.run(self._callback, *self._args)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            cb = format_helpers._format_callback_source(\n                self._callback, self._args)\n            msg = f'Exception in callback {cb}'\n            context = {\n                'message': msg,\n                'exception': exc,\n                'handle': self,\n            }\n            if self._source_traceback:\n                context['source_traceback'] = self._source_traceback\n            self._loop.call_exception_handler(context)\n        self = None  # Needed to break cycles when an exception occurs.\n\n\nclass TimerHandle(Handle):\n    \"\"\"Object returned by timed callback registration methods.\"\"\"\n\n    __slots__ = ['_scheduled', '_when']\n\n    def __init__(self, when, callback, args, loop, context=None):\n        assert when is not None\n        super().__init__(callback, args, loop, context)\n        if self._source_traceback:\n            del self._source_traceback[-1]\n        self._when = when\n        self._scheduled = False\n\n    def _repr_info(self):\n        info = super()._repr_info()\n        pos = 2 if self._cancelled else 1\n        info.insert(pos, f'when={self._when}')\n        return info\n\n    def __hash__(self):\n        return hash(self._when)\n\n    def __lt__(self, other):\n        if isinstance(other, TimerHandle):\n            return self._when < other._when\n        return NotImplemented\n\n    def __le__(self, other):\n        if isinstance(other, TimerHandle):\n            return self._when < other._when or self.__eq__(other)\n        return NotImplemented\n\n    def __gt__(self, other):\n        if isinstance(other, TimerHandle):\n            return self._when > other._when\n        return NotImplemented\n\n    def __ge__(self, other):\n        if isinstance(other, TimerHandle):\n            return self._when > other._when or self.__eq__(other)\n        return NotImplemented\n\n    def __eq__(self, other):\n        if isinstance(other, TimerHandle):\n            return (self._when == other._when and\n                    self._callback == other._callback and\n                    self._args == other._args and\n                    self._cancelled == other._cancelled)\n        return NotImplemented\n\n    def cancel(self):\n        if not self._cancelled:\n            self._loop._timer_handle_cancelled(self)\n        super().cancel()\n\n    def when(self):\n        \"\"\"Return a scheduled callback time.\n\n        The time is an absolute timestamp, using the same time\n        reference as loop.time().\n        \"\"\"\n        return self._when\n\n\nclass AbstractServer:\n    \"\"\"Abstract server returned by create_server().\"\"\"\n\n    def close(self):\n        \"\"\"Stop serving.  This leaves existing connections open.\"\"\"\n        raise NotImplementedError\n\n    def get_loop(self):\n        \"\"\"Get the event loop the Server object is attached to.\"\"\"\n        raise NotImplementedError\n\n    def is_serving(self):\n        \"\"\"Return True if the server is accepting connections.\"\"\"\n        raise NotImplementedError\n\n    async def start_serving(self):\n        \"\"\"Start accepting connections.\n\n        This method is idempotent, so it can be called when\n        the server is already being serving.\n        \"\"\"\n        raise NotImplementedError\n\n    async def serve_forever(self):\n        \"\"\"Start accepting connections until the coroutine is cancelled.\n\n        The server is closed when the coroutine is cancelled.\n        \"\"\"\n        raise NotImplementedError\n\n    async def wait_closed(self):\n        \"\"\"Coroutine to wait until service is closed.\"\"\"\n        raise NotImplementedError\n\n    async def __aenter__(self):\n        return self\n\n    async def __aexit__(self, *exc):\n        self.close()\n        await self.wait_closed()\n\n\nclass AbstractEventLoop:\n    \"\"\"Abstract event loop.\"\"\"\n\n    # Running and stopping the event loop.\n\n    def run_forever(self):\n        \"\"\"Run the event loop until stop() is called.\"\"\"\n        raise NotImplementedError\n\n    def run_until_complete(self, future):\n        \"\"\"Run the event loop until a Future is done.\n\n        Return the Future's result, or raise its exception.\n        \"\"\"\n        raise NotImplementedError\n\n    def stop(self):\n        \"\"\"Stop the event loop as soon as reasonable.\n\n        Exactly how soon that is may depend on the implementation, but\n        no more I/O callbacks should be scheduled.\n        \"\"\"\n        raise NotImplementedError\n\n    def is_running(self):\n        \"\"\"Return whether the event loop is currently running.\"\"\"\n        raise NotImplementedError\n\n    def is_closed(self):\n        \"\"\"Returns True if the event loop was closed.\"\"\"\n        raise NotImplementedError\n\n    def close(self):\n        \"\"\"Close the loop.\n\n        The loop should not be running.\n\n        This is idempotent and irreversible.\n\n        No other methods should be called after this one.\n        \"\"\"\n        raise NotImplementedError\n\n    async def shutdown_asyncgens(self):\n        \"\"\"Shutdown all active asynchronous generators.\"\"\"\n        raise NotImplementedError\n\n    async def shutdown_default_executor(self):\n        \"\"\"Schedule the shutdown of the default executor.\"\"\"\n        raise NotImplementedError\n\n    # Methods scheduling callbacks.  All these return Handles.\n\n    def _timer_handle_cancelled(self, handle):\n        \"\"\"Notification that a TimerHandle has been cancelled.\"\"\"\n        raise NotImplementedError\n\n    def call_soon(self, callback, *args, context=None):\n        return self.call_later(0, callback, *args, context=context)\n\n    def call_later(self, delay, callback, *args, context=None):\n        raise NotImplementedError\n\n    def call_at(self, when, callback, *args, context=None):\n        raise NotImplementedError\n\n    def time(self):\n        raise NotImplementedError\n\n    def create_future(self):\n        raise NotImplementedError\n\n    # Method scheduling a coroutine object: create a task.\n\n    def create_task(self, coro, *, name=None):\n        raise NotImplementedError\n\n    # Methods for interacting with threads.\n\n    def call_soon_threadsafe(self, callback, *args, context=None):\n        raise NotImplementedError\n\n    def run_in_executor(self, executor, func, *args):\n        raise NotImplementedError\n\n    def set_default_executor(self, executor):\n        raise NotImplementedError\n\n    # Network I/O methods returning Futures.\n\n    async def getaddrinfo(self, host, port, *,\n                          family=0, type=0, proto=0, flags=0):\n        raise NotImplementedError\n\n    async def getnameinfo(self, sockaddr, flags=0):\n        raise NotImplementedError\n\n    async def create_connection(\n            self, protocol_factory, host=None, port=None,\n            *, ssl=None, family=0, proto=0,\n            flags=0, sock=None, local_addr=None,\n            server_hostname=None,\n            ssl_handshake_timeout=None,\n            happy_eyeballs_delay=None, interleave=None):\n        raise NotImplementedError\n\n    async def create_server(\n            self, protocol_factory, host=None, port=None,\n            *, family=socket.AF_UNSPEC,\n            flags=socket.AI_PASSIVE, sock=None, backlog=100,\n            ssl=None, reuse_address=None, reuse_port=None,\n            ssl_handshake_timeout=None,\n            start_serving=True):\n        \"\"\"A coroutine which creates a TCP server bound to host and port.\n\n        The return value is a Server object which can be used to stop\n        the service.\n\n        If host is an empty string or None all interfaces are assumed\n        and a list of multiple sockets will be returned (most likely\n        one for IPv4 and another one for IPv6). The host parameter can also be\n        a sequence (e.g. list) of hosts to bind to.\n\n        family can be set to either AF_INET or AF_INET6 to force the\n        socket to use IPv4 or IPv6. If not set it will be determined\n        from host (defaults to AF_UNSPEC).\n\n        flags is a bitmask for getaddrinfo().\n\n        sock can optionally be specified in order to use a preexisting\n        socket object.\n\n        backlog is the maximum number of queued connections passed to\n        listen() (defaults to 100).\n\n        ssl can be set to an SSLContext to enable SSL over the\n        accepted connections.\n\n        reuse_address tells the kernel to reuse a local socket in\n        TIME_WAIT state, without waiting for its natural timeout to\n        expire. If not specified will automatically be set to True on\n        UNIX.\n\n        reuse_port tells the kernel to allow this endpoint to be bound to\n        the same port as other existing endpoints are bound to, so long as\n        they all set this flag when being created. This option is not\n        supported on Windows.\n\n        ssl_handshake_timeout is the time in seconds that an SSL server\n        will wait for completion of the SSL handshake before aborting the\n        connection. Default is 60s.\n\n        start_serving set to True (default) causes the created server\n        to start accepting connections immediately.  When set to False,\n        the user should await Server.start_serving() or Server.serve_forever()\n        to make the server to start accepting connections.\n        \"\"\"\n        raise NotImplementedError\n\n    async def sendfile(self, transport, file, offset=0, count=None,\n                       *, fallback=True):\n        \"\"\"Send a file through a transport.\n\n        Return an amount of sent bytes.\n        \"\"\"\n        raise NotImplementedError\n\n    async def start_tls(self, transport, protocol, sslcontext, *,\n                        server_side=False,\n                        server_hostname=None,\n                        ssl_handshake_timeout=None):\n        \"\"\"Upgrade a transport to TLS.\n\n        Return a new transport that *protocol* should start using\n        immediately.\n        \"\"\"\n        raise NotImplementedError\n\n    async def create_unix_connection(\n            self, protocol_factory, path=None, *,\n            ssl=None, sock=None,\n            server_hostname=None,\n            ssl_handshake_timeout=None):\n        raise NotImplementedError\n\n    async def create_unix_server(\n            self, protocol_factory, path=None, *,\n            sock=None, backlog=100, ssl=None,\n            ssl_handshake_timeout=None,\n            start_serving=True):\n        \"\"\"A coroutine which creates a UNIX Domain Socket server.\n\n        The return value is a Server object, which can be used to stop\n        the service.\n\n        path is a str, representing a file system path to bind the\n        server socket to.\n\n        sock can optionally be specified in order to use a preexisting\n        socket object.\n\n        backlog is the maximum number of queued connections passed to\n        listen() (defaults to 100).\n\n        ssl can be set to an SSLContext to enable SSL over the\n        accepted connections.\n\n        ssl_handshake_timeout is the time in seconds that an SSL server\n        will wait for the SSL handshake to complete (defaults to 60s).\n\n        start_serving set to True (default) causes the created server\n        to start accepting connections immediately.  When set to False,\n        the user should await Server.start_serving() or Server.serve_forever()\n        to make the server to start accepting connections.\n        \"\"\"\n        raise NotImplementedError\n\n    async def connect_accepted_socket(\n            self, protocol_factory, sock,\n            *, ssl=None,\n            ssl_handshake_timeout=None):\n        \"\"\"Handle an accepted connection.\n\n        This is used by servers that accept connections outside of\n        asyncio, but use asyncio to handle connections.\n\n        This method is a coroutine.  When completed, the coroutine\n        returns a (transport, protocol) pair.\n        \"\"\"\n        raise NotImplementedError\n\n    async def create_datagram_endpoint(self, protocol_factory,\n                                       local_addr=None, remote_addr=None, *,\n                                       family=0, proto=0, flags=0,\n                                       reuse_address=None, reuse_port=None,\n                                       allow_broadcast=None, sock=None):\n        \"\"\"A coroutine which creates a datagram endpoint.\n\n        This method will try to establish the endpoint in the background.\n        When successful, the coroutine returns a (transport, protocol) pair.\n\n        protocol_factory must be a callable returning a protocol instance.\n\n        socket family AF_INET, socket.AF_INET6 or socket.AF_UNIX depending on\n        host (or family if specified), socket type SOCK_DGRAM.\n\n        reuse_address tells the kernel to reuse a local socket in\n        TIME_WAIT state, without waiting for its natural timeout to\n        expire. If not specified it will automatically be set to True on\n        UNIX.\n\n        reuse_port tells the kernel to allow this endpoint to be bound to\n        the same port as other existing endpoints are bound to, so long as\n        they all set this flag when being created. This option is not\n        supported on Windows and some UNIX's. If the\n        :py:data:`~socket.SO_REUSEPORT` constant is not defined then this\n        capability is unsupported.\n\n        allow_broadcast tells the kernel to allow this endpoint to send\n        messages to the broadcast address.\n\n        sock can optionally be specified in order to use a preexisting\n        socket object.\n        \"\"\"\n        raise NotImplementedError\n\n    # Pipes and subprocesses.\n\n    async def connect_read_pipe(self, protocol_factory, pipe):\n        \"\"\"Register read pipe in event loop. Set the pipe to non-blocking mode.\n\n        protocol_factory should instantiate object with Protocol interface.\n        pipe is a file-like object.\n        Return pair (transport, protocol), where transport supports the\n        ReadTransport interface.\"\"\"\n        # The reason to accept file-like object instead of just file descriptor\n        # is: we need to own pipe and close it at transport finishing\n        # Can got complicated errors if pass f.fileno(),\n        # close fd in pipe transport then close f and vice versa.\n        raise NotImplementedError\n\n    async def connect_write_pipe(self, protocol_factory, pipe):\n        \"\"\"Register write pipe in event loop.\n\n        protocol_factory should instantiate object with BaseProtocol interface.\n        Pipe is file-like object already switched to nonblocking.\n        Return pair (transport, protocol), where transport support\n        WriteTransport interface.\"\"\"\n        # The reason to accept file-like object instead of just file descriptor\n        # is: we need to own pipe and close it at transport finishing\n        # Can got complicated errors if pass f.fileno(),\n        # close fd in pipe transport then close f and vice versa.\n        raise NotImplementedError\n\n    async def subprocess_shell(self, protocol_factory, cmd, *,\n                               stdin=subprocess.PIPE,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE,\n                               **kwargs):\n        raise NotImplementedError\n\n    async def subprocess_exec(self, protocol_factory, *args,\n                              stdin=subprocess.PIPE,\n                              stdout=subprocess.PIPE,\n                              stderr=subprocess.PIPE,\n                              **kwargs):\n        raise NotImplementedError\n\n    # Ready-based callback registration methods.\n    # The add_*() methods return None.\n    # The remove_*() methods return True if something was removed,\n    # False if there was nothing to delete.\n\n    def add_reader(self, fd, callback, *args):\n        raise NotImplementedError\n\n    def remove_reader(self, fd):\n        raise NotImplementedError\n\n    def add_writer(self, fd, callback, *args):\n        raise NotImplementedError\n\n    def remove_writer(self, fd):\n        raise NotImplementedError\n\n    # Completion based I/O methods returning Futures.\n\n    async def sock_recv(self, sock, nbytes):\n        raise NotImplementedError\n\n    async def sock_recv_into(self, sock, buf):\n        raise NotImplementedError\n\n    async def sock_sendall(self, sock, data):\n        raise NotImplementedError\n\n    async def sock_connect(self, sock, address):\n        raise NotImplementedError\n\n    async def sock_accept(self, sock):\n        raise NotImplementedError\n\n    async def sock_sendfile(self, sock, file, offset=0, count=None,\n                            *, fallback=None):\n        raise NotImplementedError\n\n    # Signal handling.\n\n    def add_signal_handler(self, sig, callback, *args):\n        raise NotImplementedError\n\n    def remove_signal_handler(self, sig):\n        raise NotImplementedError\n\n    # Task factory.\n\n    def set_task_factory(self, factory):\n        raise NotImplementedError\n\n    def get_task_factory(self):\n        raise NotImplementedError\n\n    # Error handlers.\n\n    def get_exception_handler(self):\n        raise NotImplementedError\n\n    def set_exception_handler(self, handler):\n        raise NotImplementedError\n\n    def default_exception_handler(self, context):\n        raise NotImplementedError\n\n    def call_exception_handler(self, context):\n        raise NotImplementedError\n\n    # Debug flag management.\n\n    def get_debug(self):\n        raise NotImplementedError\n\n    def set_debug(self, enabled):\n        raise NotImplementedError\n\n\nclass AbstractEventLoopPolicy:\n    \"\"\"Abstract policy for accessing the event loop.\"\"\"\n\n    def get_event_loop(self):\n        \"\"\"Get the event loop for the current context.\n\n        Returns an event loop object implementing the BaseEventLoop interface,\n        or raises an exception in case no event loop has been set for the\n        current context and the current policy does not specify to create one.\n\n        It should never return None.\"\"\"\n        raise NotImplementedError\n\n    def set_event_loop(self, loop):\n        \"\"\"Set the event loop for the current context to loop.\"\"\"\n        raise NotImplementedError\n\n    def new_event_loop(self):\n        \"\"\"Create and return a new event loop object according to this\n        policy's rules. If there's need to set this loop as the event loop for\n        the current context, set_event_loop must be called explicitly.\"\"\"\n        raise NotImplementedError\n\n    # Child processes handling (Unix only).\n\n    def get_child_watcher(self):\n        \"Get the watcher for child processes.\"\n        raise NotImplementedError\n\n    def set_child_watcher(self, watcher):\n        \"\"\"Set the watcher for child processes.\"\"\"\n        raise NotImplementedError\n\n\nclass BaseDefaultEventLoopPolicy(AbstractEventLoopPolicy):\n    \"\"\"Default policy implementation for accessing the event loop.\n\n    In this policy, each thread has its own event loop.  However, we\n    only automatically create an event loop by default for the main\n    thread; other threads by default have no event loop.\n\n    Other policies may have different rules (e.g. a single global\n    event loop, or automatically creating an event loop per thread, or\n    using some other notion of context to which an event loop is\n    associated).\n    \"\"\"\n\n    _loop_factory = None\n\n    class _Local(threading.local):\n        _loop = None\n        _set_called = False\n\n    def __init__(self):\n        self._local = self._Local()\n\n    def get_event_loop(self):\n        \"\"\"Get the event loop for the current context.\n\n        Returns an instance of EventLoop or raises an exception.\n        \"\"\"\n        if (self._local._loop is None and\n                not self._local._set_called and\n                threading.current_thread() is threading.main_thread()):\n            stacklevel = 2\n            try:\n                f = sys._getframe(1)\n            except AttributeError:\n                pass\n            else:\n                while f:\n                    module = f.f_globals.get('__name__')\n                    if not (module == 'asyncio' or module.startswith('asyncio.')):\n                        break\n                    f = f.f_back\n                    stacklevel += 1\n            import warnings\n            warnings.warn('There is no current event loop',\n                          DeprecationWarning, stacklevel=stacklevel)\n            self.set_event_loop(self.new_event_loop())\n\n        if self._local._loop is None:\n            raise RuntimeError('There is no current event loop in thread %r.'\n                               % threading.current_thread().name)\n\n        return self._local._loop\n\n    def set_event_loop(self, loop):\n        \"\"\"Set the event loop.\"\"\"\n        self._local._set_called = True\n        assert loop is None or isinstance(loop, AbstractEventLoop)\n        self._local._loop = loop\n\n    def new_event_loop(self):\n        \"\"\"Create a new event loop.\n\n        You must call set_event_loop() to make this the current event\n        loop.\n        \"\"\"\n        return self._loop_factory()\n\n\n# Event loop policy.  The policy itself is always global, even if the\n# policy's rules say that there is an event loop per thread (or other\n# notion of context).  The default policy is installed by the first\n# call to get_event_loop_policy().\n_event_loop_policy = None\n\n# Lock for protecting the on-the-fly creation of the event loop policy.\n_lock = threading.Lock()\n\n\n# A TLS for the running event loop, used by _get_running_loop.\nclass _RunningLoop(threading.local):\n    loop_pid = (None, None)\n\n\n_running_loop = _RunningLoop()\n\n\ndef get_running_loop():\n    \"\"\"Return the running event loop.  Raise a RuntimeError if there is none.\n\n    This function is thread-specific.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    loop = _get_running_loop()\n    if loop is None:\n        raise RuntimeError('no running event loop')\n    return loop\n\n\ndef _get_running_loop():\n    \"\"\"Return the running event loop or None.\n\n    This is a low-level function intended to be used by event loops.\n    This function is thread-specific.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    running_loop, pid = _running_loop.loop_pid\n    if running_loop is not None and pid == os.getpid():\n        return running_loop\n\n\ndef _set_running_loop(loop):\n    \"\"\"Set the running event loop.\n\n    This is a low-level function intended to be used by event loops.\n    This function is thread-specific.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    _running_loop.loop_pid = (loop, os.getpid())\n\n\ndef _init_event_loop_policy():\n    global _event_loop_policy\n    with _lock:\n        if _event_loop_policy is None:  # pragma: no branch\n            from . import DefaultEventLoopPolicy\n            _event_loop_policy = DefaultEventLoopPolicy()\n\n\ndef get_event_loop_policy():\n    \"\"\"Get the current event loop policy.\"\"\"\n    if _event_loop_policy is None:\n        _init_event_loop_policy()\n    return _event_loop_policy\n\n\ndef set_event_loop_policy(policy):\n    \"\"\"Set the current event loop policy.\n\n    If policy is None, the default policy is restored.\"\"\"\n    global _event_loop_policy\n    assert policy is None or isinstance(policy, AbstractEventLoopPolicy)\n    _event_loop_policy = policy\n\n\ndef get_event_loop():\n    \"\"\"Return an asyncio event loop.\n\n    When called from a coroutine or a callback (e.g. scheduled with call_soon\n    or similar API), this function will always return the running event loop.\n\n    If there is no running event loop set, the function will return\n    the result of `get_event_loop_policy().get_event_loop()` call.\n    \"\"\"\n    # NOTE: this function is implemented in C (see _asynciomodule.c)\n    return _py__get_event_loop()\n\n\ndef _get_event_loop(stacklevel=3):\n    # This internal method is going away in Python 3.12, left here only for\n    # backwards compatibility with 3.10.0 - 3.10.8 and 3.11.0.\n    # Similarly, this method's C equivalent in _asyncio is going away as well.\n    # See GH-99949 for more details.\n    current_loop = _get_running_loop()\n    if current_loop is not None:\n        return current_loop\n    return get_event_loop_policy().get_event_loop()\n\n\ndef set_event_loop(loop):\n    \"\"\"Equivalent to calling get_event_loop_policy().set_event_loop(loop).\"\"\"\n    get_event_loop_policy().set_event_loop(loop)\n\n\ndef new_event_loop():\n    \"\"\"Equivalent to calling get_event_loop_policy().new_event_loop().\"\"\"\n    return get_event_loop_policy().new_event_loop()\n\n\ndef get_child_watcher():\n    \"\"\"Equivalent to calling get_event_loop_policy().get_child_watcher().\"\"\"\n    return get_event_loop_policy().get_child_watcher()\n\n\ndef set_child_watcher(watcher):\n    \"\"\"Equivalent to calling\n    get_event_loop_policy().set_child_watcher(watcher).\"\"\"\n    return get_event_loop_policy().set_child_watcher(watcher)\n\n\n# Alias pure-Python implementations for testing purposes.\n_py__get_running_loop = _get_running_loop\n_py__set_running_loop = _set_running_loop\n_py_get_running_loop = get_running_loop\n_py_get_event_loop = get_event_loop\n_py__get_event_loop = _get_event_loop\n\n\ntry:\n    # get_event_loop() is one of the most frequently called\n    # functions in asyncio.  Pure Python implementation is\n    # about 4 times slower than C-accelerated.\n    from _asyncio import (_get_running_loop, _set_running_loop,\n                          get_running_loop, get_event_loop, _get_event_loop)\nexcept ImportError:\n    pass\nelse:\n    # Alias C implementations for testing purposes.\n    _c__get_running_loop = _get_running_loop\n    _c__set_running_loop = _set_running_loop\n    _c_get_running_loop = get_running_loop\n    _c_get_event_loop = get_event_loop\n    _c__get_event_loop = _get_event_loop\n", 834], "D:\\Program\\anaconda3\\lib\\_weakrefset.py": ["# Access WeakSet through the weakref module.\n# This code is separated-out because it is needed\n# by abc.py to load everything else at startup.\n\nfrom _weakref import ref\nfrom types import GenericAlias\n\n__all__ = ['WeakSet']\n\n\nclass _IterationGuard:\n    # This context manager registers itself in the current iterators of the\n    # weak container, such as to delay all removals until the context manager\n    # exits.\n    # This technique should be relatively thread-safe (since sets are).\n\n    def __init__(self, weakcontainer):\n        # Don't create cycles\n        self.weakcontainer = ref(weakcontainer)\n\n    def __enter__(self):\n        w = self.weakcontainer()\n        if w is not None:\n            w._iterating.add(self)\n        return self\n\n    def __exit__(self, e, t, b):\n        w = self.weakcontainer()\n        if w is not None:\n            s = w._iterating\n            s.remove(self)\n            if not s:\n                w._commit_removals()\n\n\nclass WeakSet:\n    def __init__(self, data=None):\n        self.data = set()\n        def _remove(item, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(item)\n                else:\n                    self.data.discard(item)\n        self._remove = _remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        if data is not None:\n            self.update(data)\n\n    def _commit_removals(self):\n        pop = self._pending_removals.pop\n        discard = self.data.discard\n        while True:\n            try:\n                item = pop()\n            except IndexError:\n                return\n            discard(item)\n\n    def __iter__(self):\n        with _IterationGuard(self):\n            for itemref in self.data:\n                item = itemref()\n                if item is not None:\n                    # Caveat: the iterator will keep a strong reference to\n                    # `item` until it is resumed or closed.\n                    yield item\n\n    def __len__(self):\n        return len(self.data) - len(self._pending_removals)\n\n    def __contains__(self, item):\n        try:\n            wr = ref(item)\n        except TypeError:\n            return False\n        return wr in self.data\n\n    def __reduce__(self):\n        return (self.__class__, (list(self),),\n                getattr(self, '__dict__', None))\n\n    def add(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.add(ref(item, self._remove))\n\n    def clear(self):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.clear()\n\n    def copy(self):\n        return self.__class__(self)\n\n    def pop(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while True:\n            try:\n                itemref = self.data.pop()\n            except KeyError:\n                raise KeyError('pop from empty WeakSet') from None\n            item = itemref()\n            if item is not None:\n                return item\n\n    def remove(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.remove(ref(item))\n\n    def discard(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.discard(ref(item))\n\n    def update(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        for element in other:\n            self.add(element)\n\n    def __ior__(self, other):\n        self.update(other)\n        return self\n\n    def difference(self, other):\n        newset = self.copy()\n        newset.difference_update(other)\n        return newset\n    __sub__ = difference\n\n    def difference_update(self, other):\n        self.__isub__(other)\n    def __isub__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.difference_update(ref(item) for item in other)\n        return self\n\n    def intersection(self, other):\n        return self.__class__(item for item in other if item in self)\n    __and__ = intersection\n\n    def intersection_update(self, other):\n        self.__iand__(other)\n    def __iand__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.intersection_update(ref(item) for item in other)\n        return self\n\n    def issubset(self, other):\n        return self.data.issubset(ref(item) for item in other)\n    __le__ = issubset\n\n    def __lt__(self, other):\n        return self.data < set(map(ref, other))\n\n    def issuperset(self, other):\n        return self.data.issuperset(ref(item) for item in other)\n    __ge__ = issuperset\n\n    def __gt__(self, other):\n        return self.data > set(map(ref, other))\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return self.data == set(map(ref, other))\n\n    def symmetric_difference(self, other):\n        newset = self.copy()\n        newset.symmetric_difference_update(other)\n        return newset\n    __xor__ = symmetric_difference\n\n    def symmetric_difference_update(self, other):\n        self.__ixor__(other)\n    def __ixor__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.symmetric_difference_update(ref(item, self._remove) for item in other)\n        return self\n\n    def union(self, other):\n        return self.__class__(e for s in (self, other) for e in s)\n    __or__ = union\n\n    def isdisjoint(self, other):\n        return len(self.intersection(other)) == 0\n\n    def __repr__(self):\n        return repr(self.data)\n\n    __class_getitem__ = classmethod(GenericAlias)\n", 206], "D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py": ["\"\"\"Selector and proactor event loops for Windows.\"\"\"\n\nimport sys\n\nif sys.platform != 'win32':  # pragma: no cover\n    raise ImportError('win32 only')\n\nimport _overlapped\nimport _winapi\nimport errno\nimport math\nimport msvcrt\nimport socket\nimport struct\nimport time\nimport weakref\n\nfrom . import events\nfrom . import base_subprocess\nfrom . import futures\nfrom . import exceptions\nfrom . import proactor_events\nfrom . import selector_events\nfrom . import tasks\nfrom . import windows_utils\nfrom .log import logger\n\n\n__all__ = (\n    'SelectorEventLoop', 'ProactorEventLoop', 'IocpProactor',\n    'DefaultEventLoopPolicy', 'WindowsSelectorEventLoopPolicy',\n    'WindowsProactorEventLoopPolicy',\n)\n\n\nNULL = 0\nINFINITE = 0xffffffff\nERROR_CONNECTION_REFUSED = 1225\nERROR_CONNECTION_ABORTED = 1236\n\n# Initial delay in seconds for connect_pipe() before retrying to connect\nCONNECT_PIPE_INIT_DELAY = 0.001\n\n# Maximum delay in seconds for connect_pipe() before retrying to connect\nCONNECT_PIPE_MAX_DELAY = 0.100\n\n\nclass _OverlappedFuture(futures.Future):\n    \"\"\"Subclass of Future which represents an overlapped operation.\n\n    Cancelling it will immediately cancel the overlapped operation.\n    \"\"\"\n\n    def __init__(self, ov, *, loop=None):\n        super().__init__(loop=loop)\n        if self._source_traceback:\n            del self._source_traceback[-1]\n        self._ov = ov\n\n    def _repr_info(self):\n        info = super()._repr_info()\n        if self._ov is not None:\n            state = 'pending' if self._ov.pending else 'completed'\n            info.insert(1, f'overlapped=<{state}, {self._ov.address:#x}>')\n        return info\n\n    def _cancel_overlapped(self):\n        if self._ov is None:\n            return\n        try:\n            self._ov.cancel()\n        except OSError as exc:\n            context = {\n                'message': 'Cancelling an overlapped future failed',\n                'exception': exc,\n                'future': self,\n            }\n            if self._source_traceback:\n                context['source_traceback'] = self._source_traceback\n            self._loop.call_exception_handler(context)\n        self._ov = None\n\n    def cancel(self, msg=None):\n        self._cancel_overlapped()\n        return super().cancel(msg=msg)\n\n    def set_exception(self, exception):\n        super().set_exception(exception)\n        self._cancel_overlapped()\n\n    def set_result(self, result):\n        super().set_result(result)\n        self._ov = None\n\n\nclass _BaseWaitHandleFuture(futures.Future):\n    \"\"\"Subclass of Future which represents a wait handle.\"\"\"\n\n    def __init__(self, ov, handle, wait_handle, *, loop=None):\n        super().__init__(loop=loop)\n        if self._source_traceback:\n            del self._source_traceback[-1]\n        # Keep a reference to the Overlapped object to keep it alive until the\n        # wait is unregistered\n        self._ov = ov\n        self._handle = handle\n        self._wait_handle = wait_handle\n\n        # Should we call UnregisterWaitEx() if the wait completes\n        # or is cancelled?\n        self._registered = True\n\n    def _poll(self):\n        # non-blocking wait: use a timeout of 0 millisecond\n        return (_winapi.WaitForSingleObject(self._handle, 0) ==\n                _winapi.WAIT_OBJECT_0)\n\n    def _repr_info(self):\n        info = super()._repr_info()\n        info.append(f'handle={self._handle:#x}')\n        if self._handle is not None:\n            state = 'signaled' if self._poll() else 'waiting'\n            info.append(state)\n        if self._wait_handle is not None:\n            info.append(f'wait_handle={self._wait_handle:#x}')\n        return info\n\n    def _unregister_wait_cb(self, fut):\n        # The wait was unregistered: it's not safe to destroy the Overlapped\n        # object\n        self._ov = None\n\n    def _unregister_wait(self):\n        if not self._registered:\n            return\n        self._registered = False\n\n        wait_handle = self._wait_handle\n        self._wait_handle = None\n        try:\n            _overlapped.UnregisterWait(wait_handle)\n        except OSError as exc:\n            if exc.winerror != _overlapped.ERROR_IO_PENDING:\n                context = {\n                    'message': 'Failed to unregister the wait handle',\n                    'exception': exc,\n                    'future': self,\n                }\n                if self._source_traceback:\n                    context['source_traceback'] = self._source_traceback\n                self._loop.call_exception_handler(context)\n                return\n            # ERROR_IO_PENDING means that the unregister is pending\n\n        self._unregister_wait_cb(None)\n\n    def cancel(self, msg=None):\n        self._unregister_wait()\n        return super().cancel(msg=msg)\n\n    def set_exception(self, exception):\n        self._unregister_wait()\n        super().set_exception(exception)\n\n    def set_result(self, result):\n        self._unregister_wait()\n        super().set_result(result)\n\n\nclass _WaitCancelFuture(_BaseWaitHandleFuture):\n    \"\"\"Subclass of Future which represents a wait for the cancellation of a\n    _WaitHandleFuture using an event.\n    \"\"\"\n\n    def __init__(self, ov, event, wait_handle, *, loop=None):\n        super().__init__(ov, event, wait_handle, loop=loop)\n\n        self._done_callback = None\n\n    def cancel(self):\n        raise RuntimeError(\"_WaitCancelFuture must not be cancelled\")\n\n    def set_result(self, result):\n        super().set_result(result)\n        if self._done_callback is not None:\n            self._done_callback(self)\n\n    def set_exception(self, exception):\n        super().set_exception(exception)\n        if self._done_callback is not None:\n            self._done_callback(self)\n\n\nclass _WaitHandleFuture(_BaseWaitHandleFuture):\n    def __init__(self, ov, handle, wait_handle, proactor, *, loop=None):\n        super().__init__(ov, handle, wait_handle, loop=loop)\n        self._proactor = proactor\n        self._unregister_proactor = True\n        self._event = _overlapped.CreateEvent(None, True, False, None)\n        self._event_fut = None\n\n    def _unregister_wait_cb(self, fut):\n        if self._event is not None:\n            _winapi.CloseHandle(self._event)\n            self._event = None\n            self._event_fut = None\n\n        # If the wait was cancelled, the wait may never be signalled, so\n        # it's required to unregister it. Otherwise, IocpProactor.close() will\n        # wait forever for an event which will never come.\n        #\n        # If the IocpProactor already received the event, it's safe to call\n        # _unregister() because we kept a reference to the Overlapped object\n        # which is used as a unique key.\n        self._proactor._unregister(self._ov)\n        self._proactor = None\n\n        super()._unregister_wait_cb(fut)\n\n    def _unregister_wait(self):\n        if not self._registered:\n            return\n        self._registered = False\n\n        wait_handle = self._wait_handle\n        self._wait_handle = None\n        try:\n            _overlapped.UnregisterWaitEx(wait_handle, self._event)\n        except OSError as exc:\n            if exc.winerror != _overlapped.ERROR_IO_PENDING:\n                context = {\n                    'message': 'Failed to unregister the wait handle',\n                    'exception': exc,\n                    'future': self,\n                }\n                if self._source_traceback:\n                    context['source_traceback'] = self._source_traceback\n                self._loop.call_exception_handler(context)\n                return\n            # ERROR_IO_PENDING is not an error, the wait was unregistered\n\n        self._event_fut = self._proactor._wait_cancel(self._event,\n                                                      self._unregister_wait_cb)\n\n\nclass PipeServer(object):\n    \"\"\"Class representing a pipe server.\n\n    This is much like a bound, listening socket.\n    \"\"\"\n    def __init__(self, address):\n        self._address = address\n        self._free_instances = weakref.WeakSet()\n        # initialize the pipe attribute before calling _server_pipe_handle()\n        # because this function can raise an exception and the destructor calls\n        # the close() method\n        self._pipe = None\n        self._accept_pipe_future = None\n        self._pipe = self._server_pipe_handle(True)\n\n    def _get_unconnected_pipe(self):\n        # Create new instance and return previous one.  This ensures\n        # that (until the server is closed) there is always at least\n        # one pipe handle for address.  Therefore if a client attempt\n        # to connect it will not fail with FileNotFoundError.\n        tmp, self._pipe = self._pipe, self._server_pipe_handle(False)\n        return tmp\n\n    def _server_pipe_handle(self, first):\n        # Return a wrapper for a new pipe handle.\n        if self.closed():\n            return None\n        flags = _winapi.PIPE_ACCESS_DUPLEX | _winapi.FILE_FLAG_OVERLAPPED\n        if first:\n            flags |= _winapi.FILE_FLAG_FIRST_PIPE_INSTANCE\n        h = _winapi.CreateNamedPipe(\n            self._address, flags,\n            _winapi.PIPE_TYPE_MESSAGE | _winapi.PIPE_READMODE_MESSAGE |\n            _winapi.PIPE_WAIT,\n            _winapi.PIPE_UNLIMITED_INSTANCES,\n            windows_utils.BUFSIZE, windows_utils.BUFSIZE,\n            _winapi.NMPWAIT_WAIT_FOREVER, _winapi.NULL)\n        pipe = windows_utils.PipeHandle(h)\n        self._free_instances.add(pipe)\n        return pipe\n\n    def closed(self):\n        return (self._address is None)\n\n    def close(self):\n        if self._accept_pipe_future is not None:\n            self._accept_pipe_future.cancel()\n            self._accept_pipe_future = None\n        # Close all instances which have not been connected to by a client.\n        if self._address is not None:\n            for pipe in self._free_instances:\n                pipe.close()\n            self._pipe = None\n            self._address = None\n            self._free_instances.clear()\n\n    __del__ = close\n\n\nclass _WindowsSelectorEventLoop(selector_events.BaseSelectorEventLoop):\n    \"\"\"Windows version of selector event loop.\"\"\"\n\n\nclass ProactorEventLoop(proactor_events.BaseProactorEventLoop):\n    \"\"\"Windows version of proactor event loop using IOCP.\"\"\"\n\n    def __init__(self, proactor=None):\n        if proactor is None:\n            proactor = IocpProactor()\n        super().__init__(proactor)\n\n    def run_forever(self):\n        try:\n            assert self._self_reading_future is None\n            self.call_soon(self._loop_self_reading)\n            super().run_forever()\n        finally:\n            if self._self_reading_future is not None:\n                ov = self._self_reading_future._ov\n                self._self_reading_future.cancel()\n                # self_reading_future was just cancelled so if it hasn't been\n                # finished yet, it never will be (it's possible that it has\n                # already finished and its callback is waiting in the queue,\n                # where it could still happen if the event loop is restarted).\n                # Unregister it otherwise IocpProactor.close will wait for it\n                # forever\n                if ov is not None:\n                    self._proactor._unregister(ov)\n                self._self_reading_future = None\n\n    async def create_pipe_connection(self, protocol_factory, address):\n        f = self._proactor.connect_pipe(address)\n        pipe = await f\n        protocol = protocol_factory()\n        trans = self._make_duplex_pipe_transport(pipe, protocol,\n                                                 extra={'addr': address})\n        return trans, protocol\n\n    async def start_serving_pipe(self, protocol_factory, address):\n        server = PipeServer(address)\n\n        def loop_accept_pipe(f=None):\n            pipe = None\n            try:\n                if f:\n                    pipe = f.result()\n                    server._free_instances.discard(pipe)\n\n                    if server.closed():\n                        # A client connected before the server was closed:\n                        # drop the client (close the pipe) and exit\n                        pipe.close()\n                        return\n\n                    protocol = protocol_factory()\n                    self._make_duplex_pipe_transport(\n                        pipe, protocol, extra={'addr': address})\n\n                pipe = server._get_unconnected_pipe()\n                if pipe is None:\n                    return\n\n                f = self._proactor.accept_pipe(pipe)\n            except OSError as exc:\n                if pipe and pipe.fileno() != -1:\n                    self.call_exception_handler({\n                        'message': 'Pipe accept failed',\n                        'exception': exc,\n                        'pipe': pipe,\n                    })\n                    pipe.close()\n                elif self._debug:\n                    logger.warning(\"Accept pipe failed on pipe %r\",\n                                   pipe, exc_info=True)\n            except exceptions.CancelledError:\n                if pipe:\n                    pipe.close()\n            else:\n                server._accept_pipe_future = f\n                f.add_done_callback(loop_accept_pipe)\n\n        self.call_soon(loop_accept_pipe)\n        return [server]\n\n    async def _make_subprocess_transport(self, protocol, args, shell,\n                                         stdin, stdout, stderr, bufsize,\n                                         extra=None, **kwargs):\n        waiter = self.create_future()\n        transp = _WindowsSubprocessTransport(self, protocol, args, shell,\n                                             stdin, stdout, stderr, bufsize,\n                                             waiter=waiter, extra=extra,\n                                             **kwargs)\n        try:\n            await waiter\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException:\n            transp.close()\n            await transp._wait()\n            raise\n\n        return transp\n\n\nclass IocpProactor:\n    \"\"\"Proactor implementation using IOCP.\"\"\"\n\n    def __init__(self, concurrency=0xffffffff):\n        self._loop = None\n        self._results = []\n        self._iocp = _overlapped.CreateIoCompletionPort(\n            _overlapped.INVALID_HANDLE_VALUE, NULL, 0, concurrency)\n        self._cache = {}\n        self._registered = weakref.WeakSet()\n        self._unregistered = []\n        self._stopped_serving = weakref.WeakSet()\n\n    def _check_closed(self):\n        if self._iocp is None:\n            raise RuntimeError('IocpProactor is closed')\n\n    def __repr__(self):\n        info = ['overlapped#=%s' % len(self._cache),\n                'result#=%s' % len(self._results)]\n        if self._iocp is None:\n            info.append('closed')\n        return '<%s %s>' % (self.__class__.__name__, \" \".join(info))\n\n    def set_loop(self, loop):\n        self._loop = loop\n\n    def select(self, timeout=None):\n        if not self._results:\n            self._poll(timeout)\n        tmp = self._results\n        self._results = []\n        try:\n            return tmp\n        finally:\n            # Needed to break cycles when an exception occurs.\n            tmp = None\n\n    def _result(self, value):\n        fut = self._loop.create_future()\n        fut.set_result(value)\n        return fut\n\n    def recv(self, conn, nbytes, flags=0):\n        self._register_with_iocp(conn)\n        ov = _overlapped.Overlapped(NULL)\n        try:\n            if isinstance(conn, socket.socket):\n                ov.WSARecv(conn.fileno(), nbytes, flags)\n            else:\n                ov.ReadFile(conn.fileno(), nbytes)\n        except BrokenPipeError:\n            return self._result(b'')\n\n        def finish_recv(trans, key, ov):\n            try:\n                return ov.getresult()\n            except OSError as exc:\n                if exc.winerror in (_overlapped.ERROR_NETNAME_DELETED,\n                                    _overlapped.ERROR_OPERATION_ABORTED):\n                    raise ConnectionResetError(*exc.args)\n                else:\n                    raise\n\n        return self._register(ov, conn, finish_recv)\n\n    def recv_into(self, conn, buf, flags=0):\n        self._register_with_iocp(conn)\n        ov = _overlapped.Overlapped(NULL)\n        try:\n            if isinstance(conn, socket.socket):\n                ov.WSARecvInto(conn.fileno(), buf, flags)\n            else:\n                ov.ReadFileInto(conn.fileno(), buf)\n        except BrokenPipeError:\n            return self._result(0)\n\n        def finish_recv(trans, key, ov):\n            try:\n                return ov.getresult()\n            except OSError as exc:\n                if exc.winerror in (_overlapped.ERROR_NETNAME_DELETED,\n                                    _overlapped.ERROR_OPERATION_ABORTED):\n                    raise ConnectionResetError(*exc.args)\n                else:\n                    raise\n\n        return self._register(ov, conn, finish_recv)\n\n    def recvfrom(self, conn, nbytes, flags=0):\n        self._register_with_iocp(conn)\n        ov = _overlapped.Overlapped(NULL)\n        try:\n            ov.WSARecvFrom(conn.fileno(), nbytes, flags)\n        except BrokenPipeError:\n            return self._result((b'', None))\n\n        def finish_recv(trans, key, ov):\n            try:\n                return ov.getresult()\n            except OSError as exc:\n                if exc.winerror in (_overlapped.ERROR_NETNAME_DELETED,\n                                    _overlapped.ERROR_OPERATION_ABORTED):\n                    raise ConnectionResetError(*exc.args)\n                else:\n                    raise\n\n        return self._register(ov, conn, finish_recv)\n\n    def sendto(self, conn, buf, flags=0, addr=None):\n        self._register_with_iocp(conn)\n        ov = _overlapped.Overlapped(NULL)\n\n        ov.WSASendTo(conn.fileno(), buf, flags, addr)\n\n        def finish_send(trans, key, ov):\n            try:\n                return ov.getresult()\n            except OSError as exc:\n                if exc.winerror in (_overlapped.ERROR_NETNAME_DELETED,\n                                    _overlapped.ERROR_OPERATION_ABORTED):\n                    raise ConnectionResetError(*exc.args)\n                else:\n                    raise\n\n        return self._register(ov, conn, finish_send)\n\n    def send(self, conn, buf, flags=0):\n        self._register_with_iocp(conn)\n        ov = _overlapped.Overlapped(NULL)\n        if isinstance(conn, socket.socket):\n            ov.WSASend(conn.fileno(), buf, flags)\n        else:\n            ov.WriteFile(conn.fileno(), buf)\n\n        def finish_send(trans, key, ov):\n            try:\n                return ov.getresult()\n            except OSError as exc:\n                if exc.winerror in (_overlapped.ERROR_NETNAME_DELETED,\n                                    _overlapped.ERROR_OPERATION_ABORTED):\n                    raise ConnectionResetError(*exc.args)\n                else:\n                    raise\n\n        return self._register(ov, conn, finish_send)\n\n    def accept(self, listener):\n        self._register_with_iocp(listener)\n        conn = self._get_accept_socket(listener.family)\n        ov = _overlapped.Overlapped(NULL)\n        ov.AcceptEx(listener.fileno(), conn.fileno())\n\n        def finish_accept(trans, key, ov):\n            ov.getresult()\n            # Use SO_UPDATE_ACCEPT_CONTEXT so getsockname() etc work.\n            buf = struct.pack('@P', listener.fileno())\n            conn.setsockopt(socket.SOL_SOCKET,\n                            _overlapped.SO_UPDATE_ACCEPT_CONTEXT, buf)\n            conn.settimeout(listener.gettimeout())\n            return conn, conn.getpeername()\n\n        async def accept_coro(future, conn):\n            # Coroutine closing the accept socket if the future is cancelled\n            try:\n                await future\n            except exceptions.CancelledError:\n                conn.close()\n                raise\n\n        future = self._register(ov, listener, finish_accept)\n        coro = accept_coro(future, conn)\n        tasks.ensure_future(coro, loop=self._loop)\n        return future\n\n    def connect(self, conn, address):\n        if conn.type == socket.SOCK_DGRAM:\n            # WSAConnect will complete immediately for UDP sockets so we don't\n            # need to register any IOCP operation\n            _overlapped.WSAConnect(conn.fileno(), address)\n            fut = self._loop.create_future()\n            fut.set_result(None)\n            return fut\n\n        self._register_with_iocp(conn)\n        # The socket needs to be locally bound before we call ConnectEx().\n        try:\n            _overlapped.BindLocal(conn.fileno(), conn.family)\n        except OSError as e:\n            if e.winerror != errno.WSAEINVAL:\n                raise\n            # Probably already locally bound; check using getsockname().\n            if conn.getsockname()[1] == 0:\n                raise\n        ov = _overlapped.Overlapped(NULL)\n        ov.ConnectEx(conn.fileno(), address)\n\n        def finish_connect(trans, key, ov):\n            ov.getresult()\n            # Use SO_UPDATE_CONNECT_CONTEXT so getsockname() etc work.\n            conn.setsockopt(socket.SOL_SOCKET,\n                            _overlapped.SO_UPDATE_CONNECT_CONTEXT, 0)\n            return conn\n\n        return self._register(ov, conn, finish_connect)\n\n    def sendfile(self, sock, file, offset, count):\n        self._register_with_iocp(sock)\n        ov = _overlapped.Overlapped(NULL)\n        offset_low = offset & 0xffff_ffff\n        offset_high = (offset >> 32) & 0xffff_ffff\n        ov.TransmitFile(sock.fileno(),\n                        msvcrt.get_osfhandle(file.fileno()),\n                        offset_low, offset_high,\n                        count, 0, 0)\n\n        def finish_sendfile(trans, key, ov):\n            try:\n                return ov.getresult()\n            except OSError as exc:\n                if exc.winerror in (_overlapped.ERROR_NETNAME_DELETED,\n                                    _overlapped.ERROR_OPERATION_ABORTED):\n                    raise ConnectionResetError(*exc.args)\n                else:\n                    raise\n        return self._register(ov, sock, finish_sendfile)\n\n    def accept_pipe(self, pipe):\n        self._register_with_iocp(pipe)\n        ov = _overlapped.Overlapped(NULL)\n        connected = ov.ConnectNamedPipe(pipe.fileno())\n\n        if connected:\n            # ConnectNamePipe() failed with ERROR_PIPE_CONNECTED which means\n            # that the pipe is connected. There is no need to wait for the\n            # completion of the connection.\n            return self._result(pipe)\n\n        def finish_accept_pipe(trans, key, ov):\n            ov.getresult()\n            return pipe\n\n        return self._register(ov, pipe, finish_accept_pipe)\n\n    async def connect_pipe(self, address):\n        delay = CONNECT_PIPE_INIT_DELAY\n        while True:\n            # Unfortunately there is no way to do an overlapped connect to\n            # a pipe.  Call CreateFile() in a loop until it doesn't fail with\n            # ERROR_PIPE_BUSY.\n            try:\n                handle = _overlapped.ConnectPipe(address)\n                break\n            except OSError as exc:\n                if exc.winerror != _overlapped.ERROR_PIPE_BUSY:\n                    raise\n\n            # ConnectPipe() failed with ERROR_PIPE_BUSY: retry later\n            delay = min(delay * 2, CONNECT_PIPE_MAX_DELAY)\n            await tasks.sleep(delay)\n\n        return windows_utils.PipeHandle(handle)\n\n    def wait_for_handle(self, handle, timeout=None):\n        \"\"\"Wait for a handle.\n\n        Return a Future object. The result of the future is True if the wait\n        completed, or False if the wait did not complete (on timeout).\n        \"\"\"\n        return self._wait_for_handle(handle, timeout, False)\n\n    def _wait_cancel(self, event, done_callback):\n        fut = self._wait_for_handle(event, None, True)\n        # add_done_callback() cannot be used because the wait may only complete\n        # in IocpProactor.close(), while the event loop is not running.\n        fut._done_callback = done_callback\n        return fut\n\n    def _wait_for_handle(self, handle, timeout, _is_cancel):\n        self._check_closed()\n\n        if timeout is None:\n            ms = _winapi.INFINITE\n        else:\n            # RegisterWaitForSingleObject() has a resolution of 1 millisecond,\n            # round away from zero to wait *at least* timeout seconds.\n            ms = math.ceil(timeout * 1e3)\n\n        # We only create ov so we can use ov.address as a key for the cache.\n        ov = _overlapped.Overlapped(NULL)\n        wait_handle = _overlapped.RegisterWaitWithQueue(\n            handle, self._iocp, ov.address, ms)\n        if _is_cancel:\n            f = _WaitCancelFuture(ov, handle, wait_handle, loop=self._loop)\n        else:\n            f = _WaitHandleFuture(ov, handle, wait_handle, self,\n                                  loop=self._loop)\n        if f._source_traceback:\n            del f._source_traceback[-1]\n\n        def finish_wait_for_handle(trans, key, ov):\n            # Note that this second wait means that we should only use\n            # this with handles types where a successful wait has no\n            # effect.  So events or processes are all right, but locks\n            # or semaphores are not.  Also note if the handle is\n            # signalled and then quickly reset, then we may return\n            # False even though we have not timed out.\n            return f._poll()\n\n        self._cache[ov.address] = (f, ov, 0, finish_wait_for_handle)\n        return f\n\n    def _register_with_iocp(self, obj):\n        # To get notifications of finished ops on this objects sent to the\n        # completion port, were must register the handle.\n        if obj not in self._registered:\n            self._registered.add(obj)\n            _overlapped.CreateIoCompletionPort(obj.fileno(), self._iocp, 0, 0)\n            # XXX We could also use SetFileCompletionNotificationModes()\n            # to avoid sending notifications to completion port of ops\n            # that succeed immediately.\n\n    def _register(self, ov, obj, callback):\n        self._check_closed()\n\n        # Return a future which will be set with the result of the\n        # operation when it completes.  The future's value is actually\n        # the value returned by callback().\n        f = _OverlappedFuture(ov, loop=self._loop)\n        if f._source_traceback:\n            del f._source_traceback[-1]\n        if not ov.pending:\n            # The operation has completed, so no need to postpone the\n            # work.  We cannot take this short cut if we need the\n            # NumberOfBytes, CompletionKey values returned by\n            # PostQueuedCompletionStatus().\n            try:\n                value = callback(None, None, ov)\n            except OSError as e:\n                f.set_exception(e)\n            else:\n                f.set_result(value)\n            # Even if GetOverlappedResult() was called, we have to wait for the\n            # notification of the completion in GetQueuedCompletionStatus().\n            # Register the overlapped operation to keep a reference to the\n            # OVERLAPPED object, otherwise the memory is freed and Windows may\n            # read uninitialized memory.\n\n        # Register the overlapped operation for later.  Note that\n        # we only store obj to prevent it from being garbage\n        # collected too early.\n        self._cache[ov.address] = (f, ov, obj, callback)\n        return f\n\n    def _unregister(self, ov):\n        \"\"\"Unregister an overlapped object.\n\n        Call this method when its future has been cancelled. The event can\n        already be signalled (pending in the proactor event queue). It is also\n        safe if the event is never signalled (because it was cancelled).\n        \"\"\"\n        self._check_closed()\n        self._unregistered.append(ov)\n\n    def _get_accept_socket(self, family):\n        s = socket.socket(family)\n        s.settimeout(0)\n        return s\n\n    def _poll(self, timeout=None):\n        if timeout is None:\n            ms = INFINITE\n        elif timeout < 0:\n            raise ValueError(\"negative timeout\")\n        else:\n            # GetQueuedCompletionStatus() has a resolution of 1 millisecond,\n            # round away from zero to wait *at least* timeout seconds.\n            ms = math.ceil(timeout * 1e3)\n            if ms >= INFINITE:\n                raise ValueError(\"timeout too big\")\n\n        while True:\n            status = _overlapped.GetQueuedCompletionStatus(self._iocp, ms)\n            if status is None:\n                break\n            ms = 0\n\n            err, transferred, key, address = status\n            try:\n                f, ov, obj, callback = self._cache.pop(address)\n            except KeyError:\n                if self._loop.get_debug():\n                    self._loop.call_exception_handler({\n                        'message': ('GetQueuedCompletionStatus() returned an '\n                                    'unexpected event'),\n                        'status': ('err=%s transferred=%s key=%#x address=%#x'\n                                   % (err, transferred, key, address)),\n                    })\n\n                # key is either zero, or it is used to return a pipe\n                # handle which should be closed to avoid a leak.\n                if key not in (0, _overlapped.INVALID_HANDLE_VALUE):\n                    _winapi.CloseHandle(key)\n                continue\n\n            if obj in self._stopped_serving:\n                f.cancel()\n            # Don't call the callback if _register() already read the result or\n            # if the overlapped has been cancelled\n            elif not f.done():\n                try:\n                    value = callback(transferred, key, ov)\n                except OSError as e:\n                    f.set_exception(e)\n                    self._results.append(f)\n                else:\n                    f.set_result(value)\n                    self._results.append(f)\n                finally:\n                    f = None\n\n        # Remove unregistered futures\n        for ov in self._unregistered:\n            self._cache.pop(ov.address, None)\n        self._unregistered.clear()\n\n    def _stop_serving(self, obj):\n        # obj is a socket or pipe handle.  It will be closed in\n        # BaseProactorEventLoop._stop_serving() which will make any\n        # pending operations fail quickly.\n        self._stopped_serving.add(obj)\n\n    def close(self):\n        if self._iocp is None:\n            # already closed\n            return\n\n        # Cancel remaining registered operations.\n        for address, (fut, ov, obj, callback) in list(self._cache.items()):\n            if fut.cancelled():\n                # Nothing to do with cancelled futures\n                pass\n            elif isinstance(fut, _WaitCancelFuture):\n                # _WaitCancelFuture must not be cancelled\n                pass\n            else:\n                try:\n                    fut.cancel()\n                except OSError as exc:\n                    if self._loop is not None:\n                        context = {\n                            'message': 'Cancelling a future failed',\n                            'exception': exc,\n                            'future': fut,\n                        }\n                        if fut._source_traceback:\n                            context['source_traceback'] = fut._source_traceback\n                        self._loop.call_exception_handler(context)\n\n        # Wait until all cancelled overlapped complete: don't exit with running\n        # overlapped to prevent a crash. Display progress every second if the\n        # loop is still running.\n        msg_update = 1.0\n        start_time = time.monotonic()\n        next_msg = start_time + msg_update\n        while self._cache:\n            if next_msg <= time.monotonic():\n                logger.debug('%r is running after closing for %.1f seconds',\n                             self, time.monotonic() - start_time)\n                next_msg = time.monotonic() + msg_update\n\n            # handle a few events, or timeout\n            self._poll(msg_update)\n\n        self._results = []\n\n        _winapi.CloseHandle(self._iocp)\n        self._iocp = None\n\n    def __del__(self):\n        self.close()\n\n\nclass _WindowsSubprocessTransport(base_subprocess.BaseSubprocessTransport):\n\n    def _start(self, args, shell, stdin, stdout, stderr, bufsize, **kwargs):\n        self._proc = windows_utils.Popen(\n            args, shell=shell, stdin=stdin, stdout=stdout, stderr=stderr,\n            bufsize=bufsize, **kwargs)\n\n        def callback(f):\n            returncode = self._proc.poll()\n            self._process_exited(returncode)\n\n        f = self._loop._proactor.wait_for_handle(int(self._proc._handle))\n        f.add_done_callback(callback)\n\n\nSelectorEventLoop = _WindowsSelectorEventLoop\n\n\nclass WindowsSelectorEventLoopPolicy(events.BaseDefaultEventLoopPolicy):\n    _loop_factory = SelectorEventLoop\n\n\nclass WindowsProactorEventLoopPolicy(events.BaseDefaultEventLoopPolicy):\n    _loop_factory = ProactorEventLoop\n\n\nDefaultEventLoopPolicy = WindowsProactorEventLoopPolicy\n", 919], "D:\\Program\\anaconda3\\lib\\os.py": ["r\"\"\"OS routines for NT or Posix depending on what system we're on.\n\nThis exports:\n  - all functions from posix or nt, e.g. unlink, stat, etc.\n  - os.path is either posixpath or ntpath\n  - os.name is either 'posix' or 'nt'\n  - os.curdir is a string representing the current directory (always '.')\n  - os.pardir is a string representing the parent directory (always '..')\n  - os.sep is the (or a most common) pathname separator ('/' or '\\\\')\n  - os.extsep is the extension separator (always '.')\n  - os.altsep is the alternate pathname separator (None or '/')\n  - os.pathsep is the component separator used in $PATH etc\n  - os.linesep is the line separator in text files ('\\r' or '\\n' or '\\r\\n')\n  - os.defpath is the default search path for executables\n  - os.devnull is the file path of the null device ('/dev/null', etc.)\n\nPrograms that import and use 'os' stand a better chance of being\nportable between different platforms.  Of course, they must then\nonly use functions that are defined by all platforms (e.g., unlink\nand opendir), and leave all pathname manipulation to os.path\n(e.g., split and join).\n\"\"\"\n\n#'\nimport abc\nimport sys\nimport stat as st\n\nfrom _collections_abc import _check_methods\n\nGenericAlias = type(list[int])\n\n_names = sys.builtin_module_names\n\n# Note:  more names are added to __all__ later.\n__all__ = [\"altsep\", \"curdir\", \"pardir\", \"sep\", \"pathsep\", \"linesep\",\n           \"defpath\", \"name\", \"path\", \"devnull\", \"SEEK_SET\", \"SEEK_CUR\",\n           \"SEEK_END\", \"fsencode\", \"fsdecode\", \"get_exec_path\", \"fdopen\",\n           \"extsep\"]\n\ndef _exists(name):\n    return name in globals()\n\ndef _get_exports_list(module):\n    try:\n        return list(module.__all__)\n    except AttributeError:\n        return [n for n in dir(module) if n[0] != '_']\n\n# Any new dependencies of the os module and/or changes in path separator\n# requires updating importlib as well.\nif 'posix' in _names:\n    name = 'posix'\n    linesep = '\\n'\n    from posix import *\n    try:\n        from posix import _exit\n        __all__.append('_exit')\n    except ImportError:\n        pass\n    import posixpath as path\n\n    try:\n        from posix import _have_functions\n    except ImportError:\n        pass\n\n    import posix\n    __all__.extend(_get_exports_list(posix))\n    del posix\n\nelif 'nt' in _names:\n    name = 'nt'\n    linesep = '\\r\\n'\n    from nt import *\n    try:\n        from nt import _exit\n        __all__.append('_exit')\n    except ImportError:\n        pass\n    import ntpath as path\n\n    import nt\n    __all__.extend(_get_exports_list(nt))\n    del nt\n\n    try:\n        from nt import _have_functions\n    except ImportError:\n        pass\n\nelse:\n    raise ImportError('no os specific module found')\n\nsys.modules['os.path'] = path\nfrom os.path import (curdir, pardir, sep, pathsep, defpath, extsep, altsep,\n    devnull)\n\ndel _names\n\n\nif _exists(\"_have_functions\"):\n    _globals = globals()\n    def _add(str, fn):\n        if (fn in _globals) and (str in _have_functions):\n            _set.add(_globals[fn])\n\n    _set = set()\n    _add(\"HAVE_FACCESSAT\",  \"access\")\n    _add(\"HAVE_FCHMODAT\",   \"chmod\")\n    _add(\"HAVE_FCHOWNAT\",   \"chown\")\n    _add(\"HAVE_FSTATAT\",    \"stat\")\n    _add(\"HAVE_FUTIMESAT\",  \"utime\")\n    _add(\"HAVE_LINKAT\",     \"link\")\n    _add(\"HAVE_MKDIRAT\",    \"mkdir\")\n    _add(\"HAVE_MKFIFOAT\",   \"mkfifo\")\n    _add(\"HAVE_MKNODAT\",    \"mknod\")\n    _add(\"HAVE_OPENAT\",     \"open\")\n    _add(\"HAVE_READLINKAT\", \"readlink\")\n    _add(\"HAVE_RENAMEAT\",   \"rename\")\n    _add(\"HAVE_SYMLINKAT\",  \"symlink\")\n    _add(\"HAVE_UNLINKAT\",   \"unlink\")\n    _add(\"HAVE_UNLINKAT\",   \"rmdir\")\n    _add(\"HAVE_UTIMENSAT\",  \"utime\")\n    supports_dir_fd = _set\n\n    _set = set()\n    _add(\"HAVE_FACCESSAT\",  \"access\")\n    supports_effective_ids = _set\n\n    _set = set()\n    _add(\"HAVE_FCHDIR\",     \"chdir\")\n    _add(\"HAVE_FCHMOD\",     \"chmod\")\n    _add(\"HAVE_FCHOWN\",     \"chown\")\n    _add(\"HAVE_FDOPENDIR\",  \"listdir\")\n    _add(\"HAVE_FDOPENDIR\",  \"scandir\")\n    _add(\"HAVE_FEXECVE\",    \"execve\")\n    _set.add(stat) # fstat always works\n    _add(\"HAVE_FTRUNCATE\",  \"truncate\")\n    _add(\"HAVE_FUTIMENS\",   \"utime\")\n    _add(\"HAVE_FUTIMES\",    \"utime\")\n    _add(\"HAVE_FPATHCONF\",  \"pathconf\")\n    if _exists(\"statvfs\") and _exists(\"fstatvfs\"): # mac os x10.3\n        _add(\"HAVE_FSTATVFS\", \"statvfs\")\n    supports_fd = _set\n\n    _set = set()\n    _add(\"HAVE_FACCESSAT\",  \"access\")\n    # Some platforms don't support lchmod().  Often the function exists\n    # anyway, as a stub that always returns ENOSUP or perhaps EOPNOTSUPP.\n    # (No, I don't know why that's a good design.)  ./configure will detect\n    # this and reject it--so HAVE_LCHMOD still won't be defined on such\n    # platforms.  This is Very Helpful.\n    #\n    # However, sometimes platforms without a working lchmod() *do* have\n    # fchmodat().  (Examples: Linux kernel 3.2 with glibc 2.15,\n    # OpenIndiana 3.x.)  And fchmodat() has a flag that theoretically makes\n    # it behave like lchmod().  So in theory it would be a suitable\n    # replacement for lchmod().  But when lchmod() doesn't work, fchmodat()'s\n    # flag doesn't work *either*.  Sadly ./configure isn't sophisticated\n    # enough to detect this condition--it only determines whether or not\n    # fchmodat() minimally works.\n    #\n    # Therefore we simply ignore fchmodat() when deciding whether or not\n    # os.chmod supports follow_symlinks.  Just checking lchmod() is\n    # sufficient.  After all--if you have a working fchmodat(), your\n    # lchmod() almost certainly works too.\n    #\n    # _add(\"HAVE_FCHMODAT\",   \"chmod\")\n    _add(\"HAVE_FCHOWNAT\",   \"chown\")\n    _add(\"HAVE_FSTATAT\",    \"stat\")\n    _add(\"HAVE_LCHFLAGS\",   \"chflags\")\n    _add(\"HAVE_LCHMOD\",     \"chmod\")\n    if _exists(\"lchown\"): # mac os x10.3\n        _add(\"HAVE_LCHOWN\", \"chown\")\n    _add(\"HAVE_LINKAT\",     \"link\")\n    _add(\"HAVE_LUTIMES\",    \"utime\")\n    _add(\"HAVE_LSTAT\",      \"stat\")\n    _add(\"HAVE_FSTATAT\",    \"stat\")\n    _add(\"HAVE_UTIMENSAT\",  \"utime\")\n    _add(\"MS_WINDOWS\",      \"stat\")\n    supports_follow_symlinks = _set\n\n    del _set\n    del _have_functions\n    del _globals\n    del _add\n\n\n# Python uses fixed values for the SEEK_ constants; they are mapped\n# to native constants if necessary in posixmodule.c\n# Other possible SEEK values are directly imported from posixmodule.c\nSEEK_SET = 0\nSEEK_CUR = 1\nSEEK_END = 2\n\n# Super directory utilities.\n# (Inspired by Eric Raymond; the doc strings are mostly his)\n\ndef makedirs(name, mode=0o777, exist_ok=False):\n    \"\"\"makedirs(name [, mode=0o777][, exist_ok=False])\n\n    Super-mkdir; create a leaf directory and all intermediate ones.  Works like\n    mkdir, except that any intermediate path segment (not just the rightmost)\n    will be created if it does not exist. If the target directory already\n    exists, raise an OSError if exist_ok is False. Otherwise no exception is\n    raised.  This is recursive.\n\n    \"\"\"\n    head, tail = path.split(name)\n    if not tail:\n        head, tail = path.split(head)\n    if head and tail and not path.exists(head):\n        try:\n            makedirs(head, exist_ok=exist_ok)\n        except FileExistsError:\n            # Defeats race condition when another thread created the path\n            pass\n        cdir = curdir\n        if isinstance(tail, bytes):\n            cdir = bytes(curdir, 'ASCII')\n        if tail == cdir:           # xxx/newdir/. exists if xxx/newdir exists\n            return\n    try:\n        mkdir(name, mode)\n    except OSError:\n        # Cannot rely on checking for EEXIST, since the operating system\n        # could give priority to other errors like EACCES or EROFS\n        if not exist_ok or not path.isdir(name):\n            raise\n\ndef removedirs(name):\n    \"\"\"removedirs(name)\n\n    Super-rmdir; remove a leaf directory and all empty intermediate\n    ones.  Works like rmdir except that, if the leaf directory is\n    successfully removed, directories corresponding to rightmost path\n    segments will be pruned away until either the whole path is\n    consumed or an error occurs.  Errors during this latter phase are\n    ignored -- they generally mean that a directory was not empty.\n\n    \"\"\"\n    rmdir(name)\n    head, tail = path.split(name)\n    if not tail:\n        head, tail = path.split(head)\n    while head and tail:\n        try:\n            rmdir(head)\n        except OSError:\n            break\n        head, tail = path.split(head)\n\ndef renames(old, new):\n    \"\"\"renames(old, new)\n\n    Super-rename; create directories as necessary and delete any left\n    empty.  Works like rename, except creation of any intermediate\n    directories needed to make the new pathname good is attempted\n    first.  After the rename, directories corresponding to rightmost\n    path segments of the old name will be pruned until either the\n    whole path is consumed or a nonempty directory is found.\n\n    Note: this function can fail with the new directory structure made\n    if you lack permissions needed to unlink the leaf directory or\n    file.\n\n    \"\"\"\n    head, tail = path.split(new)\n    if head and tail and not path.exists(head):\n        makedirs(head)\n    rename(old, new)\n    head, tail = path.split(old)\n    if head and tail:\n        try:\n            removedirs(head)\n        except OSError:\n            pass\n\n__all__.extend([\"makedirs\", \"removedirs\", \"renames\"])\n\ndef walk(top, topdown=True, onerror=None, followlinks=False):\n    \"\"\"Directory tree generator.\n\n    For each directory in the directory tree rooted at top (including top\n    itself, but excluding '.' and '..'), yields a 3-tuple\n\n        dirpath, dirnames, filenames\n\n    dirpath is a string, the path to the directory.  dirnames is a list of\n    the names of the subdirectories in dirpath (including symlinks to directories,\n    and excluding '.' and '..').\n    filenames is a list of the names of the non-directory files in dirpath.\n    Note that the names in the lists are just names, with no path components.\n    To get a full path (which begins with top) to a file or directory in\n    dirpath, do os.path.join(dirpath, name).\n\n    If optional arg 'topdown' is true or not specified, the triple for a\n    directory is generated before the triples for any of its subdirectories\n    (directories are generated top down).  If topdown is false, the triple\n    for a directory is generated after the triples for all of its\n    subdirectories (directories are generated bottom up).\n\n    When topdown is true, the caller can modify the dirnames list in-place\n    (e.g., via del or slice assignment), and walk will only recurse into the\n    subdirectories whose names remain in dirnames; this can be used to prune the\n    search, or to impose a specific order of visiting.  Modifying dirnames when\n    topdown is false has no effect on the behavior of os.walk(), since the\n    directories in dirnames have already been generated by the time dirnames\n    itself is generated. No matter the value of topdown, the list of\n    subdirectories is retrieved before the tuples for the directory and its\n    subdirectories are generated.\n\n    By default errors from the os.scandir() call are ignored.  If\n    optional arg 'onerror' is specified, it should be a function; it\n    will be called with one argument, an OSError instance.  It can\n    report the error to continue with the walk, or raise the exception\n    to abort the walk.  Note that the filename is available as the\n    filename attribute of the exception object.\n\n    By default, os.walk does not follow symbolic links to subdirectories on\n    systems that support them.  In order to get this functionality, set the\n    optional argument 'followlinks' to true.\n\n    Caution:  if you pass a relative pathname for top, don't change the\n    current working directory between resumptions of walk.  walk never\n    changes the current directory, and assumes that the client doesn't\n    either.\n\n    Example:\n\n    import os\n    from os.path import join, getsize\n    for root, dirs, files in os.walk('python/Lib/email'):\n        print(root, \"consumes\", end=\"\")\n        print(sum(getsize(join(root, name)) for name in files), end=\"\")\n        print(\"bytes in\", len(files), \"non-directory files\")\n        if 'CVS' in dirs:\n            dirs.remove('CVS')  # don't visit CVS directories\n\n    \"\"\"\n    sys.audit(\"os.walk\", top, topdown, onerror, followlinks)\n    return _walk(fspath(top), topdown, onerror, followlinks)\n\ndef _walk(top, topdown, onerror, followlinks):\n    dirs = []\n    nondirs = []\n    walk_dirs = []\n\n    # We may not have read permission for top, in which case we can't\n    # get a list of the files the directory contains.  os.walk\n    # always suppressed the exception then, rather than blow up for a\n    # minor reason when (say) a thousand readable directories are still\n    # left to visit.  That logic is copied here.\n    try:\n        # Note that scandir is global in this module due\n        # to earlier import-*.\n        scandir_it = scandir(top)\n    except OSError as error:\n        if onerror is not None:\n            onerror(error)\n        return\n\n    with scandir_it:\n        while True:\n            try:\n                try:\n                    entry = next(scandir_it)\n                except StopIteration:\n                    break\n            except OSError as error:\n                if onerror is not None:\n                    onerror(error)\n                return\n\n            try:\n                is_dir = entry.is_dir()\n            except OSError:\n                # If is_dir() raises an OSError, consider that the entry is not\n                # a directory, same behaviour than os.path.isdir().\n                is_dir = False\n\n            if is_dir:\n                dirs.append(entry.name)\n            else:\n                nondirs.append(entry.name)\n\n            if not topdown and is_dir:\n                # Bottom-up: recurse into sub-directory, but exclude symlinks to\n                # directories if followlinks is False\n                if followlinks:\n                    walk_into = True\n                else:\n                    try:\n                        is_symlink = entry.is_symlink()\n                    except OSError:\n                        # If is_symlink() raises an OSError, consider that the\n                        # entry is not a symbolic link, same behaviour than\n                        # os.path.islink().\n                        is_symlink = False\n                    walk_into = not is_symlink\n\n                if walk_into:\n                    walk_dirs.append(entry.path)\n\n    # Yield before recursion if going top down\n    if topdown:\n        yield top, dirs, nondirs\n\n        # Recurse into sub-directories\n        islink, join = path.islink, path.join\n        for dirname in dirs:\n            new_path = join(top, dirname)\n            # Issue #23605: os.path.islink() is used instead of caching\n            # entry.is_symlink() result during the loop on os.scandir() because\n            # the caller can replace the directory entry during the \"yield\"\n            # above.\n            if followlinks or not islink(new_path):\n                yield from _walk(new_path, topdown, onerror, followlinks)\n    else:\n        # Recurse into sub-directories\n        for new_path in walk_dirs:\n            yield from _walk(new_path, topdown, onerror, followlinks)\n        # Yield after recursion if going bottom up\n        yield top, dirs, nondirs\n\n__all__.append(\"walk\")\n\nif {open, stat} <= supports_dir_fd and {scandir, stat} <= supports_fd:\n\n    def fwalk(top=\".\", topdown=True, onerror=None, *, follow_symlinks=False, dir_fd=None):\n        \"\"\"Directory tree generator.\n\n        This behaves exactly like walk(), except that it yields a 4-tuple\n\n            dirpath, dirnames, filenames, dirfd\n\n        `dirpath`, `dirnames` and `filenames` are identical to walk() output,\n        and `dirfd` is a file descriptor referring to the directory `dirpath`.\n\n        The advantage of fwalk() over walk() is that it's safe against symlink\n        races (when follow_symlinks is False).\n\n        If dir_fd is not None, it should be a file descriptor open to a directory,\n          and top should be relative; top will then be relative to that directory.\n          (dir_fd is always supported for fwalk.)\n\n        Caution:\n        Since fwalk() yields file descriptors, those are only valid until the\n        next iteration step, so you should dup() them if you want to keep them\n        for a longer period.\n\n        Example:\n\n        import os\n        for root, dirs, files, rootfd in os.fwalk('python/Lib/email'):\n            print(root, \"consumes\", end=\"\")\n            print(sum(os.stat(name, dir_fd=rootfd).st_size for name in files),\n                  end=\"\")\n            print(\"bytes in\", len(files), \"non-directory files\")\n            if 'CVS' in dirs:\n                dirs.remove('CVS')  # don't visit CVS directories\n        \"\"\"\n        sys.audit(\"os.fwalk\", top, topdown, onerror, follow_symlinks, dir_fd)\n        if not isinstance(top, int) or not hasattr(top, '__index__'):\n            top = fspath(top)\n        # Note: To guard against symlink races, we use the standard\n        # lstat()/open()/fstat() trick.\n        if not follow_symlinks:\n            orig_st = stat(top, follow_symlinks=False, dir_fd=dir_fd)\n        topfd = open(top, O_RDONLY, dir_fd=dir_fd)\n        try:\n            if (follow_symlinks or (st.S_ISDIR(orig_st.st_mode) and\n                                    path.samestat(orig_st, stat(topfd)))):\n                yield from _fwalk(topfd, top, isinstance(top, bytes),\n                                  topdown, onerror, follow_symlinks)\n        finally:\n            close(topfd)\n\n    def _fwalk(topfd, toppath, isbytes, topdown, onerror, follow_symlinks):\n        # Note: This uses O(depth of the directory tree) file descriptors: if\n        # necessary, it can be adapted to only require O(1) FDs, see issue\n        # #13734.\n\n        scandir_it = scandir(topfd)\n        dirs = []\n        nondirs = []\n        entries = None if topdown or follow_symlinks else []\n        for entry in scandir_it:\n            name = entry.name\n            if isbytes:\n                name = fsencode(name)\n            try:\n                if entry.is_dir():\n                    dirs.append(name)\n                    if entries is not None:\n                        entries.append(entry)\n                else:\n                    nondirs.append(name)\n            except OSError:\n                try:\n                    # Add dangling symlinks, ignore disappeared files\n                    if entry.is_symlink():\n                        nondirs.append(name)\n                except OSError:\n                    pass\n\n        if topdown:\n            yield toppath, dirs, nondirs, topfd\n\n        for name in dirs if entries is None else zip(dirs, entries):\n            try:\n                if not follow_symlinks:\n                    if topdown:\n                        orig_st = stat(name, dir_fd=topfd, follow_symlinks=False)\n                    else:\n                        assert entries is not None\n                        name, entry = name\n                        orig_st = entry.stat(follow_symlinks=False)\n                dirfd = open(name, O_RDONLY, dir_fd=topfd)\n            except OSError as err:\n                if onerror is not None:\n                    onerror(err)\n                continue\n            try:\n                if follow_symlinks or path.samestat(orig_st, stat(dirfd)):\n                    dirpath = path.join(toppath, name)\n                    yield from _fwalk(dirfd, dirpath, isbytes,\n                                      topdown, onerror, follow_symlinks)\n            finally:\n                close(dirfd)\n\n        if not topdown:\n            yield toppath, dirs, nondirs, topfd\n\n    __all__.append(\"fwalk\")\n\ndef execl(file, *args):\n    \"\"\"execl(file, *args)\n\n    Execute the executable file with argument list args, replacing the\n    current process. \"\"\"\n    execv(file, args)\n\ndef execle(file, *args):\n    \"\"\"execle(file, *args, env)\n\n    Execute the executable file with argument list args and\n    environment env, replacing the current process. \"\"\"\n    env = args[-1]\n    execve(file, args[:-1], env)\n\ndef execlp(file, *args):\n    \"\"\"execlp(file, *args)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args, replacing the current process. \"\"\"\n    execvp(file, args)\n\ndef execlpe(file, *args):\n    \"\"\"execlpe(file, *args, env)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args and environment env, replacing the current\n    process. \"\"\"\n    env = args[-1]\n    execvpe(file, args[:-1], env)\n\ndef execvp(file, args):\n    \"\"\"execvp(file, args)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args, replacing the current process.\n    args may be a list or tuple of strings. \"\"\"\n    _execvpe(file, args)\n\ndef execvpe(file, args, env):\n    \"\"\"execvpe(file, args, env)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args and environment env, replacing the\n    current process.\n    args may be a list or tuple of strings. \"\"\"\n    _execvpe(file, args, env)\n\n__all__.extend([\"execl\",\"execle\",\"execlp\",\"execlpe\",\"execvp\",\"execvpe\"])\n\ndef _execvpe(file, args, env=None):\n    if env is not None:\n        exec_func = execve\n        argrest = (args, env)\n    else:\n        exec_func = execv\n        argrest = (args,)\n        env = environ\n\n    if path.dirname(file):\n        exec_func(file, *argrest)\n        return\n    saved_exc = None\n    path_list = get_exec_path(env)\n    if name != 'nt':\n        file = fsencode(file)\n        path_list = map(fsencode, path_list)\n    for dir in path_list:\n        fullname = path.join(dir, file)\n        try:\n            exec_func(fullname, *argrest)\n        except (FileNotFoundError, NotADirectoryError) as e:\n            last_exc = e\n        except OSError as e:\n            last_exc = e\n            if saved_exc is None:\n                saved_exc = e\n    if saved_exc is not None:\n        raise saved_exc\n    raise last_exc\n\n\ndef get_exec_path(env=None):\n    \"\"\"Returns the sequence of directories that will be searched for the\n    named executable (similar to a shell) when launching a process.\n\n    *env* must be an environment variable dict or None.  If *env* is None,\n    os.environ will be used.\n    \"\"\"\n    # Use a local import instead of a global import to limit the number of\n    # modules loaded at startup: the os module is always loaded at startup by\n    # Python. It may also avoid a bootstrap issue.\n    import warnings\n\n    if env is None:\n        env = environ\n\n    # {b'PATH': ...}.get('PATH') and {'PATH': ...}.get(b'PATH') emit a\n    # BytesWarning when using python -b or python -bb: ignore the warning\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\", BytesWarning)\n\n        try:\n            path_list = env.get('PATH')\n        except TypeError:\n            path_list = None\n\n        if supports_bytes_environ:\n            try:\n                path_listb = env[b'PATH']\n            except (KeyError, TypeError):\n                pass\n            else:\n                if path_list is not None:\n                    raise ValueError(\n                        \"env cannot contain 'PATH' and b'PATH' keys\")\n                path_list = path_listb\n\n            if path_list is not None and isinstance(path_list, bytes):\n                path_list = fsdecode(path_list)\n\n    if path_list is None:\n        path_list = defpath\n    return path_list.split(pathsep)\n\n\n# Change environ to automatically call putenv() and unsetenv()\nfrom _collections_abc import MutableMapping, Mapping\n\nclass _Environ(MutableMapping):\n    def __init__(self, data, encodekey, decodekey, encodevalue, decodevalue):\n        self.encodekey = encodekey\n        self.decodekey = decodekey\n        self.encodevalue = encodevalue\n        self.decodevalue = decodevalue\n        self._data = data\n\n    def __getitem__(self, key):\n        try:\n            value = self._data[self.encodekey(key)]\n        except KeyError:\n            # raise KeyError with the original key value\n            raise KeyError(key) from None\n        return self.decodevalue(value)\n\n    def __setitem__(self, key, value):\n        key = self.encodekey(key)\n        value = self.encodevalue(value)\n        putenv(key, value)\n        self._data[key] = value\n\n    def __delitem__(self, key):\n        encodedkey = self.encodekey(key)\n        unsetenv(encodedkey)\n        try:\n            del self._data[encodedkey]\n        except KeyError:\n            # raise KeyError with the original key value\n            raise KeyError(key) from None\n\n    def __iter__(self):\n        # list() from dict object is an atomic operation\n        keys = list(self._data)\n        for key in keys:\n            yield self.decodekey(key)\n\n    def __len__(self):\n        return len(self._data)\n\n    def __repr__(self):\n        return 'environ({{{}}})'.format(', '.join(\n            ('{!r}: {!r}'.format(self.decodekey(key), self.decodevalue(value))\n            for key, value in self._data.items())))\n\n    def copy(self):\n        return dict(self)\n\n    def setdefault(self, key, value):\n        if key not in self:\n            self[key] = value\n        return self[key]\n\n    def __ior__(self, other):\n        self.update(other)\n        return self\n\n    def __or__(self, other):\n        if not isinstance(other, Mapping):\n            return NotImplemented\n        new = dict(self)\n        new.update(other)\n        return new\n\n    def __ror__(self, other):\n        if not isinstance(other, Mapping):\n            return NotImplemented\n        new = dict(other)\n        new.update(self)\n        return new\n\ndef _createenviron():\n    if name == 'nt':\n        # Where Env Var Names Must Be UPPERCASE\n        def check_str(value):\n            if not isinstance(value, str):\n                raise TypeError(\"str expected, not %s\" % type(value).__name__)\n            return value\n        encode = check_str\n        decode = str\n        def encodekey(key):\n            return encode(key).upper()\n        data = {}\n        for key, value in environ.items():\n            data[encodekey(key)] = value\n    else:\n        # Where Env Var Names Can Be Mixed Case\n        encoding = sys.getfilesystemencoding()\n        def encode(value):\n            if not isinstance(value, str):\n                raise TypeError(\"str expected, not %s\" % type(value).__name__)\n            return value.encode(encoding, 'surrogateescape')\n        def decode(value):\n            return value.decode(encoding, 'surrogateescape')\n        encodekey = encode\n        data = environ\n    return _Environ(data,\n        encodekey, decode,\n        encode, decode)\n\n# unicode environ\nenviron = _createenviron()\ndel _createenviron\n\n\ndef getenv(key, default=None):\n    \"\"\"Get an environment variable, return None if it doesn't exist.\n    The optional second argument can specify an alternate default.\n    key, default and the result are str.\"\"\"\n    return environ.get(key, default)\n\nsupports_bytes_environ = (name != 'nt')\n__all__.extend((\"getenv\", \"supports_bytes_environ\"))\n\nif supports_bytes_environ:\n    def _check_bytes(value):\n        if not isinstance(value, bytes):\n            raise TypeError(\"bytes expected, not %s\" % type(value).__name__)\n        return value\n\n    # bytes environ\n    environb = _Environ(environ._data,\n        _check_bytes, bytes,\n        _check_bytes, bytes)\n    del _check_bytes\n\n    def getenvb(key, default=None):\n        \"\"\"Get an environment variable, return None if it doesn't exist.\n        The optional second argument can specify an alternate default.\n        key, default and the result are bytes.\"\"\"\n        return environb.get(key, default)\n\n    __all__.extend((\"environb\", \"getenvb\"))\n\ndef _fscodec():\n    encoding = sys.getfilesystemencoding()\n    errors = sys.getfilesystemencodeerrors()\n\n    def fsencode(filename):\n        \"\"\"Encode filename (an os.PathLike, bytes, or str) to the filesystem\n        encoding with 'surrogateescape' error handler, return bytes unchanged.\n        On Windows, use 'strict' error handler if the file system encoding is\n        'mbcs' (which is the default encoding).\n        \"\"\"\n        filename = fspath(filename)  # Does type-checking of `filename`.\n        if isinstance(filename, str):\n            return filename.encode(encoding, errors)\n        else:\n            return filename\n\n    def fsdecode(filename):\n        \"\"\"Decode filename (an os.PathLike, bytes, or str) from the filesystem\n        encoding with 'surrogateescape' error handler, return str unchanged. On\n        Windows, use 'strict' error handler if the file system encoding is\n        'mbcs' (which is the default encoding).\n        \"\"\"\n        filename = fspath(filename)  # Does type-checking of `filename`.\n        if isinstance(filename, bytes):\n            return filename.decode(encoding, errors)\n        else:\n            return filename\n\n    return fsencode, fsdecode\n\nfsencode, fsdecode = _fscodec()\ndel _fscodec\n\n# Supply spawn*() (probably only for Unix)\nif _exists(\"fork\") and not _exists(\"spawnv\") and _exists(\"execv\"):\n\n    P_WAIT = 0\n    P_NOWAIT = P_NOWAITO = 1\n\n    __all__.extend([\"P_WAIT\", \"P_NOWAIT\", \"P_NOWAITO\"])\n\n    # XXX Should we support P_DETACH?  I suppose it could fork()**2\n    # and close the std I/O streams.  Also, P_OVERLAY is the same\n    # as execv*()?\n\n    def _spawnvef(mode, file, args, env, func):\n        # Internal helper; func is the exec*() function to use\n        if not isinstance(args, (tuple, list)):\n            raise TypeError('argv must be a tuple or a list')\n        if not args or not args[0]:\n            raise ValueError('argv first element cannot be empty')\n        pid = fork()\n        if not pid:\n            # Child\n            try:\n                if env is None:\n                    func(file, args)\n                else:\n                    func(file, args, env)\n            except:\n                _exit(127)\n        else:\n            # Parent\n            if mode == P_NOWAIT:\n                return pid # Caller is responsible for waiting!\n            while 1:\n                wpid, sts = waitpid(pid, 0)\n                if WIFSTOPPED(sts):\n                    continue\n\n                return waitstatus_to_exitcode(sts)\n\n    def spawnv(mode, file, args):\n        \"\"\"spawnv(mode, file, args) -> integer\n\nExecute file with arguments from args in a subprocess.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, None, execv)\n\n    def spawnve(mode, file, args, env):\n        \"\"\"spawnve(mode, file, args, env) -> integer\n\nExecute file with arguments from args in a subprocess with the\nspecified environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, env, execve)\n\n    # Note: spawnvp[e] isn't currently supported on Windows\n\n    def spawnvp(mode, file, args):\n        \"\"\"spawnvp(mode, file, args) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, None, execvp)\n\n    def spawnvpe(mode, file, args, env):\n        \"\"\"spawnvpe(mode, file, args, env) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess with the supplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, env, execvpe)\n\n\n    __all__.extend([\"spawnv\", \"spawnve\", \"spawnvp\", \"spawnvpe\"])\n\n\nif _exists(\"spawnv\"):\n    # These aren't supplied by the basic Windows code\n    # but can be easily implemented in Python\n\n    def spawnl(mode, file, *args):\n        \"\"\"spawnl(mode, file, *args) -> integer\n\nExecute file with arguments from args in a subprocess.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return spawnv(mode, file, args)\n\n    def spawnle(mode, file, *args):\n        \"\"\"spawnle(mode, file, *args, env) -> integer\n\nExecute file with arguments from args in a subprocess with the\nsupplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        env = args[-1]\n        return spawnve(mode, file, args[:-1], env)\n\n\n    __all__.extend([\"spawnl\", \"spawnle\"])\n\n\nif _exists(\"spawnvp\"):\n    # At the moment, Windows doesn't implement spawnvp[e],\n    # so it won't have spawnlp[e] either.\n    def spawnlp(mode, file, *args):\n        \"\"\"spawnlp(mode, file, *args) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess with the supplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return spawnvp(mode, file, args)\n\n    def spawnlpe(mode, file, *args):\n        \"\"\"spawnlpe(mode, file, *args, env) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess with the supplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        env = args[-1]\n        return spawnvpe(mode, file, args[:-1], env)\n\n\n    __all__.extend([\"spawnlp\", \"spawnlpe\"])\n\n# VxWorks has no user space shell provided. As a result, running\n# command in a shell can't be supported.\nif sys.platform != 'vxworks':\n    # Supply os.popen()\n    def popen(cmd, mode=\"r\", buffering=-1):\n        if not isinstance(cmd, str):\n            raise TypeError(\"invalid cmd type (%s, expected string)\" % type(cmd))\n        if mode not in (\"r\", \"w\"):\n            raise ValueError(\"invalid mode %r\" % mode)\n        if buffering == 0 or buffering is None:\n            raise ValueError(\"popen() does not support unbuffered streams\")\n        import subprocess, io\n        if mode == \"r\":\n            proc = subprocess.Popen(cmd,\n                                    shell=True, text=True,\n                                    stdout=subprocess.PIPE,\n                                    bufsize=buffering)\n            return _wrap_close(proc.stdout, proc)\n        else:\n            proc = subprocess.Popen(cmd,\n                                    shell=True, text=True,\n                                    stdin=subprocess.PIPE,\n                                    bufsize=buffering)\n            return _wrap_close(proc.stdin, proc)\n\n    # Helper for popen() -- a proxy for a file whose close waits for the process\n    class _wrap_close:\n        def __init__(self, stream, proc):\n            self._stream = stream\n            self._proc = proc\n        def close(self):\n            self._stream.close()\n            returncode = self._proc.wait()\n            if returncode == 0:\n                return None\n            if name == 'nt':\n                return returncode\n            else:\n                return returncode << 8  # Shift left to match old behavior\n        def __enter__(self):\n            return self\n        def __exit__(self, *args):\n            self.close()\n        def __getattr__(self, name):\n            return getattr(self._stream, name)\n        def __iter__(self):\n            return iter(self._stream)\n\n    __all__.append(\"popen\")\n\n# Supply os.fdopen()\ndef fdopen(fd, mode=\"r\", buffering=-1, encoding=None, *args, **kwargs):\n    if not isinstance(fd, int):\n        raise TypeError(\"invalid fd type (%s, expected integer)\" % type(fd))\n    import io\n    if \"b\" not in mode:\n        encoding = io.text_encoding(encoding)\n    return io.open(fd, mode, buffering, encoding, *args, **kwargs)\n\n\n# For testing purposes, make sure the function is available when the C\n# implementation exists.\ndef _fspath(path):\n    \"\"\"Return the path representation of a path-like object.\n\n    If str or bytes is passed in, it is returned unchanged. Otherwise the\n    os.PathLike interface is used to get the path representation. If the\n    path representation is not str or bytes, TypeError is raised. If the\n    provided path is not str, bytes, or os.PathLike, TypeError is raised.\n    \"\"\"\n    if isinstance(path, (str, bytes)):\n        return path\n\n    # Work from the object's type to match method resolution of other magic\n    # methods.\n    path_type = type(path)\n    try:\n        path_repr = path_type.__fspath__(path)\n    except AttributeError:\n        if hasattr(path_type, '__fspath__'):\n            raise\n        else:\n            raise TypeError(\"expected str, bytes or os.PathLike object, \"\n                            \"not \" + path_type.__name__)\n    if isinstance(path_repr, (str, bytes)):\n        return path_repr\n    else:\n        raise TypeError(\"expected {}.__fspath__() to return str or bytes, \"\n                        \"not {}\".format(path_type.__name__,\n                                        type(path_repr).__name__))\n\n# If there is no C implementation, make the pure Python version the\n# implementation as transparently as possible.\nif not _exists('fspath'):\n    fspath = _fspath\n    fspath.__name__ = \"fspath\"\n\n\nclass PathLike(abc.ABC):\n\n    \"\"\"Abstract base class for implementing the file system path protocol.\"\"\"\n\n    @abc.abstractmethod\n    def __fspath__(self):\n        \"\"\"Return the file system path representation of the object.\"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def __subclasshook__(cls, subclass):\n        if cls is PathLike:\n            return _check_methods(subclass, '__fspath__')\n        return NotImplemented\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nif name == 'nt':\n    class _AddedDllDirectory:\n        def __init__(self, path, cookie, remove_dll_directory):\n            self.path = path\n            self._cookie = cookie\n            self._remove_dll_directory = remove_dll_directory\n        def close(self):\n            self._remove_dll_directory(self._cookie)\n            self.path = None\n        def __enter__(self):\n            return self\n        def __exit__(self, *args):\n            self.close()\n        def __repr__(self):\n            if self.path:\n                return \"<AddedDllDirectory({!r})>\".format(self.path)\n            return \"<AddedDllDirectory()>\"\n\n    def add_dll_directory(path):\n        \"\"\"Add a path to the DLL search path.\n\n        This search path is used when resolving dependencies for imported\n        extension modules (the module itself is resolved through sys.path),\n        and also by ctypes.\n\n        Remove the directory by calling close() on the returned object or\n        using it in a with statement.\n        \"\"\"\n        import nt\n        cookie = nt._add_dll_directory(path)\n        return _AddedDllDirectory(\n            path,\n            cookie,\n            nt._remove_dll_directory\n        )\n", 1123], "D:\\Program\\anaconda3\\lib\\_collections_abc.py": ["# Copyright 2007 Google, Inc. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Abstract Base Classes (ABCs) for collections, according to PEP 3119.\n\nUnit tests are in test_collections.\n\"\"\"\n\nfrom abc import ABCMeta, abstractmethod\nimport sys\n\nGenericAlias = type(list[int])\nEllipsisType = type(...)\ndef _f(): pass\nFunctionType = type(_f)\ndel _f\n\n__all__ = [\"Awaitable\", \"Coroutine\",\n           \"AsyncIterable\", \"AsyncIterator\", \"AsyncGenerator\",\n           \"Hashable\", \"Iterable\", \"Iterator\", \"Generator\", \"Reversible\",\n           \"Sized\", \"Container\", \"Callable\", \"Collection\",\n           \"Set\", \"MutableSet\",\n           \"Mapping\", \"MutableMapping\",\n           \"MappingView\", \"KeysView\", \"ItemsView\", \"ValuesView\",\n           \"Sequence\", \"MutableSequence\",\n           \"ByteString\",\n           ]\n\n# This module has been renamed from collections.abc to _collections_abc to\n# speed up interpreter startup. Some of the types such as MutableMapping are\n# required early but collections module imports a lot of other modules.\n# See issue #19218\n__name__ = \"collections.abc\"\n\n# Private list of types that we want to register with the various ABCs\n# so that they will pass tests like:\n#       it = iter(somebytearray)\n#       assert isinstance(it, Iterable)\n# Note:  in other implementations, these types might not be distinct\n# and they may have their own implementation specific types that\n# are not included on this list.\nbytes_iterator = type(iter(b''))\nbytearray_iterator = type(iter(bytearray()))\n#callable_iterator = ???\ndict_keyiterator = type(iter({}.keys()))\ndict_valueiterator = type(iter({}.values()))\ndict_itemiterator = type(iter({}.items()))\nlist_iterator = type(iter([]))\nlist_reverseiterator = type(iter(reversed([])))\nrange_iterator = type(iter(range(0)))\nlongrange_iterator = type(iter(range(1 << 1000)))\nset_iterator = type(iter(set()))\nstr_iterator = type(iter(\"\"))\ntuple_iterator = type(iter(()))\nzip_iterator = type(iter(zip()))\n## views ##\ndict_keys = type({}.keys())\ndict_values = type({}.values())\ndict_items = type({}.items())\n## misc ##\nmappingproxy = type(type.__dict__)\ngenerator = type((lambda: (yield))())\n## coroutine ##\nasync def _coro(): pass\n_coro = _coro()\ncoroutine = type(_coro)\n_coro.close()  # Prevent ResourceWarning\ndel _coro\n## asynchronous generator ##\nasync def _ag(): yield\n_ag = _ag()\nasync_generator = type(_ag)\ndel _ag\n\n\n### ONE-TRICK PONIES ###\n\ndef _check_methods(C, *methods):\n    mro = C.__mro__\n    for method in methods:\n        for B in mro:\n            if method in B.__dict__:\n                if B.__dict__[method] is None:\n                    return NotImplemented\n                break\n        else:\n            return NotImplemented\n    return True\n\nclass Hashable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __hash__(self):\n        return 0\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Hashable:\n            return _check_methods(C, \"__hash__\")\n        return NotImplemented\n\n\nclass Awaitable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __await__(self):\n        yield\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Awaitable:\n            return _check_methods(C, \"__await__\")\n        return NotImplemented\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nclass Coroutine(Awaitable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def send(self, value):\n        \"\"\"Send a value into the coroutine.\n        Return next yielded value or raise StopIteration.\n        \"\"\"\n        raise StopIteration\n\n    @abstractmethod\n    def throw(self, typ, val=None, tb=None):\n        \"\"\"Raise an exception in the coroutine.\n        Return next yielded value or raise StopIteration.\n        \"\"\"\n        if val is None:\n            if tb is None:\n                raise typ\n            val = typ()\n        if tb is not None:\n            val = val.with_traceback(tb)\n        raise val\n\n    def close(self):\n        \"\"\"Raise GeneratorExit inside coroutine.\n        \"\"\"\n        try:\n            self.throw(GeneratorExit)\n        except (GeneratorExit, StopIteration):\n            pass\n        else:\n            raise RuntimeError(\"coroutine ignored GeneratorExit\")\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Coroutine:\n            return _check_methods(C, '__await__', 'send', 'throw', 'close')\n        return NotImplemented\n\n\nCoroutine.register(coroutine)\n\n\nclass AsyncIterable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __aiter__(self):\n        return AsyncIterator()\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AsyncIterable:\n            return _check_methods(C, \"__aiter__\")\n        return NotImplemented\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nclass AsyncIterator(AsyncIterable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    async def __anext__(self):\n        \"\"\"Return the next item or raise StopAsyncIteration when exhausted.\"\"\"\n        raise StopAsyncIteration\n\n    def __aiter__(self):\n        return self\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AsyncIterator:\n            return _check_methods(C, \"__anext__\", \"__aiter__\")\n        return NotImplemented\n\n\nclass AsyncGenerator(AsyncIterator):\n\n    __slots__ = ()\n\n    async def __anext__(self):\n        \"\"\"Return the next item from the asynchronous generator.\n        When exhausted, raise StopAsyncIteration.\n        \"\"\"\n        return await self.asend(None)\n\n    @abstractmethod\n    async def asend(self, value):\n        \"\"\"Send a value into the asynchronous generator.\n        Return next yielded value or raise StopAsyncIteration.\n        \"\"\"\n        raise StopAsyncIteration\n\n    @abstractmethod\n    async def athrow(self, typ, val=None, tb=None):\n        \"\"\"Raise an exception in the asynchronous generator.\n        Return next yielded value or raise StopAsyncIteration.\n        \"\"\"\n        if val is None:\n            if tb is None:\n                raise typ\n            val = typ()\n        if tb is not None:\n            val = val.with_traceback(tb)\n        raise val\n\n    async def aclose(self):\n        \"\"\"Raise GeneratorExit inside coroutine.\n        \"\"\"\n        try:\n            await self.athrow(GeneratorExit)\n        except (GeneratorExit, StopAsyncIteration):\n            pass\n        else:\n            raise RuntimeError(\"asynchronous generator ignored GeneratorExit\")\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is AsyncGenerator:\n            return _check_methods(C, '__aiter__', '__anext__',\n                                  'asend', 'athrow', 'aclose')\n        return NotImplemented\n\n\nAsyncGenerator.register(async_generator)\n\n\nclass Iterable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __iter__(self):\n        while False:\n            yield None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Iterable:\n            return _check_methods(C, \"__iter__\")\n        return NotImplemented\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nclass Iterator(Iterable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __next__(self):\n        'Return the next item from the iterator. When exhausted, raise StopIteration'\n        raise StopIteration\n\n    def __iter__(self):\n        return self\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Iterator:\n            return _check_methods(C, '__iter__', '__next__')\n        return NotImplemented\n\n\nIterator.register(bytes_iterator)\nIterator.register(bytearray_iterator)\n#Iterator.register(callable_iterator)\nIterator.register(dict_keyiterator)\nIterator.register(dict_valueiterator)\nIterator.register(dict_itemiterator)\nIterator.register(list_iterator)\nIterator.register(list_reverseiterator)\nIterator.register(range_iterator)\nIterator.register(longrange_iterator)\nIterator.register(set_iterator)\nIterator.register(str_iterator)\nIterator.register(tuple_iterator)\nIterator.register(zip_iterator)\n\n\nclass Reversible(Iterable):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __reversed__(self):\n        while False:\n            yield None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Reversible:\n            return _check_methods(C, \"__reversed__\", \"__iter__\")\n        return NotImplemented\n\n\nclass Generator(Iterator):\n\n    __slots__ = ()\n\n    def __next__(self):\n        \"\"\"Return the next item from the generator.\n        When exhausted, raise StopIteration.\n        \"\"\"\n        return self.send(None)\n\n    @abstractmethod\n    def send(self, value):\n        \"\"\"Send a value into the generator.\n        Return next yielded value or raise StopIteration.\n        \"\"\"\n        raise StopIteration\n\n    @abstractmethod\n    def throw(self, typ, val=None, tb=None):\n        \"\"\"Raise an exception in the generator.\n        Return next yielded value or raise StopIteration.\n        \"\"\"\n        if val is None:\n            if tb is None:\n                raise typ\n            val = typ()\n        if tb is not None:\n            val = val.with_traceback(tb)\n        raise val\n\n    def close(self):\n        \"\"\"Raise GeneratorExit inside generator.\n        \"\"\"\n        try:\n            self.throw(GeneratorExit)\n        except (GeneratorExit, StopIteration):\n            pass\n        else:\n            raise RuntimeError(\"generator ignored GeneratorExit\")\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Generator:\n            return _check_methods(C, '__iter__', '__next__',\n                                  'send', 'throw', 'close')\n        return NotImplemented\n\n\nGenerator.register(generator)\n\n\nclass Sized(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __len__(self):\n        return 0\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Sized:\n            return _check_methods(C, \"__len__\")\n        return NotImplemented\n\n\nclass Container(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __contains__(self, x):\n        return False\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Container:\n            return _check_methods(C, \"__contains__\")\n        return NotImplemented\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nclass Collection(Sized, Iterable, Container):\n\n    __slots__ = ()\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Collection:\n            return _check_methods(C,  \"__len__\", \"__iter__\", \"__contains__\")\n        return NotImplemented\n\n\nclass _CallableGenericAlias(GenericAlias):\n    \"\"\" Represent `Callable[argtypes, resulttype]`.\n\n    This sets ``__args__`` to a tuple containing the flattened ``argtypes``\n    followed by ``resulttype``.\n\n    Example: ``Callable[[int, str], float]`` sets ``__args__`` to\n    ``(int, str, float)``.\n    \"\"\"\n\n    __slots__ = ()\n\n    def __new__(cls, origin, args):\n        if not (isinstance(args, tuple) and len(args) == 2):\n            raise TypeError(\n                \"Callable must be used as Callable[[arg, ...], result].\")\n        t_args, t_result = args\n        if isinstance(t_args, list):\n            args = (*t_args, t_result)\n        elif not _is_param_expr(t_args):\n            raise TypeError(f\"Expected a list of types, an ellipsis, \"\n                            f\"ParamSpec, or Concatenate. Got {t_args}\")\n        return super().__new__(cls, origin, args)\n\n    @property\n    def __parameters__(self):\n        params = []\n        for arg in self.__args__:\n            if isinstance(arg, type) and not isinstance(arg, GenericAlias):\n                continue\n            # Looks like a genericalias\n            if hasattr(arg, \"__parameters__\") and isinstance(arg.__parameters__, tuple):\n                params.extend(arg.__parameters__)\n            else:\n                if _is_typevarlike(arg):\n                    params.append(arg)\n        return tuple(dict.fromkeys(params))\n\n    def __repr__(self):\n        if len(self.__args__) == 2 and _is_param_expr(self.__args__[0]):\n            return super().__repr__()\n        return (f'collections.abc.Callable'\n                f'[[{\", \".join([_type_repr(a) for a in self.__args__[:-1]])}], '\n                f'{_type_repr(self.__args__[-1])}]')\n\n    def __reduce__(self):\n        args = self.__args__\n        if not (len(args) == 2 and _is_param_expr(args[0])):\n            args = list(args[:-1]), args[-1]\n        return _CallableGenericAlias, (Callable, args)\n\n    def __getitem__(self, item):\n        # Called during TypeVar substitution, returns the custom subclass\n        # rather than the default types.GenericAlias object.  Most of the\n        # code is copied from typing's _GenericAlias and the builtin\n        # types.GenericAlias.\n\n        # A special case in PEP 612 where if X = Callable[P, int],\n        # then X[int, str] == X[[int, str]].\n        param_len = len(self.__parameters__)\n        if param_len == 0:\n            raise TypeError(f'{self} is not a generic class')\n        if not isinstance(item, tuple):\n            item = (item,)\n        if (param_len == 1 and _is_param_expr(self.__parameters__[0])\n                and item and not _is_param_expr(item[0])):\n            item = (list(item),)\n        item_len = len(item)\n        if item_len != param_len:\n            raise TypeError(f'Too {\"many\" if item_len > param_len else \"few\"}'\n                            f' arguments for {self};'\n                            f' actual {item_len}, expected {param_len}')\n        subst = dict(zip(self.__parameters__, item))\n        new_args = []\n        for arg in self.__args__:\n            if isinstance(arg, type) and not isinstance(arg, GenericAlias):\n                new_args.append(arg)\n                continue\n            if _is_typevarlike(arg):\n                if _is_param_expr(arg):\n                    arg = subst[arg]\n                    if not _is_param_expr(arg):\n                        raise TypeError(f\"Expected a list of types, an ellipsis, \"\n                                        f\"ParamSpec, or Concatenate. Got {arg}\")\n                else:\n                    arg = subst[arg]\n            # Looks like a GenericAlias\n            elif hasattr(arg, '__parameters__') and isinstance(arg.__parameters__, tuple):\n                subparams = arg.__parameters__\n                if subparams:\n                    subargs = tuple(subst[x] for x in subparams)\n                    arg = arg[subargs]\n            if isinstance(arg, tuple):\n                new_args.extend(arg)\n            else:\n                new_args.append(arg)\n\n        # args[0] occurs due to things like Z[[int, str, bool]] from PEP 612\n        if not isinstance(new_args[0], list):\n            t_result = new_args[-1]\n            t_args = new_args[:-1]\n            new_args = (t_args, t_result)\n        return _CallableGenericAlias(Callable, tuple(new_args))\n\n\ndef _is_typevarlike(arg):\n    obj = type(arg)\n    # looks like a TypeVar/ParamSpec\n    return (obj.__module__ == 'typing'\n            and obj.__name__ in {'ParamSpec', 'TypeVar'})\n\ndef _is_param_expr(obj):\n    \"\"\"Checks if obj matches either a list of types, ``...``, ``ParamSpec`` or\n    ``_ConcatenateGenericAlias`` from typing.py\n    \"\"\"\n    if obj is Ellipsis:\n        return True\n    if isinstance(obj, list):\n        return True\n    obj = type(obj)\n    names = ('ParamSpec', '_ConcatenateGenericAlias')\n    return obj.__module__ == 'typing' and any(obj.__name__ == name for name in names)\n\ndef _type_repr(obj):\n    \"\"\"Return the repr() of an object, special-casing types (internal helper).\n\n    Copied from :mod:`typing` since collections.abc\n    shouldn't depend on that module.\n    \"\"\"\n    if isinstance(obj, GenericAlias):\n        return repr(obj)\n    if isinstance(obj, type):\n        if obj.__module__ == 'builtins':\n            return obj.__qualname__\n        return f'{obj.__module__}.{obj.__qualname__}'\n    if obj is Ellipsis:\n        return '...'\n    if isinstance(obj, FunctionType):\n        return obj.__name__\n    return repr(obj)\n\n\nclass Callable(metaclass=ABCMeta):\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __call__(self, *args, **kwds):\n        return False\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Callable:\n            return _check_methods(C, \"__call__\")\n        return NotImplemented\n\n    __class_getitem__ = classmethod(_CallableGenericAlias)\n\n\n### SETS ###\n\n\nclass Set(Collection):\n    \"\"\"A set is a finite, iterable container.\n\n    This class provides concrete generic implementations of all\n    methods except for __contains__, __iter__ and __len__.\n\n    To override the comparisons (presumably for speed, as the\n    semantics are fixed), redefine __le__ and __ge__,\n    then the other operations will automatically follow suit.\n    \"\"\"\n\n    __slots__ = ()\n\n    def __le__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        if len(self) > len(other):\n            return False\n        for elem in self:\n            if elem not in other:\n                return False\n        return True\n\n    def __lt__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) < len(other) and self.__le__(other)\n\n    def __gt__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) > len(other) and self.__ge__(other)\n\n    def __ge__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        if len(self) < len(other):\n            return False\n        for elem in other:\n            if elem not in self:\n                return False\n        return True\n\n    def __eq__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) == len(other) and self.__le__(other)\n\n    @classmethod\n    def _from_iterable(cls, it):\n        '''Construct an instance of the class from any iterable input.\n\n        Must override this method if the class constructor signature\n        does not accept an iterable for an input.\n        '''\n        return cls(it)\n\n    def __and__(self, other):\n        if not isinstance(other, Iterable):\n            return NotImplemented\n        return self._from_iterable(value for value in other if value in self)\n\n    __rand__ = __and__\n\n    def isdisjoint(self, other):\n        'Return True if two sets have a null intersection.'\n        for value in other:\n            if value in self:\n                return False\n        return True\n\n    def __or__(self, other):\n        if not isinstance(other, Iterable):\n            return NotImplemented\n        chain = (e for s in (self, other) for e in s)\n        return self._from_iterable(chain)\n\n    __ror__ = __or__\n\n    def __sub__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return self._from_iterable(value for value in self\n                                   if value not in other)\n\n    def __rsub__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return self._from_iterable(value for value in other\n                                   if value not in self)\n\n    def __xor__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return (self - other) | (other - self)\n\n    __rxor__ = __xor__\n\n    def _hash(self):\n        \"\"\"Compute the hash value of a set.\n\n        Note that we don't define __hash__: not all sets are hashable.\n        But if you define a hashable set type, its __hash__ should\n        call this function.\n\n        This must be compatible __eq__.\n\n        All sets ought to compare equal if they contain the same\n        elements, regardless of how they are implemented, and\n        regardless of the order of the elements; so there's not much\n        freedom for __eq__ or __hash__.  We match the algorithm used\n        by the built-in frozenset type.\n        \"\"\"\n        MAX = sys.maxsize\n        MASK = 2 * MAX + 1\n        n = len(self)\n        h = 1927868237 * (n + 1)\n        h &= MASK\n        for x in self:\n            hx = hash(x)\n            h ^= (hx ^ (hx << 16) ^ 89869747)  * 3644798167\n            h &= MASK\n        h ^= (h >> 11) ^ (h >> 25)\n        h = h * 69069 + 907133923\n        h &= MASK\n        if h > MAX:\n            h -= MASK + 1\n        if h == -1:\n            h = 590923713\n        return h\n\n\nSet.register(frozenset)\n\n\nclass MutableSet(Set):\n    \"\"\"A mutable set is a finite, iterable container.\n\n    This class provides concrete generic implementations of all\n    methods except for __contains__, __iter__, __len__,\n    add(), and discard().\n\n    To override the comparisons (presumably for speed, as the\n    semantics are fixed), all you have to do is redefine __le__ and\n    then the other operations will automatically follow suit.\n    \"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def add(self, value):\n        \"\"\"Add an element.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def discard(self, value):\n        \"\"\"Remove an element.  Do not raise an exception if absent.\"\"\"\n        raise NotImplementedError\n\n    def remove(self, value):\n        \"\"\"Remove an element. If not a member, raise a KeyError.\"\"\"\n        if value not in self:\n            raise KeyError(value)\n        self.discard(value)\n\n    def pop(self):\n        \"\"\"Return the popped value.  Raise KeyError if empty.\"\"\"\n        it = iter(self)\n        try:\n            value = next(it)\n        except StopIteration:\n            raise KeyError from None\n        self.discard(value)\n        return value\n\n    def clear(self):\n        \"\"\"This is slow (creates N new iterators!) but effective.\"\"\"\n        try:\n            while True:\n                self.pop()\n        except KeyError:\n            pass\n\n    def __ior__(self, it):\n        for value in it:\n            self.add(value)\n        return self\n\n    def __iand__(self, it):\n        for value in (self - it):\n            self.discard(value)\n        return self\n\n    def __ixor__(self, it):\n        if it is self:\n            self.clear()\n        else:\n            if not isinstance(it, Set):\n                it = self._from_iterable(it)\n            for value in it:\n                if value in self:\n                    self.discard(value)\n                else:\n                    self.add(value)\n        return self\n\n    def __isub__(self, it):\n        if it is self:\n            self.clear()\n        else:\n            for value in it:\n                self.discard(value)\n        return self\n\n\nMutableSet.register(set)\n\n\n### MAPPINGS ###\n\nclass Mapping(Collection):\n    \"\"\"A Mapping is a generic container for associating key/value\n    pairs.\n\n    This class provides concrete generic implementations of all\n    methods except for __getitem__, __iter__, and __len__.\n    \"\"\"\n\n    __slots__ = ()\n\n    # Tell ABCMeta.__new__ that this class should have TPFLAGS_MAPPING set.\n    __abc_tpflags__ = 1 << 6 # Py_TPFLAGS_MAPPING\n\n    @abstractmethod\n    def __getitem__(self, key):\n        raise KeyError\n\n    def get(self, key, default=None):\n        'D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.'\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def __contains__(self, key):\n        try:\n            self[key]\n        except KeyError:\n            return False\n        else:\n            return True\n\n    def keys(self):\n        \"D.keys() -> a set-like object providing a view on D's keys\"\n        return KeysView(self)\n\n    def items(self):\n        \"D.items() -> a set-like object providing a view on D's items\"\n        return ItemsView(self)\n\n    def values(self):\n        \"D.values() -> an object providing a view on D's values\"\n        return ValuesView(self)\n\n    def __eq__(self, other):\n        if not isinstance(other, Mapping):\n            return NotImplemented\n        return dict(self.items()) == dict(other.items())\n\n    __reversed__ = None\n\nMapping.register(mappingproxy)\n\n\nclass MappingView(Sized):\n\n    __slots__ = '_mapping',\n\n    def __init__(self, mapping):\n        self._mapping = mapping\n\n    def __len__(self):\n        return len(self._mapping)\n\n    def __repr__(self):\n        return '{0.__class__.__name__}({0._mapping!r})'.format(self)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n\nclass KeysView(MappingView, Set):\n\n    __slots__ = ()\n\n    @classmethod\n    def _from_iterable(cls, it):\n        return set(it)\n\n    def __contains__(self, key):\n        return key in self._mapping\n\n    def __iter__(self):\n        yield from self._mapping\n\n\nKeysView.register(dict_keys)\n\n\nclass ItemsView(MappingView, Set):\n\n    __slots__ = ()\n\n    @classmethod\n    def _from_iterable(cls, it):\n        return set(it)\n\n    def __contains__(self, item):\n        key, value = item\n        try:\n            v = self._mapping[key]\n        except KeyError:\n            return False\n        else:\n            return v is value or v == value\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield (key, self._mapping[key])\n\n\nItemsView.register(dict_items)\n\n\nclass ValuesView(MappingView, Collection):\n\n    __slots__ = ()\n\n    def __contains__(self, value):\n        for key in self._mapping:\n            v = self._mapping[key]\n            if v is value or v == value:\n                return True\n        return False\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield self._mapping[key]\n\n\nValuesView.register(dict_values)\n\n\nclass MutableMapping(Mapping):\n    \"\"\"A MutableMapping is a generic container for associating\n    key/value pairs.\n\n    This class provides concrete generic implementations of all\n    methods except for __getitem__, __setitem__, __delitem__,\n    __iter__, and __len__.\n    \"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __setitem__(self, key, value):\n        raise KeyError\n\n    @abstractmethod\n    def __delitem__(self, key):\n        raise KeyError\n\n    __marker = object()\n\n    def pop(self, key, default=__marker):\n        '''D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n          If key is not found, d is returned if given, otherwise KeyError is raised.\n        '''\n        try:\n            value = self[key]\n        except KeyError:\n            if default is self.__marker:\n                raise\n            return default\n        else:\n            del self[key]\n            return value\n\n    def popitem(self):\n        '''D.popitem() -> (k, v), remove and return some (key, value) pair\n           as a 2-tuple; but raise KeyError if D is empty.\n        '''\n        try:\n            key = next(iter(self))\n        except StopIteration:\n            raise KeyError from None\n        value = self[key]\n        del self[key]\n        return key, value\n\n    def clear(self):\n        'D.clear() -> None.  Remove all items from D.'\n        try:\n            while True:\n                self.popitem()\n        except KeyError:\n            pass\n\n    def update(self, other=(), /, **kwds):\n        ''' D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.\n            If E present and has a .keys() method, does:     for k in E: D[k] = E[k]\n            If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v\n            In either case, this is followed by: for k, v in F.items(): D[k] = v\n        '''\n        if isinstance(other, Mapping):\n            for key in other:\n                self[key] = other[key]\n        elif hasattr(other, \"keys\"):\n            for key in other.keys():\n                self[key] = other[key]\n        else:\n            for key, value in other:\n                self[key] = value\n        for key, value in kwds.items():\n            self[key] = value\n\n    def setdefault(self, key, default=None):\n        'D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D'\n        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n        return default\n\n\nMutableMapping.register(dict)\n\n\n### SEQUENCES ###\n\nclass Sequence(Reversible, Collection):\n    \"\"\"All the operations on a read-only sequence.\n\n    Concrete subclasses must override __new__ or __init__,\n    __getitem__, and __len__.\n    \"\"\"\n\n    __slots__ = ()\n\n    # Tell ABCMeta.__new__ that this class should have TPFLAGS_SEQUENCE set.\n    __abc_tpflags__ = 1 << 5 # Py_TPFLAGS_SEQUENCE\n\n    @abstractmethod\n    def __getitem__(self, index):\n        raise IndexError\n\n    def __iter__(self):\n        i = 0\n        try:\n            while True:\n                v = self[i]\n                yield v\n                i += 1\n        except IndexError:\n            return\n\n    def __contains__(self, value):\n        for v in self:\n            if v is value or v == value:\n                return True\n        return False\n\n    def __reversed__(self):\n        for i in reversed(range(len(self))):\n            yield self[i]\n\n    def index(self, value, start=0, stop=None):\n        '''S.index(value, [start, [stop]]) -> integer -- return first index of value.\n           Raises ValueError if the value is not present.\n\n           Supporting start and stop arguments is optional, but\n           recommended.\n        '''\n        if start is not None and start < 0:\n            start = max(len(self) + start, 0)\n        if stop is not None and stop < 0:\n            stop += len(self)\n\n        i = start\n        while stop is None or i < stop:\n            try:\n                v = self[i]\n                if v is value or v == value:\n                    return i\n            except IndexError:\n                break\n            i += 1\n        raise ValueError\n\n    def count(self, value):\n        'S.count(value) -> integer -- return number of occurrences of value'\n        return sum(1 for v in self if v is value or v == value)\n\nSequence.register(tuple)\nSequence.register(str)\nSequence.register(range)\nSequence.register(memoryview)\n\n\nclass ByteString(Sequence):\n    \"\"\"This unifies bytes and bytearray.\n\n    XXX Should add all their methods.\n    \"\"\"\n\n    __slots__ = ()\n\nByteString.register(bytes)\nByteString.register(bytearray)\n\n\nclass MutableSequence(Sequence):\n    \"\"\"All the operations on a read-write sequence.\n\n    Concrete subclasses must provide __new__ or __init__,\n    __getitem__, __setitem__, __delitem__, __len__, and insert().\n    \"\"\"\n\n    __slots__ = ()\n\n    @abstractmethod\n    def __setitem__(self, index, value):\n        raise IndexError\n\n    @abstractmethod\n    def __delitem__(self, index):\n        raise IndexError\n\n    @abstractmethod\n    def insert(self, index, value):\n        'S.insert(index, value) -- insert value before index'\n        raise IndexError\n\n    def append(self, value):\n        'S.append(value) -- append value to the end of the sequence'\n        self.insert(len(self), value)\n\n    def clear(self):\n        'S.clear() -> None -- remove all items from S'\n        try:\n            while True:\n                self.pop()\n        except IndexError:\n            pass\n\n    def reverse(self):\n        'S.reverse() -- reverse *IN PLACE*'\n        n = len(self)\n        for i in range(n//2):\n            self[i], self[n-i-1] = self[n-i-1], self[i]\n\n    def extend(self, values):\n        'S.extend(iterable) -- extend sequence by appending elements from the iterable'\n        if values is self:\n            values = list(values)\n        for v in values:\n            self.append(v)\n\n    def pop(self, index=-1):\n        '''S.pop([index]) -> item -- remove and return item at index (default last).\n           Raise IndexError if list is empty or index is out of range.\n        '''\n        v = self[index]\n        del self[index]\n        return v\n\n    def remove(self, value):\n        '''S.remove(value) -- remove first occurrence of value.\n           Raise ValueError if the value is not present.\n        '''\n        del self[self.index(value)]\n\n    def __iadd__(self, values):\n        self.extend(values)\n        return self\n\n\nMutableSequence.register(list)\nMutableSequence.register(bytearray)  # Multiply inheriting, see ByteString\n", 1171], "D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py": ["\"\"\"Base implementation of event loop.\n\nThe event loop can be broken up into a multiplexer (the part\nresponsible for notifying us of I/O events) and the event loop proper,\nwhich wraps a multiplexer with functionality for scheduling callbacks,\nimmediately or at a given time in the future.\n\nWhenever a public API takes a callback, subsequent positional\narguments will be passed to the callback if/when it is called.  This\navoids the proliferation of trivial lambdas implementing closures.\nKeyword arguments for the callback are not supported; this is a\nconscious design decision, leaving the door open for keyword arguments\nto modify the meaning of the API call itself.\n\"\"\"\n\nimport collections\nimport collections.abc\nimport concurrent.futures\nimport functools\nimport heapq\nimport itertools\nimport os\nimport socket\nimport stat\nimport subprocess\nimport threading\nimport time\nimport traceback\nimport sys\nimport warnings\nimport weakref\n\ntry:\n    import ssl\nexcept ImportError:  # pragma: no cover\n    ssl = None\n\nfrom . import constants\nfrom . import coroutines\nfrom . import events\nfrom . import exceptions\nfrom . import futures\nfrom . import protocols\nfrom . import sslproto\nfrom . import staggered\nfrom . import tasks\nfrom . import transports\nfrom . import trsock\nfrom .log import logger\n\n\n__all__ = 'BaseEventLoop','Server',\n\n\n# Minimum number of _scheduled timer handles before cleanup of\n# cancelled handles is performed.\n_MIN_SCHEDULED_TIMER_HANDLES = 100\n\n# Minimum fraction of _scheduled timer handles that are cancelled\n# before cleanup of cancelled handles is performed.\n_MIN_CANCELLED_TIMER_HANDLES_FRACTION = 0.5\n\n\n_HAS_IPv6 = hasattr(socket, 'AF_INET6')\n\n# Maximum timeout passed to select to avoid OS limitations\nMAXIMUM_SELECT_TIMEOUT = 24 * 3600\n\n# Used for deprecation and removal of `loop.create_datagram_endpoint()`'s\n# *reuse_address* parameter\n_unset = object()\n\n\ndef _format_handle(handle):\n    cb = handle._callback\n    if isinstance(getattr(cb, '__self__', None), tasks.Task):\n        # format the task\n        return repr(cb.__self__)\n    else:\n        return str(handle)\n\n\ndef _format_pipe(fd):\n    if fd == subprocess.PIPE:\n        return '<pipe>'\n    elif fd == subprocess.STDOUT:\n        return '<stdout>'\n    else:\n        return repr(fd)\n\n\ndef _set_reuseport(sock):\n    if not hasattr(socket, 'SO_REUSEPORT'):\n        raise ValueError('reuse_port not supported by socket module')\n    else:\n        try:\n            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)\n        except OSError:\n            raise ValueError('reuse_port not supported by socket module, '\n                             'SO_REUSEPORT defined but not implemented.')\n\n\ndef _ipaddr_info(host, port, family, type, proto, flowinfo=0, scopeid=0):\n    # Try to skip getaddrinfo if \"host\" is already an IP. Users might have\n    # handled name resolution in their own code and pass in resolved IPs.\n    if not hasattr(socket, 'inet_pton'):\n        return\n\n    if proto not in {0, socket.IPPROTO_TCP, socket.IPPROTO_UDP} or \\\n            host is None:\n        return None\n\n    if type == socket.SOCK_STREAM:\n        proto = socket.IPPROTO_TCP\n    elif type == socket.SOCK_DGRAM:\n        proto = socket.IPPROTO_UDP\n    else:\n        return None\n\n    if port is None:\n        port = 0\n    elif isinstance(port, bytes) and port == b'':\n        port = 0\n    elif isinstance(port, str) and port == '':\n        port = 0\n    else:\n        # If port's a service name like \"http\", don't skip getaddrinfo.\n        try:\n            port = int(port)\n        except (TypeError, ValueError):\n            return None\n\n    if family == socket.AF_UNSPEC:\n        afs = [socket.AF_INET]\n        if _HAS_IPv6:\n            afs.append(socket.AF_INET6)\n    else:\n        afs = [family]\n\n    if isinstance(host, bytes):\n        host = host.decode('idna')\n    if '%' in host:\n        # Linux's inet_pton doesn't accept an IPv6 zone index after host,\n        # like '::1%lo0'.\n        return None\n\n    for af in afs:\n        try:\n            socket.inet_pton(af, host)\n            # The host has already been resolved.\n            if _HAS_IPv6 and af == socket.AF_INET6:\n                return af, type, proto, '', (host, port, flowinfo, scopeid)\n            else:\n                return af, type, proto, '', (host, port)\n        except OSError:\n            pass\n\n    # \"host\" is not an IP address.\n    return None\n\n\ndef _interleave_addrinfos(addrinfos, first_address_family_count=1):\n    \"\"\"Interleave list of addrinfo tuples by family.\"\"\"\n    # Group addresses by family\n    addrinfos_by_family = collections.OrderedDict()\n    for addr in addrinfos:\n        family = addr[0]\n        if family not in addrinfos_by_family:\n            addrinfos_by_family[family] = []\n        addrinfos_by_family[family].append(addr)\n    addrinfos_lists = list(addrinfos_by_family.values())\n\n    reordered = []\n    if first_address_family_count > 1:\n        reordered.extend(addrinfos_lists[0][:first_address_family_count - 1])\n        del addrinfos_lists[0][:first_address_family_count - 1]\n    reordered.extend(\n        a for a in itertools.chain.from_iterable(\n            itertools.zip_longest(*addrinfos_lists)\n        ) if a is not None)\n    return reordered\n\n\ndef _run_until_complete_cb(fut):\n    if not fut.cancelled():\n        exc = fut.exception()\n        if isinstance(exc, (SystemExit, KeyboardInterrupt)):\n            # Issue #22429: run_forever() already finished, no need to\n            # stop it.\n            return\n    futures._get_loop(fut).stop()\n\n\nif hasattr(socket, 'TCP_NODELAY'):\n    def _set_nodelay(sock):\n        if (sock.family in {socket.AF_INET, socket.AF_INET6} and\n                sock.type == socket.SOCK_STREAM and\n                sock.proto == socket.IPPROTO_TCP):\n            sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\nelse:\n    def _set_nodelay(sock):\n        pass\n\n\ndef _check_ssl_socket(sock):\n    if ssl is not None and isinstance(sock, ssl.SSLSocket):\n        raise TypeError(\"Socket cannot be of type SSLSocket\")\n\n\nclass _SendfileFallbackProtocol(protocols.Protocol):\n    def __init__(self, transp):\n        if not isinstance(transp, transports._FlowControlMixin):\n            raise TypeError(\"transport should be _FlowControlMixin instance\")\n        self._transport = transp\n        self._proto = transp.get_protocol()\n        self._should_resume_reading = transp.is_reading()\n        self._should_resume_writing = transp._protocol_paused\n        transp.pause_reading()\n        transp.set_protocol(self)\n        if self._should_resume_writing:\n            self._write_ready_fut = self._transport._loop.create_future()\n        else:\n            self._write_ready_fut = None\n\n    async def drain(self):\n        if self._transport.is_closing():\n            raise ConnectionError(\"Connection closed by peer\")\n        fut = self._write_ready_fut\n        if fut is None:\n            return\n        await fut\n\n    def connection_made(self, transport):\n        raise RuntimeError(\"Invalid state: \"\n                           \"connection should have been established already.\")\n\n    def connection_lost(self, exc):\n        if self._write_ready_fut is not None:\n            # Never happens if peer disconnects after sending the whole content\n            # Thus disconnection is always an exception from user perspective\n            if exc is None:\n                self._write_ready_fut.set_exception(\n                    ConnectionError(\"Connection is closed by peer\"))\n            else:\n                self._write_ready_fut.set_exception(exc)\n        self._proto.connection_lost(exc)\n\n    def pause_writing(self):\n        if self._write_ready_fut is not None:\n            return\n        self._write_ready_fut = self._transport._loop.create_future()\n\n    def resume_writing(self):\n        if self._write_ready_fut is None:\n            return\n        self._write_ready_fut.set_result(False)\n        self._write_ready_fut = None\n\n    def data_received(self, data):\n        raise RuntimeError(\"Invalid state: reading should be paused\")\n\n    def eof_received(self):\n        raise RuntimeError(\"Invalid state: reading should be paused\")\n\n    async def restore(self):\n        self._transport.set_protocol(self._proto)\n        if self._should_resume_reading:\n            self._transport.resume_reading()\n        if self._write_ready_fut is not None:\n            # Cancel the future.\n            # Basically it has no effect because protocol is switched back,\n            # no code should wait for it anymore.\n            self._write_ready_fut.cancel()\n        if self._should_resume_writing:\n            self._proto.resume_writing()\n\n\nclass Server(events.AbstractServer):\n\n    def __init__(self, loop, sockets, protocol_factory, ssl_context, backlog,\n                 ssl_handshake_timeout):\n        self._loop = loop\n        self._sockets = sockets\n        self._active_count = 0\n        self._waiters = []\n        self._protocol_factory = protocol_factory\n        self._backlog = backlog\n        self._ssl_context = ssl_context\n        self._ssl_handshake_timeout = ssl_handshake_timeout\n        self._serving = False\n        self._serving_forever_fut = None\n\n    def __repr__(self):\n        return f'<{self.__class__.__name__} sockets={self.sockets!r}>'\n\n    def _attach(self):\n        assert self._sockets is not None\n        self._active_count += 1\n\n    def _detach(self):\n        assert self._active_count > 0\n        self._active_count -= 1\n        if self._active_count == 0 and self._sockets is None:\n            self._wakeup()\n\n    def _wakeup(self):\n        waiters = self._waiters\n        self._waiters = None\n        for waiter in waiters:\n            if not waiter.done():\n                waiter.set_result(waiter)\n\n    def _start_serving(self):\n        if self._serving:\n            return\n        self._serving = True\n        for sock in self._sockets:\n            sock.listen(self._backlog)\n            self._loop._start_serving(\n                self._protocol_factory, sock, self._ssl_context,\n                self, self._backlog, self._ssl_handshake_timeout)\n\n    def get_loop(self):\n        return self._loop\n\n    def is_serving(self):\n        return self._serving\n\n    @property\n    def sockets(self):\n        if self._sockets is None:\n            return ()\n        return tuple(trsock.TransportSocket(s) for s in self._sockets)\n\n    def close(self):\n        sockets = self._sockets\n        if sockets is None:\n            return\n        self._sockets = None\n\n        for sock in sockets:\n            self._loop._stop_serving(sock)\n\n        self._serving = False\n\n        if (self._serving_forever_fut is not None and\n                not self._serving_forever_fut.done()):\n            self._serving_forever_fut.cancel()\n            self._serving_forever_fut = None\n\n        if self._active_count == 0:\n            self._wakeup()\n\n    async def start_serving(self):\n        self._start_serving()\n        # Skip one loop iteration so that all 'loop.add_reader'\n        # go through.\n        await tasks.sleep(0)\n\n    async def serve_forever(self):\n        if self._serving_forever_fut is not None:\n            raise RuntimeError(\n                f'server {self!r} is already being awaited on serve_forever()')\n        if self._sockets is None:\n            raise RuntimeError(f'server {self!r} is closed')\n\n        self._start_serving()\n        self._serving_forever_fut = self._loop.create_future()\n\n        try:\n            await self._serving_forever_fut\n        except exceptions.CancelledError:\n            try:\n                self.close()\n                await self.wait_closed()\n            finally:\n                raise\n        finally:\n            self._serving_forever_fut = None\n\n    async def wait_closed(self):\n        if self._sockets is None or self._waiters is None:\n            return\n        waiter = self._loop.create_future()\n        self._waiters.append(waiter)\n        await waiter\n\n\nclass BaseEventLoop(events.AbstractEventLoop):\n\n    def __init__(self):\n        self._timer_cancelled_count = 0\n        self._closed = False\n        self._stopping = False\n        self._ready = collections.deque()\n        self._scheduled = []\n        self._default_executor = None\n        self._internal_fds = 0\n        # Identifier of the thread running the event loop, or None if the\n        # event loop is not running\n        self._thread_id = None\n        self._clock_resolution = time.get_clock_info('monotonic').resolution\n        self._exception_handler = None\n        self.set_debug(coroutines._is_debug_mode())\n        # In debug mode, if the execution of a callback or a step of a task\n        # exceed this duration in seconds, the slow callback/task is logged.\n        self.slow_callback_duration = 0.1\n        self._current_handle = None\n        self._task_factory = None\n        self._coroutine_origin_tracking_enabled = False\n        self._coroutine_origin_tracking_saved_depth = None\n\n        # A weak set of all asynchronous generators that are\n        # being iterated by the loop.\n        self._asyncgens = weakref.WeakSet()\n        # Set to True when `loop.shutdown_asyncgens` is called.\n        self._asyncgens_shutdown_called = False\n        # Set to True when `loop.shutdown_default_executor` is called.\n        self._executor_shutdown_called = False\n\n    def __repr__(self):\n        return (\n            f'<{self.__class__.__name__} running={self.is_running()} '\n            f'closed={self.is_closed()} debug={self.get_debug()}>'\n        )\n\n    def create_future(self):\n        \"\"\"Create a Future object attached to the loop.\"\"\"\n        return futures.Future(loop=self)\n\n    def create_task(self, coro, *, name=None):\n        \"\"\"Schedule a coroutine object.\n\n        Return a task object.\n        \"\"\"\n        self._check_closed()\n        if self._task_factory is None:\n            task = tasks.Task(coro, loop=self, name=name)\n            if task._source_traceback:\n                del task._source_traceback[-1]\n        else:\n            task = self._task_factory(self, coro)\n            tasks._set_task_name(task, name)\n\n        return task\n\n    def set_task_factory(self, factory):\n        \"\"\"Set a task factory that will be used by loop.create_task().\n\n        If factory is None the default task factory will be set.\n\n        If factory is a callable, it should have a signature matching\n        '(loop, coro)', where 'loop' will be a reference to the active\n        event loop, 'coro' will be a coroutine object.  The callable\n        must return a Future.\n        \"\"\"\n        if factory is not None and not callable(factory):\n            raise TypeError('task factory must be a callable or None')\n        self._task_factory = factory\n\n    def get_task_factory(self):\n        \"\"\"Return a task factory, or None if the default one is in use.\"\"\"\n        return self._task_factory\n\n    def _make_socket_transport(self, sock, protocol, waiter=None, *,\n                               extra=None, server=None):\n        \"\"\"Create socket transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_ssl_transport(\n            self, rawsock, protocol, sslcontext, waiter=None,\n            *, server_side=False, server_hostname=None,\n            extra=None, server=None,\n            ssl_handshake_timeout=None,\n            call_connection_made=True):\n        \"\"\"Create SSL transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_datagram_transport(self, sock, protocol,\n                                 address=None, waiter=None, extra=None):\n        \"\"\"Create datagram transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_read_pipe_transport(self, pipe, protocol, waiter=None,\n                                  extra=None):\n        \"\"\"Create read pipe transport.\"\"\"\n        raise NotImplementedError\n\n    def _make_write_pipe_transport(self, pipe, protocol, waiter=None,\n                                   extra=None):\n        \"\"\"Create write pipe transport.\"\"\"\n        raise NotImplementedError\n\n    async def _make_subprocess_transport(self, protocol, args, shell,\n                                         stdin, stdout, stderr, bufsize,\n                                         extra=None, **kwargs):\n        \"\"\"Create subprocess transport.\"\"\"\n        raise NotImplementedError\n\n    def _write_to_self(self):\n        \"\"\"Write a byte to self-pipe, to wake up the event loop.\n\n        This may be called from a different thread.\n\n        The subclass is responsible for implementing the self-pipe.\n        \"\"\"\n        raise NotImplementedError\n\n    def _process_events(self, event_list):\n        \"\"\"Process selector events.\"\"\"\n        raise NotImplementedError\n\n    def _check_closed(self):\n        if self._closed:\n            raise RuntimeError('Event loop is closed')\n\n    def _check_default_executor(self):\n        if self._executor_shutdown_called:\n            raise RuntimeError('Executor shutdown has been called')\n\n    def _asyncgen_finalizer_hook(self, agen):\n        self._asyncgens.discard(agen)\n        if not self.is_closed():\n            self.call_soon_threadsafe(self.create_task, agen.aclose())\n\n    def _asyncgen_firstiter_hook(self, agen):\n        if self._asyncgens_shutdown_called:\n            warnings.warn(\n                f\"asynchronous generator {agen!r} was scheduled after \"\n                f\"loop.shutdown_asyncgens() call\",\n                ResourceWarning, source=self)\n\n        self._asyncgens.add(agen)\n\n    async def shutdown_asyncgens(self):\n        \"\"\"Shutdown all active asynchronous generators.\"\"\"\n        self._asyncgens_shutdown_called = True\n\n        if not len(self._asyncgens):\n            # If Python version is <3.6 or we don't have any asynchronous\n            # generators alive.\n            return\n\n        closing_agens = list(self._asyncgens)\n        self._asyncgens.clear()\n\n        results = await tasks.gather(\n            *[ag.aclose() for ag in closing_agens],\n            return_exceptions=True)\n\n        for result, agen in zip(results, closing_agens):\n            if isinstance(result, Exception):\n                self.call_exception_handler({\n                    'message': f'an error occurred during closing of '\n                               f'asynchronous generator {agen!r}',\n                    'exception': result,\n                    'asyncgen': agen\n                })\n\n    async def shutdown_default_executor(self):\n        \"\"\"Schedule the shutdown of the default executor.\"\"\"\n        self._executor_shutdown_called = True\n        if self._default_executor is None:\n            return\n        future = self.create_future()\n        thread = threading.Thread(target=self._do_shutdown, args=(future,))\n        thread.start()\n        try:\n            await future\n        finally:\n            thread.join()\n\n    def _do_shutdown(self, future):\n        try:\n            self._default_executor.shutdown(wait=True)\n            if not self.is_closed():\n                self.call_soon_threadsafe(future.set_result, None)\n        except Exception as ex:\n            if not self.is_closed():\n                self.call_soon_threadsafe(future.set_exception, ex)\n\n    def _check_running(self):\n        if self.is_running():\n            raise RuntimeError('This event loop is already running')\n        if events._get_running_loop() is not None:\n            raise RuntimeError(\n                'Cannot run the event loop while another loop is running')\n\n    def run_forever(self):\n        \"\"\"Run until stop() is called.\"\"\"\n        self._check_closed()\n        self._check_running()\n        self._set_coroutine_origin_tracking(self._debug)\n\n        old_agen_hooks = sys.get_asyncgen_hooks()\n        try:\n            self._thread_id = threading.get_ident()\n            sys.set_asyncgen_hooks(firstiter=self._asyncgen_firstiter_hook,\n                                   finalizer=self._asyncgen_finalizer_hook)\n\n            events._set_running_loop(self)\n            while True:\n                self._run_once()\n                if self._stopping:\n                    break\n        finally:\n            self._stopping = False\n            self._thread_id = None\n            events._set_running_loop(None)\n            self._set_coroutine_origin_tracking(False)\n            sys.set_asyncgen_hooks(*old_agen_hooks)\n\n    def run_until_complete(self, future):\n        \"\"\"Run until the Future is done.\n\n        If the argument is a coroutine, it is wrapped in a Task.\n\n        WARNING: It would be disastrous to call run_until_complete()\n        with the same coroutine twice -- it would wrap it in two\n        different Tasks and that can't be good.\n\n        Return the Future's result, or raise its exception.\n        \"\"\"\n        self._check_closed()\n        self._check_running()\n\n        new_task = not futures.isfuture(future)\n        future = tasks.ensure_future(future, loop=self)\n        if new_task:\n            # An exception is raised if the future didn't complete, so there\n            # is no need to log the \"destroy pending task\" message\n            future._log_destroy_pending = False\n\n        future.add_done_callback(_run_until_complete_cb)\n        try:\n            self.run_forever()\n        except:\n            if new_task and future.done() and not future.cancelled():\n                # The coroutine raised a BaseException. Consume the exception\n                # to not log a warning, the caller doesn't have access to the\n                # local task.\n                future.exception()\n            raise\n        finally:\n            future.remove_done_callback(_run_until_complete_cb)\n        if not future.done():\n            raise RuntimeError('Event loop stopped before Future completed.')\n\n        return future.result()\n\n    def stop(self):\n        \"\"\"Stop running the event loop.\n\n        Every callback already scheduled will still run.  This simply informs\n        run_forever to stop looping after a complete iteration.\n        \"\"\"\n        self._stopping = True\n\n    def close(self):\n        \"\"\"Close the event loop.\n\n        This clears the queues and shuts down the executor,\n        but does not wait for the executor to finish.\n\n        The event loop must not be running.\n        \"\"\"\n        if self.is_running():\n            raise RuntimeError(\"Cannot close a running event loop\")\n        if self._closed:\n            return\n        if self._debug:\n            logger.debug(\"Close %r\", self)\n        self._closed = True\n        self._ready.clear()\n        self._scheduled.clear()\n        self._executor_shutdown_called = True\n        executor = self._default_executor\n        if executor is not None:\n            self._default_executor = None\n            executor.shutdown(wait=False)\n\n    def is_closed(self):\n        \"\"\"Returns True if the event loop was closed.\"\"\"\n        return self._closed\n\n    def __del__(self, _warn=warnings.warn):\n        if not self.is_closed():\n            _warn(f\"unclosed event loop {self!r}\", ResourceWarning, source=self)\n            if not self.is_running():\n                self.close()\n\n    def is_running(self):\n        \"\"\"Returns True if the event loop is running.\"\"\"\n        return (self._thread_id is not None)\n\n    def time(self):\n        \"\"\"Return the time according to the event loop's clock.\n\n        This is a float expressed in seconds since an epoch, but the\n        epoch, precision, accuracy and drift are unspecified and may\n        differ per event loop.\n        \"\"\"\n        return time.monotonic()\n\n    def call_later(self, delay, callback, *args, context=None):\n        \"\"\"Arrange for a callback to be called at a given time.\n\n        Return a Handle: an opaque object with a cancel() method that\n        can be used to cancel the call.\n\n        The delay can be an int or float, expressed in seconds.  It is\n        always relative to the current time.\n\n        Each callback will be called exactly once.  If two callbacks\n        are scheduled for exactly the same time, it undefined which\n        will be called first.\n\n        Any positional arguments after the callback will be passed to\n        the callback when it is called.\n        \"\"\"\n        timer = self.call_at(self.time() + delay, callback, *args,\n                             context=context)\n        if timer._source_traceback:\n            del timer._source_traceback[-1]\n        return timer\n\n    def call_at(self, when, callback, *args, context=None):\n        \"\"\"Like call_later(), but uses an absolute time.\n\n        Absolute time corresponds to the event loop's time() method.\n        \"\"\"\n        self._check_closed()\n        if self._debug:\n            self._check_thread()\n            self._check_callback(callback, 'call_at')\n        timer = events.TimerHandle(when, callback, args, self, context)\n        if timer._source_traceback:\n            del timer._source_traceback[-1]\n        heapq.heappush(self._scheduled, timer)\n        timer._scheduled = True\n        return timer\n\n    def call_soon(self, callback, *args, context=None):\n        \"\"\"Arrange for a callback to be called as soon as possible.\n\n        This operates as a FIFO queue: callbacks are called in the\n        order in which they are registered.  Each callback will be\n        called exactly once.\n\n        Any positional arguments after the callback will be passed to\n        the callback when it is called.\n        \"\"\"\n        self._check_closed()\n        if self._debug:\n            self._check_thread()\n            self._check_callback(callback, 'call_soon')\n        handle = self._call_soon(callback, args, context)\n        if handle._source_traceback:\n            del handle._source_traceback[-1]\n        return handle\n\n    def _check_callback(self, callback, method):\n        if (coroutines.iscoroutine(callback) or\n                coroutines.iscoroutinefunction(callback)):\n            raise TypeError(\n                f\"coroutines cannot be used with {method}()\")\n        if not callable(callback):\n            raise TypeError(\n                f'a callable object was expected by {method}(), '\n                f'got {callback!r}')\n\n    def _call_soon(self, callback, args, context):\n        handle = events.Handle(callback, args, self, context)\n        if handle._source_traceback:\n            del handle._source_traceback[-1]\n        self._ready.append(handle)\n        return handle\n\n    def _check_thread(self):\n        \"\"\"Check that the current thread is the thread running the event loop.\n\n        Non-thread-safe methods of this class make this assumption and will\n        likely behave incorrectly when the assumption is violated.\n\n        Should only be called when (self._debug == True).  The caller is\n        responsible for checking this condition for performance reasons.\n        \"\"\"\n        if self._thread_id is None:\n            return\n        thread_id = threading.get_ident()\n        if thread_id != self._thread_id:\n            raise RuntimeError(\n                \"Non-thread-safe operation invoked on an event loop other \"\n                \"than the current one\")\n\n    def call_soon_threadsafe(self, callback, *args, context=None):\n        \"\"\"Like call_soon(), but thread-safe.\"\"\"\n        self._check_closed()\n        if self._debug:\n            self._check_callback(callback, 'call_soon_threadsafe')\n        handle = self._call_soon(callback, args, context)\n        if handle._source_traceback:\n            del handle._source_traceback[-1]\n        self._write_to_self()\n        return handle\n\n    def run_in_executor(self, executor, func, *args):\n        self._check_closed()\n        if self._debug:\n            self._check_callback(func, 'run_in_executor')\n        if executor is None:\n            executor = self._default_executor\n            # Only check when the default executor is being used\n            self._check_default_executor()\n            if executor is None:\n                executor = concurrent.futures.ThreadPoolExecutor(\n                    thread_name_prefix='asyncio'\n                )\n                self._default_executor = executor\n        return futures.wrap_future(\n            executor.submit(func, *args), loop=self)\n\n    def set_default_executor(self, executor):\n        if not isinstance(executor, concurrent.futures.ThreadPoolExecutor):\n            warnings.warn(\n                'Using the default executor that is not an instance of '\n                'ThreadPoolExecutor is deprecated and will be prohibited '\n                'in Python 3.9',\n                DeprecationWarning, 2)\n        self._default_executor = executor\n\n    def _getaddrinfo_debug(self, host, port, family, type, proto, flags):\n        msg = [f\"{host}:{port!r}\"]\n        if family:\n            msg.append(f'family={family!r}')\n        if type:\n            msg.append(f'type={type!r}')\n        if proto:\n            msg.append(f'proto={proto!r}')\n        if flags:\n            msg.append(f'flags={flags!r}')\n        msg = ', '.join(msg)\n        logger.debug('Get address info %s', msg)\n\n        t0 = self.time()\n        addrinfo = socket.getaddrinfo(host, port, family, type, proto, flags)\n        dt = self.time() - t0\n\n        msg = f'Getting address info {msg} took {dt * 1e3:.3f}ms: {addrinfo!r}'\n        if dt >= self.slow_callback_duration:\n            logger.info(msg)\n        else:\n            logger.debug(msg)\n        return addrinfo\n\n    async def getaddrinfo(self, host, port, *,\n                          family=0, type=0, proto=0, flags=0):\n        if self._debug:\n            getaddr_func = self._getaddrinfo_debug\n        else:\n            getaddr_func = socket.getaddrinfo\n\n        return await self.run_in_executor(\n            None, getaddr_func, host, port, family, type, proto, flags)\n\n    async def getnameinfo(self, sockaddr, flags=0):\n        return await self.run_in_executor(\n            None, socket.getnameinfo, sockaddr, flags)\n\n    async def sock_sendfile(self, sock, file, offset=0, count=None,\n                            *, fallback=True):\n        if self._debug and sock.gettimeout() != 0:\n            raise ValueError(\"the socket must be non-blocking\")\n        _check_ssl_socket(sock)\n        self._check_sendfile_params(sock, file, offset, count)\n        try:\n            return await self._sock_sendfile_native(sock, file,\n                                                    offset, count)\n        except exceptions.SendfileNotAvailableError as exc:\n            if not fallback:\n                raise\n        return await self._sock_sendfile_fallback(sock, file,\n                                                  offset, count)\n\n    async def _sock_sendfile_native(self, sock, file, offset, count):\n        # NB: sendfile syscall is not supported for SSL sockets and\n        # non-mmap files even if sendfile is supported by OS\n        raise exceptions.SendfileNotAvailableError(\n            f\"syscall sendfile is not available for socket {sock!r} \"\n            f\"and file {file!r} combination\")\n\n    async def _sock_sendfile_fallback(self, sock, file, offset, count):\n        if offset:\n            file.seek(offset)\n        blocksize = (\n            min(count, constants.SENDFILE_FALLBACK_READBUFFER_SIZE)\n            if count else constants.SENDFILE_FALLBACK_READBUFFER_SIZE\n        )\n        buf = bytearray(blocksize)\n        total_sent = 0\n        try:\n            while True:\n                if count:\n                    blocksize = min(count - total_sent, blocksize)\n                    if blocksize <= 0:\n                        break\n                view = memoryview(buf)[:blocksize]\n                read = await self.run_in_executor(None, file.readinto, view)\n                if not read:\n                    break  # EOF\n                await self.sock_sendall(sock, view[:read])\n                total_sent += read\n            return total_sent\n        finally:\n            if total_sent > 0 and hasattr(file, 'seek'):\n                file.seek(offset + total_sent)\n\n    def _check_sendfile_params(self, sock, file, offset, count):\n        if 'b' not in getattr(file, 'mode', 'b'):\n            raise ValueError(\"file should be opened in binary mode\")\n        if not sock.type == socket.SOCK_STREAM:\n            raise ValueError(\"only SOCK_STREAM type sockets are supported\")\n        if count is not None:\n            if not isinstance(count, int):\n                raise TypeError(\n                    \"count must be a positive integer (got {!r})\".format(count))\n            if count <= 0:\n                raise ValueError(\n                    \"count must be a positive integer (got {!r})\".format(count))\n        if not isinstance(offset, int):\n            raise TypeError(\n                \"offset must be a non-negative integer (got {!r})\".format(\n                    offset))\n        if offset < 0:\n            raise ValueError(\n                \"offset must be a non-negative integer (got {!r})\".format(\n                    offset))\n\n    async def _connect_sock(self, exceptions, addr_info, local_addr_infos=None):\n        \"\"\"Create, bind and connect one socket.\"\"\"\n        my_exceptions = []\n        exceptions.append(my_exceptions)\n        family, type_, proto, _, address = addr_info\n        sock = None\n        try:\n            sock = socket.socket(family=family, type=type_, proto=proto)\n            sock.setblocking(False)\n            if local_addr_infos is not None:\n                for _, _, _, _, laddr in local_addr_infos:\n                    try:\n                        sock.bind(laddr)\n                        break\n                    except OSError as exc:\n                        msg = (\n                            f'error while attempting to bind on '\n                            f'address {laddr!r}: '\n                            f'{exc.strerror.lower()}'\n                        )\n                        exc = OSError(exc.errno, msg)\n                        my_exceptions.append(exc)\n                else:  # all bind attempts failed\n                    raise my_exceptions.pop()\n            await self.sock_connect(sock, address)\n            return sock\n        except OSError as exc:\n            my_exceptions.append(exc)\n            if sock is not None:\n                sock.close()\n            raise\n        except:\n            if sock is not None:\n                sock.close()\n            raise\n        finally:\n            exceptions = my_exceptions = None\n\n    async def create_connection(\n            self, protocol_factory, host=None, port=None,\n            *, ssl=None, family=0,\n            proto=0, flags=0, sock=None,\n            local_addr=None, server_hostname=None,\n            ssl_handshake_timeout=None,\n            happy_eyeballs_delay=None, interleave=None):\n        \"\"\"Connect to a TCP server.\n\n        Create a streaming transport connection to a given internet host and\n        port: socket family AF_INET or socket.AF_INET6 depending on host (or\n        family if specified), socket type SOCK_STREAM. protocol_factory must be\n        a callable returning a protocol instance.\n\n        This method is a coroutine which will try to establish the connection\n        in the background.  When successful, the coroutine returns a\n        (transport, protocol) pair.\n        \"\"\"\n        if server_hostname is not None and not ssl:\n            raise ValueError('server_hostname is only meaningful with ssl')\n\n        if server_hostname is None and ssl:\n            # Use host as default for server_hostname.  It is an error\n            # if host is empty or not set, e.g. when an\n            # already-connected socket was passed or when only a port\n            # is given.  To avoid this error, you can pass\n            # server_hostname='' -- this will bypass the hostname\n            # check.  (This also means that if host is a numeric\n            # IP/IPv6 address, we will attempt to verify that exact\n            # address; this will probably fail, but it is possible to\n            # create a certificate for a specific IP address, so we\n            # don't judge it here.)\n            if not host:\n                raise ValueError('You must set server_hostname '\n                                 'when using ssl without a host')\n            server_hostname = host\n\n        if ssl_handshake_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if sock is not None:\n            _check_ssl_socket(sock)\n\n        if happy_eyeballs_delay is not None and interleave is None:\n            # If using happy eyeballs, default to interleave addresses by family\n            interleave = 1\n\n        if host is not None or port is not None:\n            if sock is not None:\n                raise ValueError(\n                    'host/port and sock can not be specified at the same time')\n\n            infos = await self._ensure_resolved(\n                (host, port), family=family,\n                type=socket.SOCK_STREAM, proto=proto, flags=flags, loop=self)\n            if not infos:\n                raise OSError('getaddrinfo() returned empty list')\n\n            if local_addr is not None:\n                laddr_infos = await self._ensure_resolved(\n                    local_addr, family=family,\n                    type=socket.SOCK_STREAM, proto=proto,\n                    flags=flags, loop=self)\n                if not laddr_infos:\n                    raise OSError('getaddrinfo() returned empty list')\n            else:\n                laddr_infos = None\n\n            if interleave:\n                infos = _interleave_addrinfos(infos, interleave)\n\n            exceptions = []\n            if happy_eyeballs_delay is None:\n                # not using happy eyeballs\n                for addrinfo in infos:\n                    try:\n                        sock = await self._connect_sock(\n                            exceptions, addrinfo, laddr_infos)\n                        break\n                    except OSError:\n                        continue\n            else:  # using happy eyeballs\n                sock, _, _ = await staggered.staggered_race(\n                    (functools.partial(self._connect_sock,\n                                       exceptions, addrinfo, laddr_infos)\n                     for addrinfo in infos),\n                    happy_eyeballs_delay, loop=self)\n\n            if sock is None:\n                exceptions = [exc for sub in exceptions for exc in sub]\n                try:\n                    if len(exceptions) == 1:\n                        raise exceptions[0]\n                    else:\n                        # If they all have the same str(), raise one.\n                        model = str(exceptions[0])\n                        if all(str(exc) == model for exc in exceptions):\n                            raise exceptions[0]\n                        # Raise a combined exception so the user can see all\n                        # the various error messages.\n                        raise OSError('Multiple exceptions: {}'.format(\n                            ', '.join(str(exc) for exc in exceptions)))\n                finally:\n                    exceptions = None\n\n        else:\n            if sock is None:\n                raise ValueError(\n                    'host and port was not specified and no sock specified')\n            if sock.type != socket.SOCK_STREAM:\n                # We allow AF_INET, AF_INET6, AF_UNIX as long as they\n                # are SOCK_STREAM.\n                # We support passing AF_UNIX sockets even though we have\n                # a dedicated API for that: create_unix_connection.\n                # Disallowing AF_UNIX in this method, breaks backwards\n                # compatibility.\n                raise ValueError(\n                    f'A Stream Socket was expected, got {sock!r}')\n\n        transport, protocol = await self._create_connection_transport(\n            sock, protocol_factory, ssl, server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout)\n        if self._debug:\n            # Get the socket from the transport because SSL transport closes\n            # the old socket and creates a new SSL socket\n            sock = transport.get_extra_info('socket')\n            logger.debug(\"%r connected to %s:%r: (%r, %r)\",\n                         sock, host, port, transport, protocol)\n        return transport, protocol\n\n    async def _create_connection_transport(\n            self, sock, protocol_factory, ssl,\n            server_hostname, server_side=False,\n            ssl_handshake_timeout=None):\n\n        sock.setblocking(False)\n\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        if ssl:\n            sslcontext = None if isinstance(ssl, bool) else ssl\n            transport = self._make_ssl_transport(\n                sock, protocol, sslcontext, waiter,\n                server_side=server_side, server_hostname=server_hostname,\n                ssl_handshake_timeout=ssl_handshake_timeout)\n        else:\n            transport = self._make_socket_transport(sock, protocol, waiter)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        return transport, protocol\n\n    async def sendfile(self, transport, file, offset=0, count=None,\n                       *, fallback=True):\n        \"\"\"Send a file to transport.\n\n        Return the total number of bytes which were sent.\n\n        The method uses high-performance os.sendfile if available.\n\n        file must be a regular file object opened in binary mode.\n\n        offset tells from where to start reading the file. If specified,\n        count is the total number of bytes to transmit as opposed to\n        sending the file until EOF is reached. File position is updated on\n        return or also in case of error in which case file.tell()\n        can be used to figure out the number of bytes\n        which were sent.\n\n        fallback set to True makes asyncio to manually read and send\n        the file when the platform does not support the sendfile syscall\n        (e.g. Windows or SSL socket on Unix).\n\n        Raise SendfileNotAvailableError if the system does not support\n        sendfile syscall and fallback is False.\n        \"\"\"\n        if transport.is_closing():\n            raise RuntimeError(\"Transport is closing\")\n        mode = getattr(transport, '_sendfile_compatible',\n                       constants._SendfileMode.UNSUPPORTED)\n        if mode is constants._SendfileMode.UNSUPPORTED:\n            raise RuntimeError(\n                f\"sendfile is not supported for transport {transport!r}\")\n        if mode is constants._SendfileMode.TRY_NATIVE:\n            try:\n                return await self._sendfile_native(transport, file,\n                                                   offset, count)\n            except exceptions.SendfileNotAvailableError as exc:\n                if not fallback:\n                    raise\n\n        if not fallback:\n            raise RuntimeError(\n                f\"fallback is disabled and native sendfile is not \"\n                f\"supported for transport {transport!r}\")\n\n        return await self._sendfile_fallback(transport, file,\n                                             offset, count)\n\n    async def _sendfile_native(self, transp, file, offset, count):\n        raise exceptions.SendfileNotAvailableError(\n            \"sendfile syscall is not supported\")\n\n    async def _sendfile_fallback(self, transp, file, offset, count):\n        if offset:\n            file.seek(offset)\n        blocksize = min(count, 16384) if count else 16384\n        buf = bytearray(blocksize)\n        total_sent = 0\n        proto = _SendfileFallbackProtocol(transp)\n        try:\n            while True:\n                if count:\n                    blocksize = min(count - total_sent, blocksize)\n                    if blocksize <= 0:\n                        return total_sent\n                view = memoryview(buf)[:blocksize]\n                read = await self.run_in_executor(None, file.readinto, view)\n                if not read:\n                    return total_sent  # EOF\n                await proto.drain()\n                transp.write(view[:read])\n                total_sent += read\n        finally:\n            if total_sent > 0 and hasattr(file, 'seek'):\n                file.seek(offset + total_sent)\n            await proto.restore()\n\n    async def start_tls(self, transport, protocol, sslcontext, *,\n                        server_side=False,\n                        server_hostname=None,\n                        ssl_handshake_timeout=None):\n        \"\"\"Upgrade transport to TLS.\n\n        Return a new transport that *protocol* should start using\n        immediately.\n        \"\"\"\n        if ssl is None:\n            raise RuntimeError('Python ssl module is not available')\n\n        if not isinstance(sslcontext, ssl.SSLContext):\n            raise TypeError(\n                f'sslcontext is expected to be an instance of ssl.SSLContext, '\n                f'got {sslcontext!r}')\n\n        if not getattr(transport, '_start_tls_compatible', False):\n            raise TypeError(\n                f'transport {transport!r} is not supported by start_tls()')\n\n        waiter = self.create_future()\n        ssl_protocol = sslproto.SSLProtocol(\n            self, protocol, sslcontext, waiter,\n            server_side, server_hostname,\n            ssl_handshake_timeout=ssl_handshake_timeout,\n            call_connection_made=False)\n\n        # Pause early so that \"ssl_protocol.data_received()\" doesn't\n        # have a chance to get called before \"ssl_protocol.connection_made()\".\n        transport.pause_reading()\n\n        transport.set_protocol(ssl_protocol)\n        conmade_cb = self.call_soon(ssl_protocol.connection_made, transport)\n        resume_cb = self.call_soon(transport.resume_reading)\n\n        try:\n            await waiter\n        except BaseException:\n            transport.close()\n            conmade_cb.cancel()\n            resume_cb.cancel()\n            raise\n\n        return ssl_protocol._app_transport\n\n    async def create_datagram_endpoint(self, protocol_factory,\n                                       local_addr=None, remote_addr=None, *,\n                                       family=0, proto=0, flags=0,\n                                       reuse_address=_unset, reuse_port=None,\n                                       allow_broadcast=None, sock=None):\n        \"\"\"Create datagram connection.\"\"\"\n        if sock is not None:\n            if sock.type != socket.SOCK_DGRAM:\n                raise ValueError(\n                    f'A UDP Socket was expected, got {sock!r}')\n            if (local_addr or remote_addr or\n                    family or proto or flags or\n                    reuse_port or allow_broadcast):\n                # show the problematic kwargs in exception msg\n                opts = dict(local_addr=local_addr, remote_addr=remote_addr,\n                            family=family, proto=proto, flags=flags,\n                            reuse_address=reuse_address, reuse_port=reuse_port,\n                            allow_broadcast=allow_broadcast)\n                problems = ', '.join(f'{k}={v}' for k, v in opts.items() if v)\n                raise ValueError(\n                    f'socket modifier keyword arguments can not be used '\n                    f'when sock is specified. ({problems})')\n            sock.setblocking(False)\n            r_addr = None\n        else:\n            if not (local_addr or remote_addr):\n                if family == 0:\n                    raise ValueError('unexpected address family')\n                addr_pairs_info = (((family, proto), (None, None)),)\n            elif hasattr(socket, 'AF_UNIX') and family == socket.AF_UNIX:\n                for addr in (local_addr, remote_addr):\n                    if addr is not None and not isinstance(addr, str):\n                        raise TypeError('string is expected')\n\n                if local_addr and local_addr[0] not in (0, '\\x00'):\n                    try:\n                        if stat.S_ISSOCK(os.stat(local_addr).st_mode):\n                            os.remove(local_addr)\n                    except FileNotFoundError:\n                        pass\n                    except OSError as err:\n                        # Directory may have permissions only to create socket.\n                        logger.error('Unable to check or remove stale UNIX '\n                                     'socket %r: %r',\n                                     local_addr, err)\n\n                addr_pairs_info = (((family, proto),\n                                    (local_addr, remote_addr)), )\n            else:\n                # join address by (family, protocol)\n                addr_infos = {}  # Using order preserving dict\n                for idx, addr in ((0, local_addr), (1, remote_addr)):\n                    if addr is not None:\n                        assert isinstance(addr, tuple) and len(addr) == 2, (\n                            '2-tuple is expected')\n\n                        infos = await self._ensure_resolved(\n                            addr, family=family, type=socket.SOCK_DGRAM,\n                            proto=proto, flags=flags, loop=self)\n                        if not infos:\n                            raise OSError('getaddrinfo() returned empty list')\n\n                        for fam, _, pro, _, address in infos:\n                            key = (fam, pro)\n                            if key not in addr_infos:\n                                addr_infos[key] = [None, None]\n                            addr_infos[key][idx] = address\n\n                # each addr has to have info for each (family, proto) pair\n                addr_pairs_info = [\n                    (key, addr_pair) for key, addr_pair in addr_infos.items()\n                    if not ((local_addr and addr_pair[0] is None) or\n                            (remote_addr and addr_pair[1] is None))]\n\n                if not addr_pairs_info:\n                    raise ValueError('can not get address information')\n\n            exceptions = []\n\n            # bpo-37228\n            if reuse_address is not _unset:\n                if reuse_address:\n                    raise ValueError(\"Passing `reuse_address=True` is no \"\n                                     \"longer supported, as the usage of \"\n                                     \"SO_REUSEPORT in UDP poses a significant \"\n                                     \"security concern.\")\n                else:\n                    warnings.warn(\"The *reuse_address* parameter has been \"\n                                  \"deprecated as of 3.5.10 and is scheduled \"\n                                  \"for removal in 3.11.\", DeprecationWarning,\n                                  stacklevel=2)\n\n            for ((family, proto),\n                 (local_address, remote_address)) in addr_pairs_info:\n                sock = None\n                r_addr = None\n                try:\n                    sock = socket.socket(\n                        family=family, type=socket.SOCK_DGRAM, proto=proto)\n                    if reuse_port:\n                        _set_reuseport(sock)\n                    if allow_broadcast:\n                        sock.setsockopt(\n                            socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n                    sock.setblocking(False)\n\n                    if local_addr:\n                        sock.bind(local_address)\n                    if remote_addr:\n                        if not allow_broadcast:\n                            await self.sock_connect(sock, remote_address)\n                        r_addr = remote_address\n                except OSError as exc:\n                    if sock is not None:\n                        sock.close()\n                    exceptions.append(exc)\n                except:\n                    if sock is not None:\n                        sock.close()\n                    raise\n                else:\n                    break\n            else:\n                raise exceptions[0]\n\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        transport = self._make_datagram_transport(\n            sock, protocol, r_addr, waiter)\n        if self._debug:\n            if local_addr:\n                logger.info(\"Datagram endpoint local_addr=%r remote_addr=%r \"\n                            \"created: (%r, %r)\",\n                            local_addr, remote_addr, transport, protocol)\n            else:\n                logger.debug(\"Datagram endpoint remote_addr=%r created: \"\n                             \"(%r, %r)\",\n                             remote_addr, transport, protocol)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        return transport, protocol\n\n    async def _ensure_resolved(self, address, *,\n                               family=0, type=socket.SOCK_STREAM,\n                               proto=0, flags=0, loop):\n        host, port = address[:2]\n        info = _ipaddr_info(host, port, family, type, proto, *address[2:])\n        if info is not None:\n            # \"host\" is already a resolved IP.\n            return [info]\n        else:\n            return await loop.getaddrinfo(host, port, family=family, type=type,\n                                          proto=proto, flags=flags)\n\n    async def _create_server_getaddrinfo(self, host, port, family, flags):\n        infos = await self._ensure_resolved((host, port), family=family,\n                                            type=socket.SOCK_STREAM,\n                                            flags=flags, loop=self)\n        if not infos:\n            raise OSError(f'getaddrinfo({host!r}) returned empty list')\n        return infos\n\n    async def create_server(\n            self, protocol_factory, host=None, port=None,\n            *,\n            family=socket.AF_UNSPEC,\n            flags=socket.AI_PASSIVE,\n            sock=None,\n            backlog=100,\n            ssl=None,\n            reuse_address=None,\n            reuse_port=None,\n            ssl_handshake_timeout=None,\n            start_serving=True):\n        \"\"\"Create a TCP server.\n\n        The host parameter can be a string, in that case the TCP server is\n        bound to host and port.\n\n        The host parameter can also be a sequence of strings and in that case\n        the TCP server is bound to all hosts of the sequence. If a host\n        appears multiple times (possibly indirectly e.g. when hostnames\n        resolve to the same IP address), the server is only bound once to that\n        host.\n\n        Return a Server object which can be used to stop the service.\n\n        This method is a coroutine.\n        \"\"\"\n        if isinstance(ssl, bool):\n            raise TypeError('ssl argument must be an SSLContext or None')\n\n        if ssl_handshake_timeout is not None and ssl is None:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if sock is not None:\n            _check_ssl_socket(sock)\n\n        if host is not None or port is not None:\n            if sock is not None:\n                raise ValueError(\n                    'host/port and sock can not be specified at the same time')\n\n            if reuse_address is None:\n                reuse_address = os.name == 'posix' and sys.platform != 'cygwin'\n            sockets = []\n            if host == '':\n                hosts = [None]\n            elif (isinstance(host, str) or\n                  not isinstance(host, collections.abc.Iterable)):\n                hosts = [host]\n            else:\n                hosts = host\n\n            fs = [self._create_server_getaddrinfo(host, port, family=family,\n                                                  flags=flags)\n                  for host in hosts]\n            infos = await tasks.gather(*fs)\n            infos = set(itertools.chain.from_iterable(infos))\n\n            completed = False\n            try:\n                for res in infos:\n                    af, socktype, proto, canonname, sa = res\n                    try:\n                        sock = socket.socket(af, socktype, proto)\n                    except socket.error:\n                        # Assume it's a bad family/type/protocol combination.\n                        if self._debug:\n                            logger.warning('create_server() failed to create '\n                                           'socket.socket(%r, %r, %r)',\n                                           af, socktype, proto, exc_info=True)\n                        continue\n                    sockets.append(sock)\n                    if reuse_address:\n                        sock.setsockopt(\n                            socket.SOL_SOCKET, socket.SO_REUSEADDR, True)\n                    if reuse_port:\n                        _set_reuseport(sock)\n                    # Disable IPv4/IPv6 dual stack support (enabled by\n                    # default on Linux) which makes a single socket\n                    # listen on both address families.\n                    if (_HAS_IPv6 and\n                            af == socket.AF_INET6 and\n                            hasattr(socket, 'IPPROTO_IPV6')):\n                        sock.setsockopt(socket.IPPROTO_IPV6,\n                                        socket.IPV6_V6ONLY,\n                                        True)\n                    try:\n                        sock.bind(sa)\n                    except OSError as err:\n                        raise OSError(err.errno, 'error while attempting '\n                                      'to bind on address %r: %s'\n                                      % (sa, err.strerror.lower())) from None\n                completed = True\n            finally:\n                if not completed:\n                    for sock in sockets:\n                        sock.close()\n        else:\n            if sock is None:\n                raise ValueError('Neither host/port nor sock were specified')\n            if sock.type != socket.SOCK_STREAM:\n                raise ValueError(f'A Stream Socket was expected, got {sock!r}')\n            sockets = [sock]\n\n        for sock in sockets:\n            sock.setblocking(False)\n\n        server = Server(self, sockets, protocol_factory,\n                        ssl, backlog, ssl_handshake_timeout)\n        if start_serving:\n            server._start_serving()\n            # Skip one loop iteration so that all 'loop.add_reader'\n            # go through.\n            await tasks.sleep(0)\n\n        if self._debug:\n            logger.info(\"%r is serving\", server)\n        return server\n\n    async def connect_accepted_socket(\n            self, protocol_factory, sock,\n            *, ssl=None,\n            ssl_handshake_timeout=None):\n        if sock.type != socket.SOCK_STREAM:\n            raise ValueError(f'A Stream Socket was expected, got {sock!r}')\n\n        if ssl_handshake_timeout is not None and not ssl:\n            raise ValueError(\n                'ssl_handshake_timeout is only meaningful with ssl')\n\n        if sock is not None:\n            _check_ssl_socket(sock)\n\n        transport, protocol = await self._create_connection_transport(\n            sock, protocol_factory, ssl, '', server_side=True,\n            ssl_handshake_timeout=ssl_handshake_timeout)\n        if self._debug:\n            # Get the socket from the transport because SSL transport closes\n            # the old socket and creates a new SSL socket\n            sock = transport.get_extra_info('socket')\n            logger.debug(\"%r handled: (%r, %r)\", sock, transport, protocol)\n        return transport, protocol\n\n    async def connect_read_pipe(self, protocol_factory, pipe):\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        transport = self._make_read_pipe_transport(pipe, protocol, waiter)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        if self._debug:\n            logger.debug('Read pipe %r connected: (%r, %r)',\n                         pipe.fileno(), transport, protocol)\n        return transport, protocol\n\n    async def connect_write_pipe(self, protocol_factory, pipe):\n        protocol = protocol_factory()\n        waiter = self.create_future()\n        transport = self._make_write_pipe_transport(pipe, protocol, waiter)\n\n        try:\n            await waiter\n        except:\n            transport.close()\n            raise\n\n        if self._debug:\n            logger.debug('Write pipe %r connected: (%r, %r)',\n                         pipe.fileno(), transport, protocol)\n        return transport, protocol\n\n    def _log_subprocess(self, msg, stdin, stdout, stderr):\n        info = [msg]\n        if stdin is not None:\n            info.append(f'stdin={_format_pipe(stdin)}')\n        if stdout is not None and stderr == subprocess.STDOUT:\n            info.append(f'stdout=stderr={_format_pipe(stdout)}')\n        else:\n            if stdout is not None:\n                info.append(f'stdout={_format_pipe(stdout)}')\n            if stderr is not None:\n                info.append(f'stderr={_format_pipe(stderr)}')\n        logger.debug(' '.join(info))\n\n    async def subprocess_shell(self, protocol_factory, cmd, *,\n                               stdin=subprocess.PIPE,\n                               stdout=subprocess.PIPE,\n                               stderr=subprocess.PIPE,\n                               universal_newlines=False,\n                               shell=True, bufsize=0,\n                               encoding=None, errors=None, text=None,\n                               **kwargs):\n        if not isinstance(cmd, (bytes, str)):\n            raise ValueError(\"cmd must be a string\")\n        if universal_newlines:\n            raise ValueError(\"universal_newlines must be False\")\n        if not shell:\n            raise ValueError(\"shell must be True\")\n        if bufsize != 0:\n            raise ValueError(\"bufsize must be 0\")\n        if text:\n            raise ValueError(\"text must be False\")\n        if encoding is not None:\n            raise ValueError(\"encoding must be None\")\n        if errors is not None:\n            raise ValueError(\"errors must be None\")\n\n        protocol = protocol_factory()\n        debug_log = None\n        if self._debug:\n            # don't log parameters: they may contain sensitive information\n            # (password) and may be too long\n            debug_log = 'run shell command %r' % cmd\n            self._log_subprocess(debug_log, stdin, stdout, stderr)\n        transport = await self._make_subprocess_transport(\n            protocol, cmd, True, stdin, stdout, stderr, bufsize, **kwargs)\n        if self._debug and debug_log is not None:\n            logger.info('%s: %r', debug_log, transport)\n        return transport, protocol\n\n    async def subprocess_exec(self, protocol_factory, program, *args,\n                              stdin=subprocess.PIPE, stdout=subprocess.PIPE,\n                              stderr=subprocess.PIPE, universal_newlines=False,\n                              shell=False, bufsize=0,\n                              encoding=None, errors=None, text=None,\n                              **kwargs):\n        if universal_newlines:\n            raise ValueError(\"universal_newlines must be False\")\n        if shell:\n            raise ValueError(\"shell must be False\")\n        if bufsize != 0:\n            raise ValueError(\"bufsize must be 0\")\n        if text:\n            raise ValueError(\"text must be False\")\n        if encoding is not None:\n            raise ValueError(\"encoding must be None\")\n        if errors is not None:\n            raise ValueError(\"errors must be None\")\n\n        popen_args = (program,) + args\n        protocol = protocol_factory()\n        debug_log = None\n        if self._debug:\n            # don't log parameters: they may contain sensitive information\n            # (password) and may be too long\n            debug_log = f'execute program {program!r}'\n            self._log_subprocess(debug_log, stdin, stdout, stderr)\n        transport = await self._make_subprocess_transport(\n            protocol, popen_args, False, stdin, stdout, stderr,\n            bufsize, **kwargs)\n        if self._debug and debug_log is not None:\n            logger.info('%s: %r', debug_log, transport)\n        return transport, protocol\n\n    def get_exception_handler(self):\n        \"\"\"Return an exception handler, or None if the default one is in use.\n        \"\"\"\n        return self._exception_handler\n\n    def set_exception_handler(self, handler):\n        \"\"\"Set handler as the new event loop exception handler.\n\n        If handler is None, the default exception handler will\n        be set.\n\n        If handler is a callable object, it should have a\n        signature matching '(loop, context)', where 'loop'\n        will be a reference to the active event loop, 'context'\n        will be a dict object (see `call_exception_handler()`\n        documentation for details about context).\n        \"\"\"\n        if handler is not None and not callable(handler):\n            raise TypeError(f'A callable object or None is expected, '\n                            f'got {handler!r}')\n        self._exception_handler = handler\n\n    def default_exception_handler(self, context):\n        \"\"\"Default exception handler.\n\n        This is called when an exception occurs and no exception\n        handler is set, and can be called by a custom exception\n        handler that wants to defer to the default behavior.\n\n        This default handler logs the error message and other\n        context-dependent information.  In debug mode, a truncated\n        stack trace is also appended showing where the given object\n        (e.g. a handle or future or task) was created, if any.\n\n        The context parameter has the same meaning as in\n        `call_exception_handler()`.\n        \"\"\"\n        message = context.get('message')\n        if not message:\n            message = 'Unhandled exception in event loop'\n\n        exception = context.get('exception')\n        if exception is not None:\n            exc_info = (type(exception), exception, exception.__traceback__)\n        else:\n            exc_info = False\n\n        if ('source_traceback' not in context and\n                self._current_handle is not None and\n                self._current_handle._source_traceback):\n            context['handle_traceback'] = \\\n                self._current_handle._source_traceback\n\n        log_lines = [message]\n        for key in sorted(context):\n            if key in {'message', 'exception'}:\n                continue\n            value = context[key]\n            if key == 'source_traceback':\n                tb = ''.join(traceback.format_list(value))\n                value = 'Object created at (most recent call last):\\n'\n                value += tb.rstrip()\n            elif key == 'handle_traceback':\n                tb = ''.join(traceback.format_list(value))\n                value = 'Handle created at (most recent call last):\\n'\n                value += tb.rstrip()\n            else:\n                value = repr(value)\n            log_lines.append(f'{key}: {value}')\n\n        logger.error('\\n'.join(log_lines), exc_info=exc_info)\n\n    def call_exception_handler(self, context):\n        \"\"\"Call the current event loop's exception handler.\n\n        The context argument is a dict containing the following keys:\n\n        - 'message': Error message;\n        - 'exception' (optional): Exception object;\n        - 'future' (optional): Future instance;\n        - 'task' (optional): Task instance;\n        - 'handle' (optional): Handle instance;\n        - 'protocol' (optional): Protocol instance;\n        - 'transport' (optional): Transport instance;\n        - 'socket' (optional): Socket instance;\n        - 'asyncgen' (optional): Asynchronous generator that caused\n                                 the exception.\n\n        New keys maybe introduced in the future.\n\n        Note: do not overload this method in an event loop subclass.\n        For custom exception handling, use the\n        `set_exception_handler()` method.\n        \"\"\"\n        if self._exception_handler is None:\n            try:\n                self.default_exception_handler(context)\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException:\n                # Second protection layer for unexpected errors\n                # in the default implementation, as well as for subclassed\n                # event loops with overloaded \"default_exception_handler\".\n                logger.error('Exception in default exception handler',\n                             exc_info=True)\n        else:\n            try:\n                self._exception_handler(self, context)\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                # Exception in the user set custom exception handler.\n                try:\n                    # Let's try default handler.\n                    self.default_exception_handler({\n                        'message': 'Unhandled error in exception handler',\n                        'exception': exc,\n                        'context': context,\n                    })\n                except (SystemExit, KeyboardInterrupt):\n                    raise\n                except BaseException:\n                    # Guard 'default_exception_handler' in case it is\n                    # overloaded.\n                    logger.error('Exception in default exception handler '\n                                 'while handling an unexpected error '\n                                 'in custom exception handler',\n                                 exc_info=True)\n\n    def _add_callback(self, handle):\n        \"\"\"Add a Handle to _scheduled (TimerHandle) or _ready.\"\"\"\n        assert isinstance(handle, events.Handle), 'A Handle is required here'\n        if handle._cancelled:\n            return\n        assert not isinstance(handle, events.TimerHandle)\n        self._ready.append(handle)\n\n    def _add_callback_signalsafe(self, handle):\n        \"\"\"Like _add_callback() but called from a signal handler.\"\"\"\n        self._add_callback(handle)\n        self._write_to_self()\n\n    def _timer_handle_cancelled(self, handle):\n        \"\"\"Notification that a TimerHandle has been cancelled.\"\"\"\n        if handle._scheduled:\n            self._timer_cancelled_count += 1\n\n    def _run_once(self):\n        \"\"\"Run one full iteration of the event loop.\n\n        This calls all currently ready callbacks, polls for I/O,\n        schedules the resulting callbacks, and finally schedules\n        'call_later' callbacks.\n        \"\"\"\n\n        sched_count = len(self._scheduled)\n        if (sched_count > _MIN_SCHEDULED_TIMER_HANDLES and\n            self._timer_cancelled_count / sched_count >\n                _MIN_CANCELLED_TIMER_HANDLES_FRACTION):\n            # Remove delayed calls that were cancelled if their number\n            # is too high\n            new_scheduled = []\n            for handle in self._scheduled:\n                if handle._cancelled:\n                    handle._scheduled = False\n                else:\n                    new_scheduled.append(handle)\n\n            heapq.heapify(new_scheduled)\n            self._scheduled = new_scheduled\n            self._timer_cancelled_count = 0\n        else:\n            # Remove delayed calls that were cancelled from head of queue.\n            while self._scheduled and self._scheduled[0]._cancelled:\n                self._timer_cancelled_count -= 1\n                handle = heapq.heappop(self._scheduled)\n                handle._scheduled = False\n\n        timeout = None\n        if self._ready or self._stopping:\n            timeout = 0\n        elif self._scheduled:\n            # Compute the desired timeout.\n            when = self._scheduled[0]._when\n            timeout = min(max(0, when - self.time()), MAXIMUM_SELECT_TIMEOUT)\n\n        event_list = self._selector.select(timeout)\n        self._process_events(event_list)\n        # Needed to break cycles when an exception occurs.\n        event_list = None\n\n        # Handle 'later' callbacks that are ready.\n        end_time = self.time() + self._clock_resolution\n        while self._scheduled:\n            handle = self._scheduled[0]\n            if handle._when >= end_time:\n                break\n            handle = heapq.heappop(self._scheduled)\n            handle._scheduled = False\n            self._ready.append(handle)\n\n        # This is the only place where callbacks are actually *called*.\n        # All other places just add them to ready.\n        # Note: We run all currently scheduled callbacks, but not any\n        # callbacks scheduled by callbacks run this time around --\n        # they will be run the next time (after another I/O poll).\n        # Use an idiom that is thread-safe without using locks.\n        ntodo = len(self._ready)\n        for i in range(ntodo):\n            handle = self._ready.popleft()\n            if handle._cancelled:\n                continue\n            if self._debug:\n                try:\n                    self._current_handle = handle\n                    t0 = self.time()\n                    handle._run()\n                    dt = self.time() - t0\n                    if dt >= self.slow_callback_duration:\n                        logger.warning('Executing %s took %.3f seconds',\n                                       _format_handle(handle), dt)\n                finally:\n                    self._current_handle = None\n            else:\n                handle._run()\n        handle = None  # Needed to break cycles when an exception occurs.\n\n    def _set_coroutine_origin_tracking(self, enabled):\n        if bool(enabled) == bool(self._coroutine_origin_tracking_enabled):\n            return\n\n        if enabled:\n            self._coroutine_origin_tracking_saved_depth = (\n                sys.get_coroutine_origin_tracking_depth())\n            sys.set_coroutine_origin_tracking_depth(\n                constants.DEBUG_STACK_DEPTH)\n        else:\n            sys.set_coroutine_origin_tracking_depth(\n                self._coroutine_origin_tracking_saved_depth)\n\n        self._coroutine_origin_tracking_enabled = enabled\n\n    def get_debug(self):\n        return self._debug\n\n    def set_debug(self, enabled):\n        self._debug = enabled\n\n        if self.is_running():\n            self.call_soon_threadsafe(self._set_coroutine_origin_tracking, enabled)\n", 1931], "D:\\Program\\anaconda3\\lib\\logging\\__init__.py": ["# Copyright 2001-2019 by Vinay Sajip. All Rights Reserved.\n#\n# Permission to use, copy, modify, and distribute this software and its\n# documentation for any purpose and without fee is hereby granted,\n# provided that the above copyright notice appear in all copies and that\n# both that copyright notice and this permission notice appear in\n# supporting documentation, and that the name of Vinay Sajip\n# not be used in advertising or publicity pertaining to distribution\n# of the software without specific, written prior permission.\n# VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n# ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\n# VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR\n# ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER\n# IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT\n# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n\"\"\"\nLogging package for Python. Based on PEP 282 and comments thereto in\ncomp.lang.python.\n\nCopyright (C) 2001-2019 Vinay Sajip. All Rights Reserved.\n\nTo use, simply 'import logging' and log away!\n\"\"\"\n\nimport sys, os, time, io, re, traceback, warnings, weakref, collections.abc\n\nfrom string import Template\nfrom string import Formatter as StrFormatter\n\n\n__all__ = ['BASIC_FORMAT', 'BufferingFormatter', 'CRITICAL', 'DEBUG', 'ERROR',\n           'FATAL', 'FileHandler', 'Filter', 'Formatter', 'Handler', 'INFO',\n           'LogRecord', 'Logger', 'LoggerAdapter', 'NOTSET', 'NullHandler',\n           'StreamHandler', 'WARN', 'WARNING', 'addLevelName', 'basicConfig',\n           'captureWarnings', 'critical', 'debug', 'disable', 'error',\n           'exception', 'fatal', 'getLevelName', 'getLogger', 'getLoggerClass',\n           'info', 'log', 'makeLogRecord', 'setLoggerClass', 'shutdown',\n           'warn', 'warning', 'getLogRecordFactory', 'setLogRecordFactory',\n           'lastResort', 'raiseExceptions']\n\nimport threading\n\n__author__  = \"Vinay Sajip <vinay_sajip@red-dove.com>\"\n__status__  = \"production\"\n# The following module attributes are no longer updated.\n__version__ = \"0.5.1.2\"\n__date__    = \"07 February 2010\"\n\n#---------------------------------------------------------------------------\n#   Miscellaneous module data\n#---------------------------------------------------------------------------\n\n#\n#_startTime is used as the base when calculating the relative time of events\n#\n_startTime = time.time()\n\n#\n#raiseExceptions is used to see if exceptions during handling should be\n#propagated\n#\nraiseExceptions = True\n\n#\n# If you don't want threading information in the log, set this to zero\n#\nlogThreads = True\n\n#\n# If you don't want multiprocessing information in the log, set this to zero\n#\nlogMultiprocessing = True\n\n#\n# If you don't want process information in the log, set this to zero\n#\nlogProcesses = True\n\n#---------------------------------------------------------------------------\n#   Level related stuff\n#---------------------------------------------------------------------------\n#\n# Default levels and level names, these can be replaced with any positive set\n# of values having corresponding names. There is a pseudo-level, NOTSET, which\n# is only really there as a lower limit for user-defined levels. Handlers and\n# loggers are initialized with NOTSET so that they will log all messages, even\n# at user-defined levels.\n#\n\nCRITICAL = 50\nFATAL = CRITICAL\nERROR = 40\nWARNING = 30\nWARN = WARNING\nINFO = 20\nDEBUG = 10\nNOTSET = 0\n\n_levelToName = {\n    CRITICAL: 'CRITICAL',\n    ERROR: 'ERROR',\n    WARNING: 'WARNING',\n    INFO: 'INFO',\n    DEBUG: 'DEBUG',\n    NOTSET: 'NOTSET',\n}\n_nameToLevel = {\n    'CRITICAL': CRITICAL,\n    'FATAL': FATAL,\n    'ERROR': ERROR,\n    'WARN': WARNING,\n    'WARNING': WARNING,\n    'INFO': INFO,\n    'DEBUG': DEBUG,\n    'NOTSET': NOTSET,\n}\n\ndef getLevelName(level):\n    \"\"\"\n    Return the textual or numeric representation of logging level 'level'.\n\n    If the level is one of the predefined levels (CRITICAL, ERROR, WARNING,\n    INFO, DEBUG) then you get the corresponding string. If you have\n    associated levels with names using addLevelName then the name you have\n    associated with 'level' is returned.\n\n    If a numeric value corresponding to one of the defined levels is passed\n    in, the corresponding string representation is returned.\n\n    If a string representation of the level is passed in, the corresponding\n    numeric value is returned.\n\n    If no matching numeric or string value is passed in, the string\n    'Level %s' % level is returned.\n    \"\"\"\n    # See Issues #22386, #27937 and #29220 for why it's this way\n    result = _levelToName.get(level)\n    if result is not None:\n        return result\n    result = _nameToLevel.get(level)\n    if result is not None:\n        return result\n    return \"Level %s\" % level\n\ndef addLevelName(level, levelName):\n    \"\"\"\n    Associate 'levelName' with 'level'.\n\n    This is used when converting levels to text during message formatting.\n    \"\"\"\n    _acquireLock()\n    try:    #unlikely to cause an exception, but you never know...\n        _levelToName[level] = levelName\n        _nameToLevel[levelName] = level\n    finally:\n        _releaseLock()\n\nif hasattr(sys, '_getframe'):\n    currentframe = lambda: sys._getframe(3)\nelse: #pragma: no cover\n    def currentframe():\n        \"\"\"Return the frame object for the caller's stack frame.\"\"\"\n        try:\n            raise Exception\n        except Exception:\n            return sys.exc_info()[2].tb_frame.f_back\n\n#\n# _srcfile is used when walking the stack to check when we've got the first\n# caller stack frame, by skipping frames whose filename is that of this\n# module's source. It therefore should contain the filename of this module's\n# source file.\n#\n# Ordinarily we would use __file__ for this, but frozen modules don't always\n# have __file__ set, for some reason (see Issue #21736). Thus, we get the\n# filename from a handy code object from a function defined in this module.\n# (There's no particular reason for picking addLevelName.)\n#\n\n_srcfile = os.path.normcase(addLevelName.__code__.co_filename)\n\n# _srcfile is only used in conjunction with sys._getframe().\n# To provide compatibility with older versions of Python, set _srcfile\n# to None if _getframe() is not available; this value will prevent\n# findCaller() from being called. You can also do this if you want to avoid\n# the overhead of fetching caller information, even when _getframe() is\n# available.\n#if not hasattr(sys, '_getframe'):\n#    _srcfile = None\n\n\ndef _checkLevel(level):\n    if isinstance(level, int):\n        rv = level\n    elif str(level) == level:\n        if level not in _nameToLevel:\n            raise ValueError(\"Unknown level: %r\" % level)\n        rv = _nameToLevel[level]\n    else:\n        raise TypeError(\"Level not an integer or a valid string: %r\"\n                        % (level,))\n    return rv\n\n#---------------------------------------------------------------------------\n#   Thread-related stuff\n#---------------------------------------------------------------------------\n\n#\n#_lock is used to serialize access to shared data structures in this module.\n#This needs to be an RLock because fileConfig() creates and configures\n#Handlers, and so might arbitrary user threads. Since Handler code updates the\n#shared dictionary _handlers, it needs to acquire the lock. But if configuring,\n#the lock would already have been acquired - so we need an RLock.\n#The same argument applies to Loggers and Manager.loggerDict.\n#\n_lock = threading.RLock()\n\ndef _acquireLock():\n    \"\"\"\n    Acquire the module-level lock for serializing access to shared data.\n\n    This should be released with _releaseLock().\n    \"\"\"\n    if _lock:\n        _lock.acquire()\n\ndef _releaseLock():\n    \"\"\"\n    Release the module-level lock acquired by calling _acquireLock().\n    \"\"\"\n    if _lock:\n        _lock.release()\n\n\n# Prevent a held logging lock from blocking a child from logging.\n\nif not hasattr(os, 'register_at_fork'):  # Windows and friends.\n    def _register_at_fork_reinit_lock(instance):\n        pass  # no-op when os.register_at_fork does not exist.\nelse:\n    # A collection of instances with a _at_fork_reinit method (logging.Handler)\n    # to be called in the child after forking.  The weakref avoids us keeping\n    # discarded Handler instances alive.\n    _at_fork_reinit_lock_weakset = weakref.WeakSet()\n\n    def _register_at_fork_reinit_lock(instance):\n        _acquireLock()\n        try:\n            _at_fork_reinit_lock_weakset.add(instance)\n        finally:\n            _releaseLock()\n\n    def _after_at_fork_child_reinit_locks():\n        for handler in _at_fork_reinit_lock_weakset:\n            handler._at_fork_reinit()\n\n        # _acquireLock() was called in the parent before forking.\n        # The lock is reinitialized to unlocked state.\n        _lock._at_fork_reinit()\n\n    os.register_at_fork(before=_acquireLock,\n                        after_in_child=_after_at_fork_child_reinit_locks,\n                        after_in_parent=_releaseLock)\n\n\n#---------------------------------------------------------------------------\n#   The logging record\n#---------------------------------------------------------------------------\n\nclass LogRecord(object):\n    \"\"\"\n    A LogRecord instance represents an event being logged.\n\n    LogRecord instances are created every time something is logged. They\n    contain all the information pertinent to the event being logged. The\n    main information passed in is in msg and args, which are combined\n    using str(msg) % args to create the message field of the record. The\n    record also includes information such as when the record was created,\n    the source line where the logging call was made, and any exception\n    information to be logged.\n    \"\"\"\n    def __init__(self, name, level, pathname, lineno,\n                 msg, args, exc_info, func=None, sinfo=None, **kwargs):\n        \"\"\"\n        Initialize a logging record with interesting information.\n        \"\"\"\n        ct = time.time()\n        self.name = name\n        self.msg = msg\n        #\n        # The following statement allows passing of a dictionary as a sole\n        # argument, so that you can do something like\n        #  logging.debug(\"a %(a)d b %(b)s\", {'a':1, 'b':2})\n        # Suggested by Stefan Behnel.\n        # Note that without the test for args[0], we get a problem because\n        # during formatting, we test to see if the arg is present using\n        # 'if self.args:'. If the event being logged is e.g. 'Value is %d'\n        # and if the passed arg fails 'if self.args:' then no formatting\n        # is done. For example, logger.warning('Value is %d', 0) would log\n        # 'Value is %d' instead of 'Value is 0'.\n        # For the use case of passing a dictionary, this should not be a\n        # problem.\n        # Issue #21172: a request was made to relax the isinstance check\n        # to hasattr(args[0], '__getitem__'). However, the docs on string\n        # formatting still seem to suggest a mapping object is required.\n        # Thus, while not removing the isinstance check, it does now look\n        # for collections.abc.Mapping rather than, as before, dict.\n        if (args and len(args) == 1 and isinstance(args[0], collections.abc.Mapping)\n            and args[0]):\n            args = args[0]\n        self.args = args\n        self.levelname = getLevelName(level)\n        self.levelno = level\n        self.pathname = pathname\n        try:\n            self.filename = os.path.basename(pathname)\n            self.module = os.path.splitext(self.filename)[0]\n        except (TypeError, ValueError, AttributeError):\n            self.filename = pathname\n            self.module = \"Unknown module\"\n        self.exc_info = exc_info\n        self.exc_text = None      # used to cache the traceback text\n        self.stack_info = sinfo\n        self.lineno = lineno\n        self.funcName = func\n        self.created = ct\n        self.msecs = int((ct - int(ct)) * 1000) + 0.0  # see gh-89047\n        self.relativeCreated = (self.created - _startTime) * 1000\n        if logThreads:\n            self.thread = threading.get_ident()\n            self.threadName = threading.current_thread().name\n        else: # pragma: no cover\n            self.thread = None\n            self.threadName = None\n        if not logMultiprocessing: # pragma: no cover\n            self.processName = None\n        else:\n            self.processName = 'MainProcess'\n            mp = sys.modules.get('multiprocessing')\n            if mp is not None:\n                # Errors may occur if multiprocessing has not finished loading\n                # yet - e.g. if a custom import hook causes third-party code\n                # to run when multiprocessing calls import. See issue 8200\n                # for an example\n                try:\n                    self.processName = mp.current_process().name\n                except Exception: #pragma: no cover\n                    pass\n        if logProcesses and hasattr(os, 'getpid'):\n            self.process = os.getpid()\n        else:\n            self.process = None\n\n    def __repr__(self):\n        return '<LogRecord: %s, %s, %s, %s, \"%s\">'%(self.name, self.levelno,\n            self.pathname, self.lineno, self.msg)\n\n    def getMessage(self):\n        \"\"\"\n        Return the message for this LogRecord.\n\n        Return the message for this LogRecord after merging any user-supplied\n        arguments with the message.\n        \"\"\"\n        msg = str(self.msg)\n        if self.args:\n            msg = msg % self.args\n        return msg\n\n#\n#   Determine which class to use when instantiating log records.\n#\n_logRecordFactory = LogRecord\n\ndef setLogRecordFactory(factory):\n    \"\"\"\n    Set the factory to be used when instantiating a log record.\n\n    :param factory: A callable which will be called to instantiate\n    a log record.\n    \"\"\"\n    global _logRecordFactory\n    _logRecordFactory = factory\n\ndef getLogRecordFactory():\n    \"\"\"\n    Return the factory to be used when instantiating a log record.\n    \"\"\"\n\n    return _logRecordFactory\n\ndef makeLogRecord(dict):\n    \"\"\"\n    Make a LogRecord whose attributes are defined by the specified dictionary,\n    This function is useful for converting a logging event received over\n    a socket connection (which is sent as a dictionary) into a LogRecord\n    instance.\n    \"\"\"\n    rv = _logRecordFactory(None, None, \"\", 0, \"\", (), None, None)\n    rv.__dict__.update(dict)\n    return rv\n\n\n#---------------------------------------------------------------------------\n#   Formatter classes and functions\n#---------------------------------------------------------------------------\n_str_formatter = StrFormatter()\ndel StrFormatter\n\n\nclass PercentStyle(object):\n\n    default_format = '%(message)s'\n    asctime_format = '%(asctime)s'\n    asctime_search = '%(asctime)'\n    validation_pattern = re.compile(r'%\\(\\w+\\)[#0+ -]*(\\*|\\d+)?(\\.(\\*|\\d+))?[diouxefgcrsa%]', re.I)\n\n    def __init__(self, fmt, *, defaults=None):\n        self._fmt = fmt or self.default_format\n        self._defaults = defaults\n\n    def usesTime(self):\n        return self._fmt.find(self.asctime_search) >= 0\n\n    def validate(self):\n        \"\"\"Validate the input format, ensure it matches the correct style\"\"\"\n        if not self.validation_pattern.search(self._fmt):\n            raise ValueError(\"Invalid format '%s' for '%s' style\" % (self._fmt, self.default_format[0]))\n\n    def _format(self, record):\n        if defaults := self._defaults:\n            values = defaults | record.__dict__\n        else:\n            values = record.__dict__\n        return self._fmt % values\n\n    def format(self, record):\n        try:\n            return self._format(record)\n        except KeyError as e:\n            raise ValueError('Formatting field not found in record: %s' % e)\n\n\nclass StrFormatStyle(PercentStyle):\n    default_format = '{message}'\n    asctime_format = '{asctime}'\n    asctime_search = '{asctime'\n\n    fmt_spec = re.compile(r'^(.?[<>=^])?[+ -]?#?0?(\\d+|{\\w+})?[,_]?(\\.(\\d+|{\\w+}))?[bcdefgnosx%]?$', re.I)\n    field_spec = re.compile(r'^(\\d+|\\w+)(\\.\\w+|\\[[^]]+\\])*$')\n\n    def _format(self, record):\n        if defaults := self._defaults:\n            values = defaults | record.__dict__\n        else:\n            values = record.__dict__\n        return self._fmt.format(**values)\n\n    def validate(self):\n        \"\"\"Validate the input format, ensure it is the correct string formatting style\"\"\"\n        fields = set()\n        try:\n            for _, fieldname, spec, conversion in _str_formatter.parse(self._fmt):\n                if fieldname:\n                    if not self.field_spec.match(fieldname):\n                        raise ValueError('invalid field name/expression: %r' % fieldname)\n                    fields.add(fieldname)\n                if conversion and conversion not in 'rsa':\n                    raise ValueError('invalid conversion: %r' % conversion)\n                if spec and not self.fmt_spec.match(spec):\n                    raise ValueError('bad specifier: %r' % spec)\n        except ValueError as e:\n            raise ValueError('invalid format: %s' % e)\n        if not fields:\n            raise ValueError('invalid format: no fields')\n\n\nclass StringTemplateStyle(PercentStyle):\n    default_format = '${message}'\n    asctime_format = '${asctime}'\n    asctime_search = '${asctime}'\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._tpl = Template(self._fmt)\n\n    def usesTime(self):\n        fmt = self._fmt\n        return fmt.find('$asctime') >= 0 or fmt.find(self.asctime_search) >= 0\n\n    def validate(self):\n        pattern = Template.pattern\n        fields = set()\n        for m in pattern.finditer(self._fmt):\n            d = m.groupdict()\n            if d['named']:\n                fields.add(d['named'])\n            elif d['braced']:\n                fields.add(d['braced'])\n            elif m.group(0) == '$':\n                raise ValueError('invalid format: bare \\'$\\' not allowed')\n        if not fields:\n            raise ValueError('invalid format: no fields')\n\n    def _format(self, record):\n        if defaults := self._defaults:\n            values = defaults | record.__dict__\n        else:\n            values = record.__dict__\n        return self._tpl.substitute(**values)\n\n\nBASIC_FORMAT = \"%(levelname)s:%(name)s:%(message)s\"\n\n_STYLES = {\n    '%': (PercentStyle, BASIC_FORMAT),\n    '{': (StrFormatStyle, '{levelname}:{name}:{message}'),\n    '$': (StringTemplateStyle, '${levelname}:${name}:${message}'),\n}\n\nclass Formatter(object):\n    \"\"\"\n    Formatter instances are used to convert a LogRecord to text.\n\n    Formatters need to know how a LogRecord is constructed. They are\n    responsible for converting a LogRecord to (usually) a string which can\n    be interpreted by either a human or an external system. The base Formatter\n    allows a formatting string to be specified. If none is supplied, the\n    style-dependent default value, \"%(message)s\", \"{message}\", or\n    \"${message}\", is used.\n\n    The Formatter can be initialized with a format string which makes use of\n    knowledge of the LogRecord attributes - e.g. the default value mentioned\n    above makes use of the fact that the user's message and arguments are pre-\n    formatted into a LogRecord's message attribute. Currently, the useful\n    attributes in a LogRecord are described by:\n\n    %(name)s            Name of the logger (logging channel)\n    %(levelno)s         Numeric logging level for the message (DEBUG, INFO,\n                        WARNING, ERROR, CRITICAL)\n    %(levelname)s       Text logging level for the message (\"DEBUG\", \"INFO\",\n                        \"WARNING\", \"ERROR\", \"CRITICAL\")\n    %(pathname)s        Full pathname of the source file where the logging\n                        call was issued (if available)\n    %(filename)s        Filename portion of pathname\n    %(module)s          Module (name portion of filename)\n    %(lineno)d          Source line number where the logging call was issued\n                        (if available)\n    %(funcName)s        Function name\n    %(created)f         Time when the LogRecord was created (time.time()\n                        return value)\n    %(asctime)s         Textual time when the LogRecord was created\n    %(msecs)d           Millisecond portion of the creation time\n    %(relativeCreated)d Time in milliseconds when the LogRecord was created,\n                        relative to the time the logging module was loaded\n                        (typically at application startup time)\n    %(thread)d          Thread ID (if available)\n    %(threadName)s      Thread name (if available)\n    %(process)d         Process ID (if available)\n    %(message)s         The result of record.getMessage(), computed just as\n                        the record is emitted\n    \"\"\"\n\n    converter = time.localtime\n\n    def __init__(self, fmt=None, datefmt=None, style='%', validate=True, *,\n                 defaults=None):\n        \"\"\"\n        Initialize the formatter with specified format strings.\n\n        Initialize the formatter either with the specified format string, or a\n        default as described above. Allow for specialized date formatting with\n        the optional datefmt argument. If datefmt is omitted, you get an\n        ISO8601-like (or RFC 3339-like) format.\n\n        Use a style parameter of '%', '{' or '$' to specify that you want to\n        use one of %-formatting, :meth:`str.format` (``{}``) formatting or\n        :class:`string.Template` formatting in your format string.\n\n        .. versionchanged:: 3.2\n           Added the ``style`` parameter.\n        \"\"\"\n        if style not in _STYLES:\n            raise ValueError('Style must be one of: %s' % ','.join(\n                             _STYLES.keys()))\n        self._style = _STYLES[style][0](fmt, defaults=defaults)\n        if validate:\n            self._style.validate()\n\n        self._fmt = self._style._fmt\n        self.datefmt = datefmt\n\n    default_time_format = '%Y-%m-%d %H:%M:%S'\n    default_msec_format = '%s,%03d'\n\n    def formatTime(self, record, datefmt=None):\n        \"\"\"\n        Return the creation time of the specified LogRecord as formatted text.\n\n        This method should be called from format() by a formatter which\n        wants to make use of a formatted time. This method can be overridden\n        in formatters to provide for any specific requirement, but the\n        basic behaviour is as follows: if datefmt (a string) is specified,\n        it is used with time.strftime() to format the creation time of the\n        record. Otherwise, an ISO8601-like (or RFC 3339-like) format is used.\n        The resulting string is returned. This function uses a user-configurable\n        function to convert the creation time to a tuple. By default,\n        time.localtime() is used; to change this for a particular formatter\n        instance, set the 'converter' attribute to a function with the same\n        signature as time.localtime() or time.gmtime(). To change it for all\n        formatters, for example if you want all logging times to be shown in GMT,\n        set the 'converter' attribute in the Formatter class.\n        \"\"\"\n        ct = self.converter(record.created)\n        if datefmt:\n            s = time.strftime(datefmt, ct)\n        else:\n            s = time.strftime(self.default_time_format, ct)\n            if self.default_msec_format:\n                s = self.default_msec_format % (s, record.msecs)\n        return s\n\n    def formatException(self, ei):\n        \"\"\"\n        Format and return the specified exception information as a string.\n\n        This default implementation just uses\n        traceback.print_exception()\n        \"\"\"\n        sio = io.StringIO()\n        tb = ei[2]\n        # See issues #9427, #1553375. Commented out for now.\n        #if getattr(self, 'fullstack', False):\n        #    traceback.print_stack(tb.tb_frame.f_back, file=sio)\n        traceback.print_exception(ei[0], ei[1], tb, None, sio)\n        s = sio.getvalue()\n        sio.close()\n        if s[-1:] == \"\\n\":\n            s = s[:-1]\n        return s\n\n    def usesTime(self):\n        \"\"\"\n        Check if the format uses the creation time of the record.\n        \"\"\"\n        return self._style.usesTime()\n\n    def formatMessage(self, record):\n        return self._style.format(record)\n\n    def formatStack(self, stack_info):\n        \"\"\"\n        This method is provided as an extension point for specialized\n        formatting of stack information.\n\n        The input data is a string as returned from a call to\n        :func:`traceback.print_stack`, but with the last trailing newline\n        removed.\n\n        The base implementation just returns the value passed in.\n        \"\"\"\n        return stack_info\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record as text.\n\n        The record's attribute dictionary is used as the operand to a\n        string formatting operation which yields the returned string.\n        Before formatting the dictionary, a couple of preparatory steps\n        are carried out. The message attribute of the record is computed\n        using LogRecord.getMessage(). If the formatting string uses the\n        time (as determined by a call to usesTime(), formatTime() is\n        called to format the event time. If there is exception information,\n        it is formatted using formatException() and appended to the message.\n        \"\"\"\n        record.message = record.getMessage()\n        if self.usesTime():\n            record.asctime = self.formatTime(record, self.datefmt)\n        s = self.formatMessage(record)\n        if record.exc_info:\n            # Cache the traceback text to avoid converting it multiple times\n            # (it's constant anyway)\n            if not record.exc_text:\n                record.exc_text = self.formatException(record.exc_info)\n        if record.exc_text:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            s = s + record.exc_text\n        if record.stack_info:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            s = s + self.formatStack(record.stack_info)\n        return s\n\n#\n#   The default formatter to use when no other is specified\n#\n_defaultFormatter = Formatter()\n\nclass BufferingFormatter(object):\n    \"\"\"\n    A formatter suitable for formatting a number of records.\n    \"\"\"\n    def __init__(self, linefmt=None):\n        \"\"\"\n        Optionally specify a formatter which will be used to format each\n        individual record.\n        \"\"\"\n        if linefmt:\n            self.linefmt = linefmt\n        else:\n            self.linefmt = _defaultFormatter\n\n    def formatHeader(self, records):\n        \"\"\"\n        Return the header string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def formatFooter(self, records):\n        \"\"\"\n        Return the footer string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def format(self, records):\n        \"\"\"\n        Format the specified records and return the result as a string.\n        \"\"\"\n        rv = \"\"\n        if len(records) > 0:\n            rv = rv + self.formatHeader(records)\n            for record in records:\n                rv = rv + self.linefmt.format(record)\n            rv = rv + self.formatFooter(records)\n        return rv\n\n#---------------------------------------------------------------------------\n#   Filter classes and functions\n#---------------------------------------------------------------------------\n\nclass Filter(object):\n    \"\"\"\n    Filter instances are used to perform arbitrary filtering of LogRecords.\n\n    Loggers and Handlers can optionally use Filter instances to filter\n    records as desired. The base filter class only allows events which are\n    below a certain point in the logger hierarchy. For example, a filter\n    initialized with \"A.B\" will allow events logged by loggers \"A.B\",\n    \"A.B.C\", \"A.B.C.D\", \"A.B.D\" etc. but not \"A.BB\", \"B.A.B\" etc. If\n    initialized with the empty string, all events are passed.\n    \"\"\"\n    def __init__(self, name=''):\n        \"\"\"\n        Initialize a filter.\n\n        Initialize with the name of the logger which, together with its\n        children, will have its events allowed through the filter. If no\n        name is specified, allow every event.\n        \"\"\"\n        self.name = name\n        self.nlen = len(name)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if the specified record is to be logged.\n\n        Returns True if the record should be logged, or False otherwise.\n        If deemed appropriate, the record may be modified in-place.\n        \"\"\"\n        if self.nlen == 0:\n            return True\n        elif self.name == record.name:\n            return True\n        elif record.name.find(self.name, 0, self.nlen) != 0:\n            return False\n        return (record.name[self.nlen] == \".\")\n\nclass Filterer(object):\n    \"\"\"\n    A base class for loggers and handlers which allows them to share\n    common code.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the list of filters to be an empty list.\n        \"\"\"\n        self.filters = []\n\n    def addFilter(self, filter):\n        \"\"\"\n        Add the specified filter to this handler.\n        \"\"\"\n        if not (filter in self.filters):\n            self.filters.append(filter)\n\n    def removeFilter(self, filter):\n        \"\"\"\n        Remove the specified filter from this handler.\n        \"\"\"\n        if filter in self.filters:\n            self.filters.remove(filter)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if a record is loggable by consulting all the filters.\n\n        The default is to allow the record to be logged; any filter can veto\n        this and the record is then dropped. Returns a zero value if a record\n        is to be dropped, else non-zero.\n\n        .. versionchanged:: 3.2\n\n           Allow filters to be just callables.\n        \"\"\"\n        rv = True\n        for f in self.filters:\n            if hasattr(f, 'filter'):\n                result = f.filter(record)\n            else:\n                result = f(record) # assume callable - will raise if not\n            if not result:\n                rv = False\n                break\n        return rv\n\n#---------------------------------------------------------------------------\n#   Handler classes and functions\n#---------------------------------------------------------------------------\n\n_handlers = weakref.WeakValueDictionary()  #map of handler names to handlers\n_handlerList = [] # added to allow handlers to be removed in reverse of order initialized\n\ndef _removeHandlerRef(wr):\n    \"\"\"\n    Remove a handler reference from the internal cleanup list.\n    \"\"\"\n    # This function can be called during module teardown, when globals are\n    # set to None. It can also be called from another thread. So we need to\n    # pre-emptively grab the necessary globals and check if they're None,\n    # to prevent race conditions and failures during interpreter shutdown.\n    acquire, release, handlers = _acquireLock, _releaseLock, _handlerList\n    if acquire and release and handlers:\n        acquire()\n        try:\n            if wr in handlers:\n                handlers.remove(wr)\n        finally:\n            release()\n\ndef _addHandlerRef(handler):\n    \"\"\"\n    Add a handler to the internal cleanup list using a weak reference.\n    \"\"\"\n    _acquireLock()\n    try:\n        _handlerList.append(weakref.ref(handler, _removeHandlerRef))\n    finally:\n        _releaseLock()\n\nclass Handler(Filterer):\n    \"\"\"\n    Handler instances dispatch logging events to specific destinations.\n\n    The base handler class. Acts as a placeholder which defines the Handler\n    interface. Handlers can optionally use Formatter instances to format\n    records as desired. By default, no formatter is specified; in this case,\n    the 'raw' message as determined by record.message is logged.\n    \"\"\"\n    def __init__(self, level=NOTSET):\n        \"\"\"\n        Initializes the instance - basically setting the formatter to None\n        and the filter list to empty.\n        \"\"\"\n        Filterer.__init__(self)\n        self._name = None\n        self.level = _checkLevel(level)\n        self.formatter = None\n        self._closed = False\n        # Add the handler to the global _handlerList (for cleanup on shutdown)\n        _addHandlerRef(self)\n        self.createLock()\n\n    def get_name(self):\n        return self._name\n\n    def set_name(self, name):\n        _acquireLock()\n        try:\n            if self._name in _handlers:\n                del _handlers[self._name]\n            self._name = name\n            if name:\n                _handlers[name] = self\n        finally:\n            _releaseLock()\n\n    name = property(get_name, set_name)\n\n    def createLock(self):\n        \"\"\"\n        Acquire a thread lock for serializing access to the underlying I/O.\n        \"\"\"\n        self.lock = threading.RLock()\n        _register_at_fork_reinit_lock(self)\n\n    def _at_fork_reinit(self):\n        self.lock._at_fork_reinit()\n\n    def acquire(self):\n        \"\"\"\n        Acquire the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.acquire()\n\n    def release(self):\n        \"\"\"\n        Release the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.release()\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this handler.  level must be an int or a str.\n        \"\"\"\n        self.level = _checkLevel(level)\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record.\n\n        If a formatter is set, use it. Otherwise, use the default formatter\n        for the module.\n        \"\"\"\n        if self.formatter:\n            fmt = self.formatter\n        else:\n            fmt = _defaultFormatter\n        return fmt.format(record)\n\n    def emit(self, record):\n        \"\"\"\n        Do whatever it takes to actually log the specified logging record.\n\n        This version is intended to be implemented by subclasses and so\n        raises a NotImplementedError.\n        \"\"\"\n        raise NotImplementedError('emit must be implemented '\n                                  'by Handler subclasses')\n\n    def handle(self, record):\n        \"\"\"\n        Conditionally emit the specified logging record.\n\n        Emission depends on filters which may have been added to the handler.\n        Wrap the actual emission of the record with acquisition/release of\n        the I/O thread lock. Returns whether the filter passed the record for\n        emission.\n        \"\"\"\n        rv = self.filter(record)\n        if rv:\n            self.acquire()\n            try:\n                self.emit(record)\n            finally:\n                self.release()\n        return rv\n\n    def setFormatter(self, fmt):\n        \"\"\"\n        Set the formatter for this handler.\n        \"\"\"\n        self.formatter = fmt\n\n    def flush(self):\n        \"\"\"\n        Ensure all logging output has been flushed.\n\n        This version does nothing and is intended to be implemented by\n        subclasses.\n        \"\"\"\n        pass\n\n    def close(self):\n        \"\"\"\n        Tidy up any resources used by the handler.\n\n        This version removes the handler from an internal map of handlers,\n        _handlers, which is used for handler lookup by name. Subclasses\n        should ensure that this gets called from overridden close()\n        methods.\n        \"\"\"\n        #get the module data lock, as we're updating a shared structure.\n        _acquireLock()\n        try:    #unlikely to raise an exception, but you never know...\n            self._closed = True\n            if self._name and self._name in _handlers:\n                del _handlers[self._name]\n        finally:\n            _releaseLock()\n\n    def handleError(self, record):\n        \"\"\"\n        Handle errors which occur during an emit() call.\n\n        This method should be called from handlers when an exception is\n        encountered during an emit() call. If raiseExceptions is false,\n        exceptions get silently ignored. This is what is mostly wanted\n        for a logging system - most users will not care about errors in\n        the logging system, they are more interested in application errors.\n        You could, however, replace this with a custom handler if you wish.\n        The record which was being processed is passed in to this method.\n        \"\"\"\n        if raiseExceptions and sys.stderr:  # see issue 13807\n            t, v, tb = sys.exc_info()\n            try:\n                sys.stderr.write('--- Logging error ---\\n')\n                traceback.print_exception(t, v, tb, None, sys.stderr)\n                sys.stderr.write('Call stack:\\n')\n                # Walk the stack frame up until we're out of logging,\n                # so as to print the calling context.\n                frame = tb.tb_frame\n                while (frame and os.path.dirname(frame.f_code.co_filename) ==\n                       __path__[0]):\n                    frame = frame.f_back\n                if frame:\n                    traceback.print_stack(frame, file=sys.stderr)\n                else:\n                    # couldn't find the right stack frame, for some reason\n                    sys.stderr.write('Logged from file %s, line %s\\n' % (\n                                     record.filename, record.lineno))\n                # Issue 18671: output logging message and arguments\n                try:\n                    sys.stderr.write('Message: %r\\n'\n                                     'Arguments: %s\\n' % (record.msg,\n                                                          record.args))\n                except RecursionError:  # See issue 36272\n                    raise\n                except Exception:\n                    sys.stderr.write('Unable to print the message and arguments'\n                                     ' - possible formatting error.\\nUse the'\n                                     ' traceback above to help find the error.\\n'\n                                    )\n            except OSError: #pragma: no cover\n                pass    # see issue 5971\n            finally:\n                del t, v, tb\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        return '<%s (%s)>' % (self.__class__.__name__, level)\n\nclass StreamHandler(Handler):\n    \"\"\"\n    A handler class which writes logging records, appropriately formatted,\n    to a stream. Note that this class does not close the stream, as\n    sys.stdout or sys.stderr may be used.\n    \"\"\"\n\n    terminator = '\\n'\n\n    def __init__(self, stream=None):\n        \"\"\"\n        Initialize the handler.\n\n        If stream is not specified, sys.stderr is used.\n        \"\"\"\n        Handler.__init__(self)\n        if stream is None:\n            stream = sys.stderr\n        self.stream = stream\n\n    def flush(self):\n        \"\"\"\n        Flushes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            if self.stream and hasattr(self.stream, \"flush\"):\n                self.stream.flush()\n        finally:\n            self.release()\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If a formatter is specified, it is used to format the record.\n        The record is then written to the stream with a trailing newline.  If\n        exception information is present, it is formatted using\n        traceback.print_exception and appended to the stream.  If the stream\n        has an 'encoding' attribute, it is used to determine how to do the\n        output to the stream.\n        \"\"\"\n        try:\n            msg = self.format(record)\n            stream = self.stream\n            # issue 35046: merged two stream.writes into one.\n            stream.write(msg + self.terminator)\n            self.flush()\n        except RecursionError:  # See issue 36272\n            raise\n        except Exception:\n            self.handleError(record)\n\n    def setStream(self, stream):\n        \"\"\"\n        Sets the StreamHandler's stream to the specified value,\n        if it is different.\n\n        Returns the old stream, if the stream was changed, or None\n        if it wasn't.\n        \"\"\"\n        if stream is self.stream:\n            result = None\n        else:\n            result = self.stream\n            self.acquire()\n            try:\n                self.flush()\n                self.stream = stream\n            finally:\n                self.release()\n        return result\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        name = getattr(self.stream, 'name', '')\n        #  bpo-36015: name can be an int\n        name = str(name)\n        if name:\n            name += ' '\n        return '<%s %s(%s)>' % (self.__class__.__name__, name, level)\n\n\nclass FileHandler(StreamHandler):\n    \"\"\"\n    A handler class which writes formatted logging records to disk files.\n    \"\"\"\n    def __init__(self, filename, mode='a', encoding=None, delay=False, errors=None):\n        \"\"\"\n        Open the specified file and use it as the stream for logging.\n        \"\"\"\n        # Issue #27493: add support for Path objects to be passed in\n        filename = os.fspath(filename)\n        #keep the absolute path, otherwise derived classes which use this\n        #may come a cropper when the current directory changes\n        self.baseFilename = os.path.abspath(filename)\n        self.mode = mode\n        self.encoding = encoding\n        if \"b\" not in mode:\n            self.encoding = io.text_encoding(encoding)\n        self.errors = errors\n        self.delay = delay\n        # bpo-26789: FileHandler keeps a reference to the builtin open()\n        # function to be able to open or reopen the file during Python\n        # finalization.\n        self._builtin_open = open\n        if delay:\n            #We don't open the stream, but we still need to call the\n            #Handler constructor to set level, formatter, lock etc.\n            Handler.__init__(self)\n            self.stream = None\n        else:\n            StreamHandler.__init__(self, self._open())\n\n    def close(self):\n        \"\"\"\n        Closes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            try:\n                if self.stream:\n                    try:\n                        self.flush()\n                    finally:\n                        stream = self.stream\n                        self.stream = None\n                        if hasattr(stream, \"close\"):\n                            stream.close()\n            finally:\n                # Issue #19523: call unconditionally to\n                # prevent a handler leak when delay is set\n                # Also see Issue #42378: we also rely on\n                # self._closed being set to True there\n                StreamHandler.close(self)\n        finally:\n            self.release()\n\n    def _open(self):\n        \"\"\"\n        Open the current base file with the (original) mode and encoding.\n        Return the resulting stream.\n        \"\"\"\n        open_func = self._builtin_open\n        return open_func(self.baseFilename, self.mode,\n                         encoding=self.encoding, errors=self.errors)\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If the stream was not opened because 'delay' was specified in the\n        constructor, open it before calling the superclass's emit.\n\n        If stream is not open, current mode is 'w' and `_closed=True`, record\n        will not be emitted (see Issue #42378).\n        \"\"\"\n        if self.stream is None:\n            if self.mode != 'w' or not self._closed:\n                self.stream = self._open()\n        if self.stream:\n            StreamHandler.emit(self, record)\n\n    def __repr__(self):\n        level = getLevelName(self.level)\n        return '<%s %s (%s)>' % (self.__class__.__name__, self.baseFilename, level)\n\n\nclass _StderrHandler(StreamHandler):\n    \"\"\"\n    This class is like a StreamHandler using sys.stderr, but always uses\n    whatever sys.stderr is currently set to rather than the value of\n    sys.stderr at handler construction time.\n    \"\"\"\n    def __init__(self, level=NOTSET):\n        \"\"\"\n        Initialize the handler.\n        \"\"\"\n        Handler.__init__(self, level)\n\n    @property\n    def stream(self):\n        return sys.stderr\n\n\n_defaultLastResort = _StderrHandler(WARNING)\nlastResort = _defaultLastResort\n\n#---------------------------------------------------------------------------\n#   Manager classes and functions\n#---------------------------------------------------------------------------\n\nclass PlaceHolder(object):\n    \"\"\"\n    PlaceHolder instances are used in the Manager logger hierarchy to take\n    the place of nodes for which no loggers have been defined. This class is\n    intended for internal use only and not as part of the public API.\n    \"\"\"\n    def __init__(self, alogger):\n        \"\"\"\n        Initialize with the specified logger being a child of this placeholder.\n        \"\"\"\n        self.loggerMap = { alogger : None }\n\n    def append(self, alogger):\n        \"\"\"\n        Add the specified logger as a child of this placeholder.\n        \"\"\"\n        if alogger not in self.loggerMap:\n            self.loggerMap[alogger] = None\n\n#\n#   Determine which class to use when instantiating loggers.\n#\n\ndef setLoggerClass(klass):\n    \"\"\"\n    Set the class to be used when instantiating a logger. The class should\n    define __init__() such that only a name argument is required, and the\n    __init__() should call Logger.__init__()\n    \"\"\"\n    if klass != Logger:\n        if not issubclass(klass, Logger):\n            raise TypeError(\"logger not derived from logging.Logger: \"\n                            + klass.__name__)\n    global _loggerClass\n    _loggerClass = klass\n\ndef getLoggerClass():\n    \"\"\"\n    Return the class to be used when instantiating a logger.\n    \"\"\"\n    return _loggerClass\n\nclass Manager(object):\n    \"\"\"\n    There is [under normal circumstances] just one Manager instance, which\n    holds the hierarchy of loggers.\n    \"\"\"\n    def __init__(self, rootnode):\n        \"\"\"\n        Initialize the manager with the root node of the logger hierarchy.\n        \"\"\"\n        self.root = rootnode\n        self.disable = 0\n        self.emittedNoHandlerWarning = False\n        self.loggerDict = {}\n        self.loggerClass = None\n        self.logRecordFactory = None\n\n    @property\n    def disable(self):\n        return self._disable\n\n    @disable.setter\n    def disable(self, value):\n        self._disable = _checkLevel(value)\n\n    def getLogger(self, name):\n        \"\"\"\n        Get a logger with the specified name (channel name), creating it\n        if it doesn't yet exist. This name is a dot-separated hierarchical\n        name, such as \"a\", \"a.b\", \"a.b.c\" or similar.\n\n        If a PlaceHolder existed for the specified name [i.e. the logger\n        didn't exist but a child of it did], replace it with the created\n        logger and fix up the parent/child references which pointed to the\n        placeholder to now point to the logger.\n        \"\"\"\n        rv = None\n        if not isinstance(name, str):\n            raise TypeError('A logger name must be a string')\n        _acquireLock()\n        try:\n            if name in self.loggerDict:\n                rv = self.loggerDict[name]\n                if isinstance(rv, PlaceHolder):\n                    ph = rv\n                    rv = (self.loggerClass or _loggerClass)(name)\n                    rv.manager = self\n                    self.loggerDict[name] = rv\n                    self._fixupChildren(ph, rv)\n                    self._fixupParents(rv)\n            else:\n                rv = (self.loggerClass or _loggerClass)(name)\n                rv.manager = self\n                self.loggerDict[name] = rv\n                self._fixupParents(rv)\n        finally:\n            _releaseLock()\n        return rv\n\n    def setLoggerClass(self, klass):\n        \"\"\"\n        Set the class to be used when instantiating a logger with this Manager.\n        \"\"\"\n        if klass != Logger:\n            if not issubclass(klass, Logger):\n                raise TypeError(\"logger not derived from logging.Logger: \"\n                                + klass.__name__)\n        self.loggerClass = klass\n\n    def setLogRecordFactory(self, factory):\n        \"\"\"\n        Set the factory to be used when instantiating a log record with this\n        Manager.\n        \"\"\"\n        self.logRecordFactory = factory\n\n    def _fixupParents(self, alogger):\n        \"\"\"\n        Ensure that there are either loggers or placeholders all the way\n        from the specified logger to the root of the logger hierarchy.\n        \"\"\"\n        name = alogger.name\n        i = name.rfind(\".\")\n        rv = None\n        while (i > 0) and not rv:\n            substr = name[:i]\n            if substr not in self.loggerDict:\n                self.loggerDict[substr] = PlaceHolder(alogger)\n            else:\n                obj = self.loggerDict[substr]\n                if isinstance(obj, Logger):\n                    rv = obj\n                else:\n                    assert isinstance(obj, PlaceHolder)\n                    obj.append(alogger)\n            i = name.rfind(\".\", 0, i - 1)\n        if not rv:\n            rv = self.root\n        alogger.parent = rv\n\n    def _fixupChildren(self, ph, alogger):\n        \"\"\"\n        Ensure that children of the placeholder ph are connected to the\n        specified logger.\n        \"\"\"\n        name = alogger.name\n        namelen = len(name)\n        for c in ph.loggerMap.keys():\n            #The if means ... if not c.parent.name.startswith(nm)\n            if c.parent.name[:namelen] != name:\n                alogger.parent = c.parent\n                c.parent = alogger\n\n    def _clear_cache(self):\n        \"\"\"\n        Clear the cache for all loggers in loggerDict\n        Called when level changes are made\n        \"\"\"\n\n        _acquireLock()\n        for logger in self.loggerDict.values():\n            if isinstance(logger, Logger):\n                logger._cache.clear()\n        self.root._cache.clear()\n        _releaseLock()\n\n#---------------------------------------------------------------------------\n#   Logger classes and functions\n#---------------------------------------------------------------------------\n\nclass Logger(Filterer):\n    \"\"\"\n    Instances of the Logger class represent a single logging channel. A\n    \"logging channel\" indicates an area of an application. Exactly how an\n    \"area\" is defined is up to the application developer. Since an\n    application can have any number of areas, logging channels are identified\n    by a unique string. Application areas can be nested (e.g. an area\n    of \"input processing\" might include sub-areas \"read CSV files\", \"read\n    XLS files\" and \"read Gnumeric files\"). To cater for this natural nesting,\n    channel names are organized into a namespace hierarchy where levels are\n    separated by periods, much like the Java or Python package namespace. So\n    in the instance given above, channel names might be \"input\" for the upper\n    level, and \"input.csv\", \"input.xls\" and \"input.gnu\" for the sub-levels.\n    There is no arbitrary limit to the depth of nesting.\n    \"\"\"\n    def __init__(self, name, level=NOTSET):\n        \"\"\"\n        Initialize the logger with a name and an optional level.\n        \"\"\"\n        Filterer.__init__(self)\n        self.name = name\n        self.level = _checkLevel(level)\n        self.parent = None\n        self.propagate = True\n        self.handlers = []\n        self.disabled = False\n        self._cache = {}\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this logger.  level must be an int or a str.\n        \"\"\"\n        self.level = _checkLevel(level)\n        self.manager._clear_cache()\n\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'DEBUG'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.debug(\"Houston, we have a %s\", \"thorny problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(DEBUG):\n            self._log(DEBUG, msg, args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'INFO'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.info(\"Houston, we have a %s\", \"interesting problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(INFO):\n            self._log(INFO, msg, args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'WARNING'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.warning(\"Houston, we have a %s\", \"bit of a problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(WARNING):\n            self._log(WARNING, msg, args, **kwargs)\n\n    def warn(self, msg, *args, **kwargs):\n        warnings.warn(\"The 'warn' method is deprecated, \"\n            \"use 'warning' instead\", DeprecationWarning, 2)\n        self.warning(msg, *args, **kwargs)\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'ERROR'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.error(\"Houston, we have a %s\", \"major problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(ERROR):\n            self._log(ERROR, msg, args, **kwargs)\n\n    def exception(self, msg, *args, exc_info=True, **kwargs):\n        \"\"\"\n        Convenience method for logging an ERROR with exception information.\n        \"\"\"\n        self.error(msg, *args, exc_info=exc_info, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'CRITICAL'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.critical(\"Houston, we have a %s\", \"major disaster\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(CRITICAL):\n            self._log(CRITICAL, msg, args, **kwargs)\n\n    def fatal(self, msg, *args, **kwargs):\n        \"\"\"\n        Don't use this method, use critical() instead.\n        \"\"\"\n        self.critical(msg, *args, **kwargs)\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with the integer severity 'level'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.log(level, \"We have a %s\", \"mysterious problem\", exc_info=1)\n        \"\"\"\n        if not isinstance(level, int):\n            if raiseExceptions:\n                raise TypeError(\"level must be an integer\")\n            else:\n                return\n        if self.isEnabledFor(level):\n            self._log(level, msg, args, **kwargs)\n\n    def findCaller(self, stack_info=False, stacklevel=1):\n        \"\"\"\n        Find the stack frame of the caller so that we can note the source\n        file name, line number and function name.\n        \"\"\"\n        f = currentframe()\n        #On some versions of IronPython, currentframe() returns None if\n        #IronPython isn't run with -X:Frames.\n        if f is not None:\n            f = f.f_back\n        orig_f = f\n        while f and stacklevel > 1:\n            f = f.f_back\n            stacklevel -= 1\n        if not f:\n            f = orig_f\n        rv = \"(unknown file)\", 0, \"(unknown function)\", None\n        while hasattr(f, \"f_code\"):\n            co = f.f_code\n            filename = os.path.normcase(co.co_filename)\n            if filename == _srcfile:\n                f = f.f_back\n                continue\n            sinfo = None\n            if stack_info:\n                sio = io.StringIO()\n                sio.write('Stack (most recent call last):\\n')\n                traceback.print_stack(f, file=sio)\n                sinfo = sio.getvalue()\n                if sinfo[-1] == '\\n':\n                    sinfo = sinfo[:-1]\n                sio.close()\n            rv = (co.co_filename, f.f_lineno, co.co_name, sinfo)\n            break\n        return rv\n\n    def makeRecord(self, name, level, fn, lno, msg, args, exc_info,\n                   func=None, extra=None, sinfo=None):\n        \"\"\"\n        A factory method which can be overridden in subclasses to create\n        specialized LogRecords.\n        \"\"\"\n        rv = _logRecordFactory(name, level, fn, lno, msg, args, exc_info, func,\n                             sinfo)\n        if extra is not None:\n            for key in extra:\n                if (key in [\"message\", \"asctime\"]) or (key in rv.__dict__):\n                    raise KeyError(\"Attempt to overwrite %r in LogRecord\" % key)\n                rv.__dict__[key] = extra[key]\n        return rv\n\n    def _log(self, level, msg, args, exc_info=None, extra=None, stack_info=False,\n             stacklevel=1):\n        \"\"\"\n        Low-level logging routine which creates a LogRecord and then calls\n        all the handlers of this logger to handle the record.\n        \"\"\"\n        sinfo = None\n        if _srcfile:\n            #IronPython doesn't track Python frames, so findCaller raises an\n            #exception on some versions of IronPython. We trap it here so that\n            #IronPython can use logging.\n            try:\n                fn, lno, func, sinfo = self.findCaller(stack_info, stacklevel)\n            except ValueError: # pragma: no cover\n                fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        else: # pragma: no cover\n            fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        if exc_info:\n            if isinstance(exc_info, BaseException):\n                exc_info = (type(exc_info), exc_info, exc_info.__traceback__)\n            elif not isinstance(exc_info, tuple):\n                exc_info = sys.exc_info()\n        record = self.makeRecord(self.name, level, fn, lno, msg, args,\n                                 exc_info, func, extra, sinfo)\n        self.handle(record)\n\n    def handle(self, record):\n        \"\"\"\n        Call the handlers for the specified record.\n\n        This method is used for unpickled records received from a socket, as\n        well as those created locally. Logger-level filtering is applied.\n        \"\"\"\n        if (not self.disabled) and self.filter(record):\n            self.callHandlers(record)\n\n    def addHandler(self, hdlr):\n        \"\"\"\n        Add the specified handler to this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if not (hdlr in self.handlers):\n                self.handlers.append(hdlr)\n        finally:\n            _releaseLock()\n\n    def removeHandler(self, hdlr):\n        \"\"\"\n        Remove the specified handler from this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if hdlr in self.handlers:\n                self.handlers.remove(hdlr)\n        finally:\n            _releaseLock()\n\n    def hasHandlers(self):\n        \"\"\"\n        See if this logger has any handlers configured.\n\n        Loop through all handlers for this logger and its parents in the\n        logger hierarchy. Return True if a handler was found, else False.\n        Stop searching up the hierarchy whenever a logger with the \"propagate\"\n        attribute set to zero is found - that will be the last logger which\n        is checked for the existence of handlers.\n        \"\"\"\n        c = self\n        rv = False\n        while c:\n            if c.handlers:\n                rv = True\n                break\n            if not c.propagate:\n                break\n            else:\n                c = c.parent\n        return rv\n\n    def callHandlers(self, record):\n        \"\"\"\n        Pass a record to all relevant handlers.\n\n        Loop through all handlers for this logger and its parents in the\n        logger hierarchy. If no handler was found, output a one-off error\n        message to sys.stderr. Stop searching up the hierarchy whenever a\n        logger with the \"propagate\" attribute set to zero is found - that\n        will be the last logger whose handlers are called.\n        \"\"\"\n        c = self\n        found = 0\n        while c:\n            for hdlr in c.handlers:\n                found = found + 1\n                if record.levelno >= hdlr.level:\n                    hdlr.handle(record)\n            if not c.propagate:\n                c = None    #break out\n            else:\n                c = c.parent\n        if (found == 0):\n            if lastResort:\n                if record.levelno >= lastResort.level:\n                    lastResort.handle(record)\n            elif raiseExceptions and not self.manager.emittedNoHandlerWarning:\n                sys.stderr.write(\"No handlers could be found for logger\"\n                                 \" \\\"%s\\\"\\n\" % self.name)\n                self.manager.emittedNoHandlerWarning = True\n\n    def getEffectiveLevel(self):\n        \"\"\"\n        Get the effective level for this logger.\n\n        Loop through this logger and its parents in the logger hierarchy,\n        looking for a non-zero logging level. Return the first one found.\n        \"\"\"\n        logger = self\n        while logger:\n            if logger.level:\n                return logger.level\n            logger = logger.parent\n        return NOTSET\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        Is this logger enabled for level 'level'?\n        \"\"\"\n        if self.disabled:\n            return False\n\n        try:\n            return self._cache[level]\n        except KeyError:\n            _acquireLock()\n            try:\n                if self.manager.disable >= level:\n                    is_enabled = self._cache[level] = False\n                else:\n                    is_enabled = self._cache[level] = (\n                        level >= self.getEffectiveLevel()\n                    )\n            finally:\n                _releaseLock()\n            return is_enabled\n\n    def getChild(self, suffix):\n        \"\"\"\n        Get a logger which is a descendant to this one.\n\n        This is a convenience method, such that\n\n        logging.getLogger('abc').getChild('def.ghi')\n\n        is the same as\n\n        logging.getLogger('abc.def.ghi')\n\n        It's useful, for example, when the parent logger is named using\n        __name__ rather than a literal string.\n        \"\"\"\n        if self.root is not self:\n            suffix = '.'.join((self.name, suffix))\n        return self.manager.getLogger(suffix)\n\n    def __repr__(self):\n        level = getLevelName(self.getEffectiveLevel())\n        return '<%s %s (%s)>' % (self.__class__.__name__, self.name, level)\n\n    def __reduce__(self):\n        # In general, only the root logger will not be accessible via its name.\n        # However, the root logger's class has its own __reduce__ method.\n        if getLogger(self.name) is not self:\n            import pickle\n            raise pickle.PicklingError('logger cannot be pickled')\n        return getLogger, (self.name,)\n\n\nclass RootLogger(Logger):\n    \"\"\"\n    A root logger is not that different to any other logger, except that\n    it must have a logging level and there is only one instance of it in\n    the hierarchy.\n    \"\"\"\n    def __init__(self, level):\n        \"\"\"\n        Initialize the logger with the name \"root\".\n        \"\"\"\n        Logger.__init__(self, \"root\", level)\n\n    def __reduce__(self):\n        return getLogger, ()\n\n_loggerClass = Logger\n\nclass LoggerAdapter(object):\n    \"\"\"\n    An adapter for loggers which makes it easier to specify contextual\n    information in logging output.\n    \"\"\"\n\n    def __init__(self, logger, extra=None):\n        \"\"\"\n        Initialize the adapter with a logger and a dict-like object which\n        provides contextual information. This constructor signature allows\n        easy stacking of LoggerAdapters, if so desired.\n\n        You can effectively pass keyword arguments as shown in the\n        following example:\n\n        adapter = LoggerAdapter(someLogger, dict(p1=v1, p2=\"v2\"))\n        \"\"\"\n        self.logger = logger\n        self.extra = extra\n\n    def process(self, msg, kwargs):\n        \"\"\"\n        Process the logging message and keyword arguments passed in to\n        a logging call to insert contextual information. You can either\n        manipulate the message itself, the keyword args or both. Return\n        the message and kwargs modified (or not) to suit your needs.\n\n        Normally, you'll only need to override this one method in a\n        LoggerAdapter subclass for your specific needs.\n        \"\"\"\n        kwargs[\"extra\"] = self.extra\n        return msg, kwargs\n\n    #\n    # Boilerplate convenience methods\n    #\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a debug call to the underlying logger.\n        \"\"\"\n        self.log(DEBUG, msg, *args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an info call to the underlying logger.\n        \"\"\"\n        self.log(INFO, msg, *args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a warning call to the underlying logger.\n        \"\"\"\n        self.log(WARNING, msg, *args, **kwargs)\n\n    def warn(self, msg, *args, **kwargs):\n        warnings.warn(\"The 'warn' method is deprecated, \"\n            \"use 'warning' instead\", DeprecationWarning, 2)\n        self.warning(msg, *args, **kwargs)\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an error call to the underlying logger.\n        \"\"\"\n        self.log(ERROR, msg, *args, **kwargs)\n\n    def exception(self, msg, *args, exc_info=True, **kwargs):\n        \"\"\"\n        Delegate an exception call to the underlying logger.\n        \"\"\"\n        self.log(ERROR, msg, *args, exc_info=exc_info, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a critical call to the underlying logger.\n        \"\"\"\n        self.log(CRITICAL, msg, *args, **kwargs)\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a log call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        if self.isEnabledFor(level):\n            msg, kwargs = self.process(msg, kwargs)\n            self.logger.log(level, msg, *args, **kwargs)\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        Is this logger enabled for level 'level'?\n        \"\"\"\n        return self.logger.isEnabledFor(level)\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the specified level on the underlying logger.\n        \"\"\"\n        self.logger.setLevel(level)\n\n    def getEffectiveLevel(self):\n        \"\"\"\n        Get the effective level for the underlying logger.\n        \"\"\"\n        return self.logger.getEffectiveLevel()\n\n    def hasHandlers(self):\n        \"\"\"\n        See if the underlying logger has any handlers.\n        \"\"\"\n        return self.logger.hasHandlers()\n\n    def _log(self, level, msg, args, exc_info=None, extra=None, stack_info=False):\n        \"\"\"\n        Low-level log implementation, proxied to allow nested logger adapters.\n        \"\"\"\n        return self.logger._log(\n            level,\n            msg,\n            args,\n            exc_info=exc_info,\n            extra=extra,\n            stack_info=stack_info,\n        )\n\n    @property\n    def manager(self):\n        return self.logger.manager\n\n    @manager.setter\n    def manager(self, value):\n        self.logger.manager = value\n\n    @property\n    def name(self):\n        return self.logger.name\n\n    def __repr__(self):\n        logger = self.logger\n        level = getLevelName(logger.getEffectiveLevel())\n        return '<%s %s (%s)>' % (self.__class__.__name__, logger.name, level)\n\nroot = RootLogger(WARNING)\nLogger.root = root\nLogger.manager = Manager(Logger.root)\n\n#---------------------------------------------------------------------------\n# Configuration classes and functions\n#---------------------------------------------------------------------------\n\ndef basicConfig(**kwargs):\n    \"\"\"\n    Do basic configuration for the logging system.\n\n    This function does nothing if the root logger already has handlers\n    configured, unless the keyword argument *force* is set to ``True``.\n    It is a convenience method intended for use by simple scripts\n    to do one-shot configuration of the logging package.\n\n    The default behaviour is to create a StreamHandler which writes to\n    sys.stderr, set a formatter using the BASIC_FORMAT format string, and\n    add the handler to the root logger.\n\n    A number of optional keyword arguments may be specified, which can alter\n    the default behaviour.\n\n    filename  Specifies that a FileHandler be created, using the specified\n              filename, rather than a StreamHandler.\n    filemode  Specifies the mode to open the file, if filename is specified\n              (if filemode is unspecified, it defaults to 'a').\n    format    Use the specified format string for the handler.\n    datefmt   Use the specified date/time format.\n    style     If a format string is specified, use this to specify the\n              type of format string (possible values '%', '{', '$', for\n              %-formatting, :meth:`str.format` and :class:`string.Template`\n              - defaults to '%').\n    level     Set the root logger level to the specified level.\n    stream    Use the specified stream to initialize the StreamHandler. Note\n              that this argument is incompatible with 'filename' - if both\n              are present, 'stream' is ignored.\n    handlers  If specified, this should be an iterable of already created\n              handlers, which will be added to the root handler. Any handler\n              in the list which does not have a formatter assigned will be\n              assigned the formatter created in this function.\n    force     If this keyword  is specified as true, any existing handlers\n              attached to the root logger are removed and closed, before\n              carrying out the configuration as specified by the other\n              arguments.\n    encoding  If specified together with a filename, this encoding is passed to\n              the created FileHandler, causing it to be used when the file is\n              opened.\n    errors    If specified together with a filename, this value is passed to the\n              created FileHandler, causing it to be used when the file is\n              opened in text mode. If not specified, the default value is\n              `backslashreplace`.\n\n    Note that you could specify a stream created using open(filename, mode)\n    rather than passing the filename and mode in. However, it should be\n    remembered that StreamHandler does not close its stream (since it may be\n    using sys.stdout or sys.stderr), whereas FileHandler closes its stream\n    when the handler is closed.\n\n    .. versionchanged:: 3.2\n       Added the ``style`` parameter.\n\n    .. versionchanged:: 3.3\n       Added the ``handlers`` parameter. A ``ValueError`` is now thrown for\n       incompatible arguments (e.g. ``handlers`` specified together with\n       ``filename``/``filemode``, or ``filename``/``filemode`` specified\n       together with ``stream``, or ``handlers`` specified together with\n       ``stream``.\n\n    .. versionchanged:: 3.8\n       Added the ``force`` parameter.\n\n    .. versionchanged:: 3.9\n       Added the ``encoding`` and ``errors`` parameters.\n    \"\"\"\n    # Add thread safety in case someone mistakenly calls\n    # basicConfig() from multiple threads\n    _acquireLock()\n    try:\n        force = kwargs.pop('force', False)\n        encoding = kwargs.pop('encoding', None)\n        errors = kwargs.pop('errors', 'backslashreplace')\n        if force:\n            for h in root.handlers[:]:\n                root.removeHandler(h)\n                h.close()\n        if len(root.handlers) == 0:\n            handlers = kwargs.pop(\"handlers\", None)\n            if handlers is None:\n                if \"stream\" in kwargs and \"filename\" in kwargs:\n                    raise ValueError(\"'stream' and 'filename' should not be \"\n                                     \"specified together\")\n            else:\n                if \"stream\" in kwargs or \"filename\" in kwargs:\n                    raise ValueError(\"'stream' or 'filename' should not be \"\n                                     \"specified together with 'handlers'\")\n            if handlers is None:\n                filename = kwargs.pop(\"filename\", None)\n                mode = kwargs.pop(\"filemode\", 'a')\n                if filename:\n                    if 'b' in mode:\n                        errors = None\n                    else:\n                        encoding = io.text_encoding(encoding)\n                    h = FileHandler(filename, mode,\n                                    encoding=encoding, errors=errors)\n                else:\n                    stream = kwargs.pop(\"stream\", None)\n                    h = StreamHandler(stream)\n                handlers = [h]\n            dfs = kwargs.pop(\"datefmt\", None)\n            style = kwargs.pop(\"style\", '%')\n            if style not in _STYLES:\n                raise ValueError('Style must be one of: %s' % ','.join(\n                                 _STYLES.keys()))\n            fs = kwargs.pop(\"format\", _STYLES[style][1])\n            fmt = Formatter(fs, dfs, style)\n            for h in handlers:\n                if h.formatter is None:\n                    h.setFormatter(fmt)\n                root.addHandler(h)\n            level = kwargs.pop(\"level\", None)\n            if level is not None:\n                root.setLevel(level)\n            if kwargs:\n                keys = ', '.join(kwargs.keys())\n                raise ValueError('Unrecognised argument(s): %s' % keys)\n    finally:\n        _releaseLock()\n\n#---------------------------------------------------------------------------\n# Utility functions at module level.\n# Basically delegate everything to the root logger.\n#---------------------------------------------------------------------------\n\ndef getLogger(name=None):\n    \"\"\"\n    Return a logger with the specified name, creating it if necessary.\n\n    If no name is specified, return the root logger.\n    \"\"\"\n    if not name or isinstance(name, str) and name == root.name:\n        return root\n    return Logger.manager.getLogger(name)\n\ndef critical(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'CRITICAL' on the root logger. If the logger\n    has no handlers, call basicConfig() to add a console handler with a\n    pre-defined format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.critical(msg, *args, **kwargs)\n\ndef fatal(msg, *args, **kwargs):\n    \"\"\"\n    Don't use this function, use critical() instead.\n    \"\"\"\n    critical(msg, *args, **kwargs)\n\ndef error(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.error(msg, *args, **kwargs)\n\ndef exception(msg, *args, exc_info=True, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger, with exception\n    information. If the logger has no handlers, basicConfig() is called to add\n    a console handler with a pre-defined format.\n    \"\"\"\n    error(msg, *args, exc_info=exc_info, **kwargs)\n\ndef warning(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'WARNING' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.warning(msg, *args, **kwargs)\n\ndef warn(msg, *args, **kwargs):\n    warnings.warn(\"The 'warn' function is deprecated, \"\n        \"use 'warning' instead\", DeprecationWarning, 2)\n    warning(msg, *args, **kwargs)\n\ndef info(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'INFO' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.info(msg, *args, **kwargs)\n\ndef debug(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'DEBUG' on the root logger. If the logger has\n    no handlers, call basicConfig() to add a console handler with a pre-defined\n    format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.debug(msg, *args, **kwargs)\n\ndef log(level, msg, *args, **kwargs):\n    \"\"\"\n    Log 'msg % args' with the integer severity 'level' on the root logger. If\n    the logger has no handlers, call basicConfig() to add a console handler\n    with a pre-defined format.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.log(level, msg, *args, **kwargs)\n\ndef disable(level=CRITICAL):\n    \"\"\"\n    Disable all logging calls of severity 'level' and below.\n    \"\"\"\n    root.manager.disable = level\n    root.manager._clear_cache()\n\ndef shutdown(handlerList=_handlerList):\n    \"\"\"\n    Perform any cleanup actions in the logging system (e.g. flushing\n    buffers).\n\n    Should be called at application exit.\n    \"\"\"\n    for wr in reversed(handlerList[:]):\n        #errors might occur, for example, if files are locked\n        #we just ignore them if raiseExceptions is not set\n        try:\n            h = wr()\n            if h:\n                try:\n                    h.acquire()\n                    h.flush()\n                    h.close()\n                except (OSError, ValueError):\n                    # Ignore errors which might be caused\n                    # because handlers have been closed but\n                    # references to them are still around at\n                    # application exit.\n                    pass\n                finally:\n                    h.release()\n        except: # ignore everything, as we're shutting down\n            if raiseExceptions:\n                raise\n            #else, swallow\n\n#Let's try and shutdown automatically on application exit...\nimport atexit\natexit.register(shutdown)\n\n# Null handler\n\nclass NullHandler(Handler):\n    \"\"\"\n    This handler does nothing. It's intended to be used to avoid the\n    \"No handlers could be found for logger XXX\" one-off warning. This is\n    important for library code, which may contain code to log events. If a user\n    of the library does not configure logging, the one-off warning might be\n    produced; to avoid this, the library developer simply needs to instantiate\n    a NullHandler and add it to the top-level logger of the library module or\n    package.\n    \"\"\"\n    def handle(self, record):\n        \"\"\"Stub.\"\"\"\n\n    def emit(self, record):\n        \"\"\"Stub.\"\"\"\n\n    def createLock(self):\n        self.lock = None\n\n    def _at_fork_reinit(self):\n        pass\n\n# Warnings integration\n\n_warnings_showwarning = None\n\ndef _showwarning(message, category, filename, lineno, file=None, line=None):\n    \"\"\"\n    Implementation of showwarnings which redirects to logging, which will first\n    check to see if the file parameter is None. If a file is specified, it will\n    delegate to the original warnings implementation of showwarning. Otherwise,\n    it will call warnings.formatwarning and will log the resulting string to a\n    warnings logger named \"py.warnings\" with level logging.WARNING.\n    \"\"\"\n    if file is not None:\n        if _warnings_showwarning is not None:\n            _warnings_showwarning(message, category, filename, lineno, file, line)\n    else:\n        s = warnings.formatwarning(message, category, filename, lineno, line)\n        logger = getLogger(\"py.warnings\")\n        if not logger.handlers:\n            logger.addHandler(NullHandler())\n        logger.warning(\"%s\", s)\n\ndef captureWarnings(capture):\n    \"\"\"\n    If capture is true, redirect all warnings to the logging package.\n    If capture is False, ensure that warnings are not redirected to logging\n    but to their original destinations.\n    \"\"\"\n    global _warnings_showwarning\n    if capture:\n        if _warnings_showwarning is None:\n            _warnings_showwarning = warnings.showwarning\n            warnings.showwarning = _showwarning\n    else:\n        if _warnings_showwarning is not None:\n            warnings.showwarning = _warnings_showwarning\n            _warnings_showwarning = None\n", 2261], "D:\\Program\\anaconda3\\lib\\socket.py": ["# Wrapper module for _socket, providing some additional facilities\n# implemented in Python.\n\n\"\"\"\\\nThis module provides socket operations and some related functions.\nOn Unix, it supports IP (Internet Protocol) and Unix domain sockets.\nOn other systems, it only supports IP. Functions specific for a\nsocket are available as methods of the socket object.\n\nFunctions:\n\nsocket() -- create a new socket object\nsocketpair() -- create a pair of new socket objects [*]\nfromfd() -- create a socket object from an open file descriptor [*]\nsend_fds() -- Send file descriptor to the socket.\nrecv_fds() -- Recieve file descriptors from the socket.\nfromshare() -- create a socket object from data received from socket.share() [*]\ngethostname() -- return the current hostname\ngethostbyname() -- map a hostname to its IP number\ngethostbyaddr() -- map an IP number or hostname to DNS info\ngetservbyname() -- map a service name and a protocol name to a port number\ngetprotobyname() -- map a protocol name (e.g. 'tcp') to a number\nntohs(), ntohl() -- convert 16, 32 bit int from network to host byte order\nhtons(), htonl() -- convert 16, 32 bit int from host to network byte order\ninet_aton() -- convert IP addr string (123.45.67.89) to 32-bit packed format\ninet_ntoa() -- convert 32-bit packed format IP to string (123.45.67.89)\nsocket.getdefaulttimeout() -- get the default timeout value\nsocket.setdefaulttimeout() -- set the default timeout value\ncreate_connection() -- connects to an address, with an optional timeout and\n                       optional source address.\n\n [*] not available on all platforms!\n\nSpecial objects:\n\nSocketType -- type object for socket objects\nerror -- exception raised for I/O errors\nhas_ipv6 -- boolean value indicating if IPv6 is supported\n\nIntEnum constants:\n\nAF_INET, AF_UNIX -- socket domains (first argument to socket() call)\nSOCK_STREAM, SOCK_DGRAM, SOCK_RAW -- socket types (second argument)\n\nInteger constants:\n\nMany other constants may be defined; these may be used in calls to\nthe setsockopt() and getsockopt() methods.\n\"\"\"\n\nimport _socket\nfrom _socket import *\n\nimport os, sys, io, selectors\nfrom enum import IntEnum, IntFlag\n\ntry:\n    import errno\nexcept ImportError:\n    errno = None\nEBADF = getattr(errno, 'EBADF', 9)\nEAGAIN = getattr(errno, 'EAGAIN', 11)\nEWOULDBLOCK = getattr(errno, 'EWOULDBLOCK', 11)\n\n__all__ = [\"fromfd\", \"getfqdn\", \"create_connection\", \"create_server\",\n           \"has_dualstack_ipv6\", \"AddressFamily\", \"SocketKind\"]\n__all__.extend(os._get_exports_list(_socket))\n\n# Set up the socket.AF_* socket.SOCK_* constants as members of IntEnums for\n# nicer string representations.\n# Note that _socket only knows about the integer values. The public interface\n# in this module understands the enums and translates them back from integers\n# where needed (e.g. .family property of a socket object).\n\nIntEnum._convert_(\n        'AddressFamily',\n        __name__,\n        lambda C: C.isupper() and C.startswith('AF_'))\n\nIntEnum._convert_(\n        'SocketKind',\n        __name__,\n        lambda C: C.isupper() and C.startswith('SOCK_'))\n\nIntFlag._convert_(\n        'MsgFlag',\n        __name__,\n        lambda C: C.isupper() and C.startswith('MSG_'))\n\nIntFlag._convert_(\n        'AddressInfo',\n        __name__,\n        lambda C: C.isupper() and C.startswith('AI_'))\n\n_LOCALHOST    = '127.0.0.1'\n_LOCALHOST_V6 = '::1'\n\n\ndef _intenum_converter(value, enum_klass):\n    \"\"\"Convert a numeric family value to an IntEnum member.\n\n    If it's not a known member, return the numeric value itself.\n    \"\"\"\n    try:\n        return enum_klass(value)\n    except ValueError:\n        return value\n\n\n# WSA error codes\nif sys.platform.lower().startswith(\"win\"):\n    errorTab = {}\n    errorTab[6] = \"Specified event object handle is invalid.\"\n    errorTab[8] = \"Insufficient memory available.\"\n    errorTab[87] = \"One or more parameters are invalid.\"\n    errorTab[995] = \"Overlapped operation aborted.\"\n    errorTab[996] = \"Overlapped I/O event object not in signaled state.\"\n    errorTab[997] = \"Overlapped operation will complete later.\"\n    errorTab[10004] = \"The operation was interrupted.\"\n    errorTab[10009] = \"A bad file handle was passed.\"\n    errorTab[10013] = \"Permission denied.\"\n    errorTab[10014] = \"A fault occurred on the network??\"  # WSAEFAULT\n    errorTab[10022] = \"An invalid operation was attempted.\"\n    errorTab[10024] = \"Too many open files.\"\n    errorTab[10035] = \"The socket operation would block\"\n    errorTab[10036] = \"A blocking operation is already in progress.\"\n    errorTab[10037] = \"Operation already in progress.\"\n    errorTab[10038] = \"Socket operation on nonsocket.\"\n    errorTab[10039] = \"Destination address required.\"\n    errorTab[10040] = \"Message too long.\"\n    errorTab[10041] = \"Protocol wrong type for socket.\"\n    errorTab[10042] = \"Bad protocol option.\"\n    errorTab[10043] = \"Protocol not supported.\"\n    errorTab[10044] = \"Socket type not supported.\"\n    errorTab[10045] = \"Operation not supported.\"\n    errorTab[10046] = \"Protocol family not supported.\"\n    errorTab[10047] = \"Address family not supported by protocol family.\"\n    errorTab[10048] = \"The network address is in use.\"\n    errorTab[10049] = \"Cannot assign requested address.\"\n    errorTab[10050] = \"Network is down.\"\n    errorTab[10051] = \"Network is unreachable.\"\n    errorTab[10052] = \"Network dropped connection on reset.\"\n    errorTab[10053] = \"Software caused connection abort.\"\n    errorTab[10054] = \"The connection has been reset.\"\n    errorTab[10055] = \"No buffer space available.\"\n    errorTab[10056] = \"Socket is already connected.\"\n    errorTab[10057] = \"Socket is not connected.\"\n    errorTab[10058] = \"The network has been shut down.\"\n    errorTab[10059] = \"Too many references.\"\n    errorTab[10060] = \"The operation timed out.\"\n    errorTab[10061] = \"Connection refused.\"\n    errorTab[10062] = \"Cannot translate name.\"\n    errorTab[10063] = \"The name is too long.\"\n    errorTab[10064] = \"The host is down.\"\n    errorTab[10065] = \"The host is unreachable.\"\n    errorTab[10066] = \"Directory not empty.\"\n    errorTab[10067] = \"Too many processes.\"\n    errorTab[10068] = \"User quota exceeded.\"\n    errorTab[10069] = \"Disk quota exceeded.\"\n    errorTab[10070] = \"Stale file handle reference.\"\n    errorTab[10071] = \"Item is remote.\"\n    errorTab[10091] = \"Network subsystem is unavailable.\"\n    errorTab[10092] = \"Winsock.dll version out of range.\"\n    errorTab[10093] = \"Successful WSAStartup not yet performed.\"\n    errorTab[10101] = \"Graceful shutdown in progress.\"\n    errorTab[10102] = \"No more results from WSALookupServiceNext.\"\n    errorTab[10103] = \"Call has been canceled.\"\n    errorTab[10104] = \"Procedure call table is invalid.\"\n    errorTab[10105] = \"Service provider is invalid.\"\n    errorTab[10106] = \"Service provider failed to initialize.\"\n    errorTab[10107] = \"System call failure.\"\n    errorTab[10108] = \"Service not found.\"\n    errorTab[10109] = \"Class type not found.\"\n    errorTab[10110] = \"No more results from WSALookupServiceNext.\"\n    errorTab[10111] = \"Call was canceled.\"\n    errorTab[10112] = \"Database query was refused.\"\n    errorTab[11001] = \"Host not found.\"\n    errorTab[11002] = \"Nonauthoritative host not found.\"\n    errorTab[11003] = \"This is a nonrecoverable error.\"\n    errorTab[11004] = \"Valid name, no data record requested type.\"\n    errorTab[11005] = \"QoS receivers.\"\n    errorTab[11006] = \"QoS senders.\"\n    errorTab[11007] = \"No QoS senders.\"\n    errorTab[11008] = \"QoS no receivers.\"\n    errorTab[11009] = \"QoS request confirmed.\"\n    errorTab[11010] = \"QoS admission error.\"\n    errorTab[11011] = \"QoS policy failure.\"\n    errorTab[11012] = \"QoS bad style.\"\n    errorTab[11013] = \"QoS bad object.\"\n    errorTab[11014] = \"QoS traffic control error.\"\n    errorTab[11015] = \"QoS generic error.\"\n    errorTab[11016] = \"QoS service type error.\"\n    errorTab[11017] = \"QoS flowspec error.\"\n    errorTab[11018] = \"Invalid QoS provider buffer.\"\n    errorTab[11019] = \"Invalid QoS filter style.\"\n    errorTab[11020] = \"Invalid QoS filter style.\"\n    errorTab[11021] = \"Incorrect QoS filter count.\"\n    errorTab[11022] = \"Invalid QoS object length.\"\n    errorTab[11023] = \"Incorrect QoS flow count.\"\n    errorTab[11024] = \"Unrecognized QoS object.\"\n    errorTab[11025] = \"Invalid QoS policy object.\"\n    errorTab[11026] = \"Invalid QoS flow descriptor.\"\n    errorTab[11027] = \"Invalid QoS provider-specific flowspec.\"\n    errorTab[11028] = \"Invalid QoS provider-specific filterspec.\"\n    errorTab[11029] = \"Invalid QoS shape discard mode object.\"\n    errorTab[11030] = \"Invalid QoS shaping rate object.\"\n    errorTab[11031] = \"Reserved policy QoS element type.\"\n    __all__.append(\"errorTab\")\n\n\nclass _GiveupOnSendfile(Exception): pass\n\n\nclass socket(_socket.socket):\n\n    \"\"\"A subclass of _socket.socket adding the makefile() method.\"\"\"\n\n    __slots__ = [\"__weakref__\", \"_io_refs\", \"_closed\"]\n\n    def __init__(self, family=-1, type=-1, proto=-1, fileno=None):\n        # For user code address family and type values are IntEnum members, but\n        # for the underlying _socket.socket they're just integers. The\n        # constructor of _socket.socket converts the given argument to an\n        # integer automatically.\n        if fileno is None:\n            if family == -1:\n                family = AF_INET\n            if type == -1:\n                type = SOCK_STREAM\n            if proto == -1:\n                proto = 0\n        _socket.socket.__init__(self, family, type, proto, fileno)\n        self._io_refs = 0\n        self._closed = False\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args):\n        if not self._closed:\n            self.close()\n\n    def __repr__(self):\n        \"\"\"Wrap __repr__() to reveal the real class name and socket\n        address(es).\n        \"\"\"\n        closed = getattr(self, '_closed', False)\n        s = \"<%s.%s%s fd=%i, family=%s, type=%s, proto=%i\" \\\n            % (self.__class__.__module__,\n               self.__class__.__qualname__,\n               \" [closed]\" if closed else \"\",\n               self.fileno(),\n               self.family,\n               self.type,\n               self.proto)\n        if not closed:\n            try:\n                laddr = self.getsockname()\n                if laddr:\n                    s += \", laddr=%s\" % str(laddr)\n            except error:\n                pass\n            try:\n                raddr = self.getpeername()\n                if raddr:\n                    s += \", raddr=%s\" % str(raddr)\n            except error:\n                pass\n        s += '>'\n        return s\n\n    def __getstate__(self):\n        raise TypeError(f\"cannot pickle {self.__class__.__name__!r} object\")\n\n    def dup(self):\n        \"\"\"dup() -> socket object\n\n        Duplicate the socket. Return a new socket object connected to the same\n        system resource. The new socket is non-inheritable.\n        \"\"\"\n        fd = dup(self.fileno())\n        sock = self.__class__(self.family, self.type, self.proto, fileno=fd)\n        sock.settimeout(self.gettimeout())\n        return sock\n\n    def accept(self):\n        \"\"\"accept() -> (socket object, address info)\n\n        Wait for an incoming connection.  Return a new socket\n        representing the connection, and the address of the client.\n        For IP sockets, the address info is a pair (hostaddr, port).\n        \"\"\"\n        fd, addr = self._accept()\n        sock = socket(self.family, self.type, self.proto, fileno=fd)\n        # Issue #7995: if no default timeout is set and the listening\n        # socket had a (non-zero) timeout, force the new socket in blocking\n        # mode to override platform-specific socket flags inheritance.\n        if getdefaulttimeout() is None and self.gettimeout():\n            sock.setblocking(True)\n        return sock, addr\n\n    def makefile(self, mode=\"r\", buffering=None, *,\n                 encoding=None, errors=None, newline=None):\n        \"\"\"makefile(...) -> an I/O stream connected to the socket\n\n        The arguments are as for io.open() after the filename, except the only\n        supported mode values are 'r' (default), 'w' and 'b'.\n        \"\"\"\n        # XXX refactor to share code?\n        if not set(mode) <= {\"r\", \"w\", \"b\"}:\n            raise ValueError(\"invalid mode %r (only r, w, b allowed)\" % (mode,))\n        writing = \"w\" in mode\n        reading = \"r\" in mode or not writing\n        assert reading or writing\n        binary = \"b\" in mode\n        rawmode = \"\"\n        if reading:\n            rawmode += \"r\"\n        if writing:\n            rawmode += \"w\"\n        raw = SocketIO(self, rawmode)\n        self._io_refs += 1\n        if buffering is None:\n            buffering = -1\n        if buffering < 0:\n            buffering = io.DEFAULT_BUFFER_SIZE\n        if buffering == 0:\n            if not binary:\n                raise ValueError(\"unbuffered streams must be binary\")\n            return raw\n        if reading and writing:\n            buffer = io.BufferedRWPair(raw, raw, buffering)\n        elif reading:\n            buffer = io.BufferedReader(raw, buffering)\n        else:\n            assert writing\n            buffer = io.BufferedWriter(raw, buffering)\n        if binary:\n            return buffer\n        encoding = io.text_encoding(encoding)\n        text = io.TextIOWrapper(buffer, encoding, errors, newline)\n        text.mode = mode\n        return text\n\n    if hasattr(os, 'sendfile'):\n\n        def _sendfile_use_sendfile(self, file, offset=0, count=None):\n            self._check_sendfile_params(file, offset, count)\n            sockno = self.fileno()\n            try:\n                fileno = file.fileno()\n            except (AttributeError, io.UnsupportedOperation) as err:\n                raise _GiveupOnSendfile(err)  # not a regular file\n            try:\n                fsize = os.fstat(fileno).st_size\n            except OSError as err:\n                raise _GiveupOnSendfile(err)  # not a regular file\n            if not fsize:\n                return 0  # empty file\n            # Truncate to 1GiB to avoid OverflowError, see bpo-38319.\n            blocksize = min(count or fsize, 2 ** 30)\n            timeout = self.gettimeout()\n            if timeout == 0:\n                raise ValueError(\"non-blocking sockets are not supported\")\n            # poll/select have the advantage of not requiring any\n            # extra file descriptor, contrarily to epoll/kqueue\n            # (also, they require a single syscall).\n            if hasattr(selectors, 'PollSelector'):\n                selector = selectors.PollSelector()\n            else:\n                selector = selectors.SelectSelector()\n            selector.register(sockno, selectors.EVENT_WRITE)\n\n            total_sent = 0\n            # localize variable access to minimize overhead\n            selector_select = selector.select\n            os_sendfile = os.sendfile\n            try:\n                while True:\n                    if timeout and not selector_select(timeout):\n                        raise TimeoutError('timed out')\n                    if count:\n                        blocksize = count - total_sent\n                        if blocksize <= 0:\n                            break\n                    try:\n                        sent = os_sendfile(sockno, fileno, offset, blocksize)\n                    except BlockingIOError:\n                        if not timeout:\n                            # Block until the socket is ready to send some\n                            # data; avoids hogging CPU resources.\n                            selector_select()\n                        continue\n                    except OSError as err:\n                        if total_sent == 0:\n                            # We can get here for different reasons, the main\n                            # one being 'file' is not a regular mmap(2)-like\n                            # file, in which case we'll fall back on using\n                            # plain send().\n                            raise _GiveupOnSendfile(err)\n                        raise err from None\n                    else:\n                        if sent == 0:\n                            break  # EOF\n                        offset += sent\n                        total_sent += sent\n                return total_sent\n            finally:\n                if total_sent > 0 and hasattr(file, 'seek'):\n                    file.seek(offset)\n    else:\n        def _sendfile_use_sendfile(self, file, offset=0, count=None):\n            raise _GiveupOnSendfile(\n                \"os.sendfile() not available on this platform\")\n\n    def _sendfile_use_send(self, file, offset=0, count=None):\n        self._check_sendfile_params(file, offset, count)\n        if self.gettimeout() == 0:\n            raise ValueError(\"non-blocking sockets are not supported\")\n        if offset:\n            file.seek(offset)\n        blocksize = min(count, 8192) if count else 8192\n        total_sent = 0\n        # localize variable access to minimize overhead\n        file_read = file.read\n        sock_send = self.send\n        try:\n            while True:\n                if count:\n                    blocksize = min(count - total_sent, blocksize)\n                    if blocksize <= 0:\n                        break\n                data = memoryview(file_read(blocksize))\n                if not data:\n                    break  # EOF\n                while True:\n                    try:\n                        sent = sock_send(data)\n                    except BlockingIOError:\n                        continue\n                    else:\n                        total_sent += sent\n                        if sent < len(data):\n                            data = data[sent:]\n                        else:\n                            break\n            return total_sent\n        finally:\n            if total_sent > 0 and hasattr(file, 'seek'):\n                file.seek(offset + total_sent)\n\n    def _check_sendfile_params(self, file, offset, count):\n        if 'b' not in getattr(file, 'mode', 'b'):\n            raise ValueError(\"file should be opened in binary mode\")\n        if not self.type & SOCK_STREAM:\n            raise ValueError(\"only SOCK_STREAM type sockets are supported\")\n        if count is not None:\n            if not isinstance(count, int):\n                raise TypeError(\n                    \"count must be a positive integer (got {!r})\".format(count))\n            if count <= 0:\n                raise ValueError(\n                    \"count must be a positive integer (got {!r})\".format(count))\n\n    def sendfile(self, file, offset=0, count=None):\n        \"\"\"sendfile(file[, offset[, count]]) -> sent\n\n        Send a file until EOF is reached by using high-performance\n        os.sendfile() and return the total number of bytes which\n        were sent.\n        *file* must be a regular file object opened in binary mode.\n        If os.sendfile() is not available (e.g. Windows) or file is\n        not a regular file socket.send() will be used instead.\n        *offset* tells from where to start reading the file.\n        If specified, *count* is the total number of bytes to transmit\n        as opposed to sending the file until EOF is reached.\n        File position is updated on return or also in case of error in\n        which case file.tell() can be used to figure out the number of\n        bytes which were sent.\n        The socket must be of SOCK_STREAM type.\n        Non-blocking sockets are not supported.\n        \"\"\"\n        try:\n            return self._sendfile_use_sendfile(file, offset, count)\n        except _GiveupOnSendfile:\n            return self._sendfile_use_send(file, offset, count)\n\n    def _decref_socketios(self):\n        if self._io_refs > 0:\n            self._io_refs -= 1\n        if self._closed:\n            self.close()\n\n    def _real_close(self, _ss=_socket.socket):\n        # This function should not reference any globals. See issue #808164.\n        _ss.close(self)\n\n    def close(self):\n        # This function should not reference any globals. See issue #808164.\n        self._closed = True\n        if self._io_refs <= 0:\n            self._real_close()\n\n    def detach(self):\n        \"\"\"detach() -> file descriptor\n\n        Close the socket object without closing the underlying file descriptor.\n        The object cannot be used after this call, but the file descriptor\n        can be reused for other purposes.  The file descriptor is returned.\n        \"\"\"\n        self._closed = True\n        return super().detach()\n\n    @property\n    def family(self):\n        \"\"\"Read-only access to the address family for this socket.\n        \"\"\"\n        return _intenum_converter(super().family, AddressFamily)\n\n    @property\n    def type(self):\n        \"\"\"Read-only access to the socket type.\n        \"\"\"\n        return _intenum_converter(super().type, SocketKind)\n\n    if os.name == 'nt':\n        def get_inheritable(self):\n            return os.get_handle_inheritable(self.fileno())\n        def set_inheritable(self, inheritable):\n            os.set_handle_inheritable(self.fileno(), inheritable)\n    else:\n        def get_inheritable(self):\n            return os.get_inheritable(self.fileno())\n        def set_inheritable(self, inheritable):\n            os.set_inheritable(self.fileno(), inheritable)\n    get_inheritable.__doc__ = \"Get the inheritable flag of the socket\"\n    set_inheritable.__doc__ = \"Set the inheritable flag of the socket\"\n\ndef fromfd(fd, family, type, proto=0):\n    \"\"\" fromfd(fd, family, type[, proto]) -> socket object\n\n    Create a socket object from a duplicate of the given file\n    descriptor.  The remaining arguments are the same as for socket().\n    \"\"\"\n    nfd = dup(fd)\n    return socket(family, type, proto, nfd)\n\nif hasattr(_socket.socket, \"sendmsg\"):\n    import array\n\n    def send_fds(sock, buffers, fds, flags=0, address=None):\n        \"\"\" send_fds(sock, buffers, fds[, flags[, address]]) -> integer\n\n        Send the list of file descriptors fds over an AF_UNIX socket.\n        \"\"\"\n        return sock.sendmsg(buffers, [(_socket.SOL_SOCKET,\n            _socket.SCM_RIGHTS, array.array(\"i\", fds))])\n    __all__.append(\"send_fds\")\n\nif hasattr(_socket.socket, \"recvmsg\"):\n    import array\n\n    def recv_fds(sock, bufsize, maxfds, flags=0):\n        \"\"\" recv_fds(sock, bufsize, maxfds[, flags]) -> (data, list of file\n        descriptors, msg_flags, address)\n\n        Receive up to maxfds file descriptors returning the message\n        data and a list containing the descriptors.\n        \"\"\"\n        # Array of ints\n        fds = array.array(\"i\")\n        msg, ancdata, flags, addr = sock.recvmsg(bufsize,\n            _socket.CMSG_LEN(maxfds * fds.itemsize))\n        for cmsg_level, cmsg_type, cmsg_data in ancdata:\n            if (cmsg_level == _socket.SOL_SOCKET and cmsg_type == _socket.SCM_RIGHTS):\n                fds.frombytes(cmsg_data[:\n                        len(cmsg_data) - (len(cmsg_data) % fds.itemsize)])\n\n        return msg, list(fds), flags, addr\n    __all__.append(\"recv_fds\")\n\nif hasattr(_socket.socket, \"share\"):\n    def fromshare(info):\n        \"\"\" fromshare(info) -> socket object\n\n        Create a socket object from the bytes object returned by\n        socket.share(pid).\n        \"\"\"\n        return socket(0, 0, 0, info)\n    __all__.append(\"fromshare\")\n\nif hasattr(_socket, \"socketpair\"):\n\n    def socketpair(family=None, type=SOCK_STREAM, proto=0):\n        \"\"\"socketpair([family[, type[, proto]]]) -> (socket object, socket object)\n\n        Create a pair of socket objects from the sockets returned by the platform\n        socketpair() function.\n        The arguments are the same as for socket() except the default family is\n        AF_UNIX if defined on the platform; otherwise, the default is AF_INET.\n        \"\"\"\n        if family is None:\n            try:\n                family = AF_UNIX\n            except NameError:\n                family = AF_INET\n        a, b = _socket.socketpair(family, type, proto)\n        a = socket(family, type, proto, a.detach())\n        b = socket(family, type, proto, b.detach())\n        return a, b\n\nelse:\n\n    # Origin: https://gist.github.com/4325783, by Geert Jansen.  Public domain.\n    def socketpair(family=AF_INET, type=SOCK_STREAM, proto=0):\n        if family == AF_INET:\n            host = _LOCALHOST\n        elif family == AF_INET6:\n            host = _LOCALHOST_V6\n        else:\n            raise ValueError(\"Only AF_INET and AF_INET6 socket address families \"\n                             \"are supported\")\n        if type != SOCK_STREAM:\n            raise ValueError(\"Only SOCK_STREAM socket type is supported\")\n        if proto != 0:\n            raise ValueError(\"Only protocol zero is supported\")\n\n        # We create a connected TCP socket. Note the trick with\n        # setblocking(False) that prevents us from having to create a thread.\n        lsock = socket(family, type, proto)\n        try:\n            lsock.bind((host, 0))\n            lsock.listen()\n            # On IPv6, ignore flow_info and scope_id\n            addr, port = lsock.getsockname()[:2]\n            csock = socket(family, type, proto)\n            try:\n                csock.setblocking(False)\n                try:\n                    csock.connect((addr, port))\n                except (BlockingIOError, InterruptedError):\n                    pass\n                csock.setblocking(True)\n                ssock, _ = lsock.accept()\n            except:\n                csock.close()\n                raise\n        finally:\n            lsock.close()\n        return (ssock, csock)\n    __all__.append(\"socketpair\")\n\nsocketpair.__doc__ = \"\"\"socketpair([family[, type[, proto]]]) -> (socket object, socket object)\nCreate a pair of socket objects from the sockets returned by the platform\nsocketpair() function.\nThe arguments are the same as for socket() except the default family is AF_UNIX\nif defined on the platform; otherwise, the default is AF_INET.\n\"\"\"\n\n_blocking_errnos = { EAGAIN, EWOULDBLOCK }\n\nclass SocketIO(io.RawIOBase):\n\n    \"\"\"Raw I/O implementation for stream sockets.\n\n    This class supports the makefile() method on sockets.  It provides\n    the raw I/O interface on top of a socket object.\n    \"\"\"\n\n    # One might wonder why not let FileIO do the job instead.  There are two\n    # main reasons why FileIO is not adapted:\n    # - it wouldn't work under Windows (where you can't used read() and\n    #   write() on a socket handle)\n    # - it wouldn't work with socket timeouts (FileIO would ignore the\n    #   timeout and consider the socket non-blocking)\n\n    # XXX More docs\n\n    def __init__(self, sock, mode):\n        if mode not in (\"r\", \"w\", \"rw\", \"rb\", \"wb\", \"rwb\"):\n            raise ValueError(\"invalid mode: %r\" % mode)\n        io.RawIOBase.__init__(self)\n        self._sock = sock\n        if \"b\" not in mode:\n            mode += \"b\"\n        self._mode = mode\n        self._reading = \"r\" in mode\n        self._writing = \"w\" in mode\n        self._timeout_occurred = False\n\n    def readinto(self, b):\n        \"\"\"Read up to len(b) bytes into the writable buffer *b* and return\n        the number of bytes read.  If the socket is non-blocking and no bytes\n        are available, None is returned.\n\n        If *b* is non-empty, a 0 return value indicates that the connection\n        was shutdown at the other end.\n        \"\"\"\n        self._checkClosed()\n        self._checkReadable()\n        if self._timeout_occurred:\n            raise OSError(\"cannot read from timed out object\")\n        while True:\n            try:\n                return self._sock.recv_into(b)\n            except timeout:\n                self._timeout_occurred = True\n                raise\n            except error as e:\n                if e.errno in _blocking_errnos:\n                    return None\n                raise\n\n    def write(self, b):\n        \"\"\"Write the given bytes or bytearray object *b* to the socket\n        and return the number of bytes written.  This can be less than\n        len(b) if not all data could be written.  If the socket is\n        non-blocking and no bytes could be written None is returned.\n        \"\"\"\n        self._checkClosed()\n        self._checkWritable()\n        try:\n            return self._sock.send(b)\n        except error as e:\n            # XXX what about EINTR?\n            if e.errno in _blocking_errnos:\n                return None\n            raise\n\n    def readable(self):\n        \"\"\"True if the SocketIO is open for reading.\n        \"\"\"\n        if self.closed:\n            raise ValueError(\"I/O operation on closed socket.\")\n        return self._reading\n\n    def writable(self):\n        \"\"\"True if the SocketIO is open for writing.\n        \"\"\"\n        if self.closed:\n            raise ValueError(\"I/O operation on closed socket.\")\n        return self._writing\n\n    def seekable(self):\n        \"\"\"True if the SocketIO is open for seeking.\n        \"\"\"\n        if self.closed:\n            raise ValueError(\"I/O operation on closed socket.\")\n        return super().seekable()\n\n    def fileno(self):\n        \"\"\"Return the file descriptor of the underlying socket.\n        \"\"\"\n        self._checkClosed()\n        return self._sock.fileno()\n\n    @property\n    def name(self):\n        if not self.closed:\n            return self.fileno()\n        else:\n            return -1\n\n    @property\n    def mode(self):\n        return self._mode\n\n    def close(self):\n        \"\"\"Close the SocketIO object.  This doesn't close the underlying\n        socket, except if all references to it have disappeared.\n        \"\"\"\n        if self.closed:\n            return\n        io.RawIOBase.close(self)\n        self._sock._decref_socketios()\n        self._sock = None\n\n\ndef getfqdn(name=''):\n    \"\"\"Get fully qualified domain name from name.\n\n    An empty argument is interpreted as meaning the local host.\n\n    First the hostname returned by gethostbyaddr() is checked, then\n    possibly existing aliases. In case no FQDN is available and `name`\n    was given, it is returned unchanged. If `name` was empty or '0.0.0.0',\n    hostname from gethostname() is returned.\n    \"\"\"\n    name = name.strip()\n    if not name or name == '0.0.0.0':\n        name = gethostname()\n    try:\n        hostname, aliases, ipaddrs = gethostbyaddr(name)\n    except error:\n        pass\n    else:\n        aliases.insert(0, hostname)\n        for name in aliases:\n            if '.' in name:\n                break\n        else:\n            name = hostname\n    return name\n\n\n_GLOBAL_DEFAULT_TIMEOUT = object()\n\ndef create_connection(address, timeout=_GLOBAL_DEFAULT_TIMEOUT,\n                      source_address=None):\n    \"\"\"Connect to *address* and return the socket object.\n\n    Convenience function.  Connect to *address* (a 2-tuple ``(host,\n    port)``) and return the socket object.  Passing the optional\n    *timeout* parameter will set the timeout on the socket instance\n    before attempting to connect.  If no *timeout* is supplied, the\n    global default timeout setting returned by :func:`getdefaulttimeout`\n    is used.  If *source_address* is set it must be a tuple of (host, port)\n    for the socket to bind as a source address before making the connection.\n    A host of '' or port 0 tells the OS to use the default.\n    \"\"\"\n\n    host, port = address\n    err = None\n    for res in getaddrinfo(host, port, 0, SOCK_STREAM):\n        af, socktype, proto, canonname, sa = res\n        sock = None\n        try:\n            sock = socket(af, socktype, proto)\n            if timeout is not _GLOBAL_DEFAULT_TIMEOUT:\n                sock.settimeout(timeout)\n            if source_address:\n                sock.bind(source_address)\n            sock.connect(sa)\n            # Break explicitly a reference cycle\n            err = None\n            return sock\n\n        except error as _:\n            err = _\n            if sock is not None:\n                sock.close()\n\n    if err is not None:\n        try:\n            raise err\n        finally:\n            # Break explicitly a reference cycle\n            err = None\n    else:\n        raise error(\"getaddrinfo returns an empty list\")\n\n\ndef has_dualstack_ipv6():\n    \"\"\"Return True if the platform supports creating a SOCK_STREAM socket\n    which can handle both AF_INET and AF_INET6 (IPv4 / IPv6) connections.\n    \"\"\"\n    if not has_ipv6 \\\n            or not hasattr(_socket, 'IPPROTO_IPV6') \\\n            or not hasattr(_socket, 'IPV6_V6ONLY'):\n        return False\n    try:\n        with socket(AF_INET6, SOCK_STREAM) as sock:\n            sock.setsockopt(IPPROTO_IPV6, IPV6_V6ONLY, 0)\n            return True\n    except error:\n        return False\n\n\ndef create_server(address, *, family=AF_INET, backlog=None, reuse_port=False,\n                  dualstack_ipv6=False):\n    \"\"\"Convenience function which creates a SOCK_STREAM type socket\n    bound to *address* (a 2-tuple (host, port)) and return the socket\n    object.\n\n    *family* should be either AF_INET or AF_INET6.\n    *backlog* is the queue size passed to socket.listen().\n    *reuse_port* dictates whether to use the SO_REUSEPORT socket option.\n    *dualstack_ipv6*: if true and the platform supports it, it will\n    create an AF_INET6 socket able to accept both IPv4 or IPv6\n    connections. When false it will explicitly disable this option on\n    platforms that enable it by default (e.g. Linux).\n\n    >>> with create_server(('', 8000)) as server:\n    ...     while True:\n    ...         conn, addr = server.accept()\n    ...         # handle new connection\n    \"\"\"\n    if reuse_port and not hasattr(_socket, \"SO_REUSEPORT\"):\n        raise ValueError(\"SO_REUSEPORT not supported on this platform\")\n    if dualstack_ipv6:\n        if not has_dualstack_ipv6():\n            raise ValueError(\"dualstack_ipv6 not supported on this platform\")\n        if family != AF_INET6:\n            raise ValueError(\"dualstack_ipv6 requires AF_INET6 family\")\n    sock = socket(family, SOCK_STREAM)\n    try:\n        # Note about Windows. We don't set SO_REUSEADDR because:\n        # 1) It's unnecessary: bind() will succeed even in case of a\n        # previous closed socket on the same address and still in\n        # TIME_WAIT state.\n        # 2) If set, another socket is free to bind() on the same\n        # address, effectively preventing this one from accepting\n        # connections. Also, it may set the process in a state where\n        # it'll no longer respond to any signals or graceful kills.\n        # See: msdn2.microsoft.com/en-us/library/ms740621(VS.85).aspx\n        if os.name not in ('nt', 'cygwin') and \\\n                hasattr(_socket, 'SO_REUSEADDR'):\n            try:\n                sock.setsockopt(SOL_SOCKET, SO_REUSEADDR, 1)\n            except error:\n                # Fail later on bind(), for platforms which may not\n                # support this option.\n                pass\n        if reuse_port:\n            sock.setsockopt(SOL_SOCKET, SO_REUSEPORT, 1)\n        if has_ipv6 and family == AF_INET6:\n            if dualstack_ipv6:\n                sock.setsockopt(IPPROTO_IPV6, IPV6_V6ONLY, 0)\n            elif hasattr(_socket, \"IPV6_V6ONLY\") and \\\n                    hasattr(_socket, \"IPPROTO_IPV6\"):\n                sock.setsockopt(IPPROTO_IPV6, IPV6_V6ONLY, 1)\n        try:\n            sock.bind(address)\n        except error as err:\n            msg = '%s (while attempting to bind on address %r)' % \\\n                (err.strerror, address)\n            raise error(err.errno, msg) from None\n        if backlog is None:\n            sock.listen()\n        else:\n            sock.listen(backlog)\n        return sock\n    except error:\n        sock.close()\n        raise\n\n\ndef getaddrinfo(host, port, family=0, type=0, proto=0, flags=0):\n    \"\"\"Resolve host and port into list of address info entries.\n\n    Translate the host/port argument into a sequence of 5-tuples that contain\n    all the necessary arguments for creating a socket connected to that service.\n    host is a domain name, a string representation of an IPv4/v6 address or\n    None. port is a string service name such as 'http', a numeric port number or\n    None. By passing None as the value of host and port, you can pass NULL to\n    the underlying C API.\n\n    The family, type and proto arguments can be optionally specified in order to\n    narrow the list of addresses returned. Passing zero as a value for each of\n    these arguments selects the full range of results.\n    \"\"\"\n    # We override this function since we want to translate the numeric family\n    # and socket type values to enum constants.\n    addrlist = []\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n        af, socktype, proto, canonname, sa = res\n        addrlist.append((_intenum_converter(af, AddressFamily),\n                         _intenum_converter(socktype, SocketKind),\n                         proto, canonname, sa))\n    return addrlist\n", 960], "D:\\Program\\anaconda3\\lib\\enum.py": ["import sys\nfrom types import MappingProxyType, DynamicClassAttribute\n\n\n__all__ = [\n        'EnumMeta',\n        'Enum', 'IntEnum', 'Flag', 'IntFlag',\n        'auto', 'unique',\n        ]\n\n\ndef _is_descriptor(obj):\n    \"\"\"\n    Returns True if obj is a descriptor, False otherwise.\n    \"\"\"\n    return (\n            hasattr(obj, '__get__') or\n            hasattr(obj, '__set__') or\n            hasattr(obj, '__delete__')\n            )\n\ndef _is_dunder(name):\n    \"\"\"\n    Returns True if a __dunder__ name, False otherwise.\n    \"\"\"\n    return (\n            len(name) > 4 and\n            name[:2] == name[-2:] == '__' and\n            name[2] != '_' and\n            name[-3] != '_'\n            )\n\ndef _is_sunder(name):\n    \"\"\"\n    Returns True if a _sunder_ name, False otherwise.\n    \"\"\"\n    return (\n            len(name) > 2 and\n            name[0] == name[-1] == '_' and\n            name[1:2] != '_' and\n            name[-2:-1] != '_'\n            )\n\ndef _is_private(cls_name, name):\n    # do not use `re` as `re` imports `enum`\n    pattern = '_%s__' % (cls_name, )\n    pat_len = len(pattern)\n    if (\n            len(name) > pat_len\n            and name.startswith(pattern)\n            and name[pat_len:pat_len+1] != ['_']\n            and (name[-1] != '_' or name[-2] != '_')\n        ):\n        return True\n    else:\n        return False\n\ndef _make_class_unpicklable(cls):\n    \"\"\"\n    Make the given class un-picklable.\n    \"\"\"\n    def _break_on_call_reduce(self, proto):\n        raise TypeError('%r cannot be pickled' % self)\n    cls.__reduce_ex__ = _break_on_call_reduce\n    cls.__module__ = '<unknown>'\n\n_auto_null = object()\nclass auto:\n    \"\"\"\n    Instances are replaced with an appropriate value in Enum class suites.\n    \"\"\"\n    value = _auto_null\n\n\nclass _EnumDict(dict):\n    \"\"\"\n    Track enum member order and ensure member names are not reused.\n\n    EnumMeta will use the names found in self._member_names as the\n    enumeration member names.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self._member_names = []\n        self._last_values = []\n        self._ignore = []\n        self._auto_called = False\n\n    def __setitem__(self, key, value):\n        \"\"\"\n        Changes anything not dundered or not a descriptor.\n\n        If an enum member name is used twice, an error is raised; duplicate\n        values are not checked for.\n\n        Single underscore (sunder) names are reserved.\n        \"\"\"\n        if _is_private(self._cls_name, key):\n            import warnings\n            warnings.warn(\n                    \"private variables, such as %r, will be normal attributes in 3.11\"\n                        % (key, ),\n                    DeprecationWarning,\n                    stacklevel=2,\n                    )\n        if _is_sunder(key):\n            if key not in (\n                    '_order_', '_create_pseudo_member_',\n                    '_generate_next_value_', '_missing_', '_ignore_',\n                    ):\n                raise ValueError('_names_ are reserved for future Enum use')\n            if key == '_generate_next_value_':\n                # check if members already defined as auto()\n                if self._auto_called:\n                    raise TypeError(\"_generate_next_value_ must be defined before members\")\n                setattr(self, '_generate_next_value', value)\n            elif key == '_ignore_':\n                if isinstance(value, str):\n                    value = value.replace(',',' ').split()\n                else:\n                    value = list(value)\n                self._ignore = value\n                already = set(value) & set(self._member_names)\n                if already:\n                    raise ValueError(\n                            '_ignore_ cannot specify already set names: %r'\n                            % (already, )\n                            )\n        elif _is_dunder(key):\n            if key == '__order__':\n                key = '_order_'\n        elif key in self._member_names:\n            # descriptor overwriting an enum?\n            raise TypeError('Attempted to reuse key: %r' % key)\n        elif key in self._ignore:\n            pass\n        elif not _is_descriptor(value):\n            if key in self:\n                # enum overwriting a descriptor?\n                raise TypeError('%r already defined as: %r' % (key, self[key]))\n            if isinstance(value, auto):\n                if value.value == _auto_null:\n                    value.value = self._generate_next_value(\n                            key,\n                            1,\n                            len(self._member_names),\n                            self._last_values[:],\n                            )\n                    self._auto_called = True\n                value = value.value\n            self._member_names.append(key)\n            self._last_values.append(value)\n        super().__setitem__(key, value)\n\n\n# Dummy value for Enum as EnumMeta explicitly checks for it, but of course\n# until EnumMeta finishes running the first time the Enum class doesn't exist.\n# This is also why there are checks in EnumMeta like `if Enum is not None`\nEnum = None\n\nclass EnumMeta(type):\n    \"\"\"\n    Metaclass for Enum\n    \"\"\"\n    @classmethod\n    def __prepare__(metacls, cls, bases, **kwds):\n        # check that previous enum members do not exist\n        metacls._check_for_existing_members(cls, bases)\n        # create the namespace dict\n        enum_dict = _EnumDict()\n        enum_dict._cls_name = cls\n        # inherit previous flags and _generate_next_value_ function\n        member_type, first_enum = metacls._get_mixins_(cls, bases)\n        if first_enum is not None:\n            enum_dict['_generate_next_value_'] = getattr(\n                    first_enum, '_generate_next_value_', None,\n                    )\n        return enum_dict\n\n    def __new__(metacls, cls, bases, classdict, **kwds):\n        # an Enum class is final once enumeration items have been defined; it\n        # cannot be mixed with other types (int, float, etc.) if it has an\n        # inherited __new__ unless a new __new__ is defined (or the resulting\n        # class will fail).\n        #\n        # remove any keys listed in _ignore_\n        classdict.setdefault('_ignore_', []).append('_ignore_')\n        ignore = classdict['_ignore_']\n        for key in ignore:\n            classdict.pop(key, None)\n        member_type, first_enum = metacls._get_mixins_(cls, bases)\n        __new__, save_new, use_args = metacls._find_new_(\n                classdict, member_type, first_enum,\n                )\n\n        # save enum items into separate mapping so they don't get baked into\n        # the new class\n        enum_members = {k: classdict[k] for k in classdict._member_names}\n        for name in classdict._member_names:\n            del classdict[name]\n\n        # adjust the sunders\n        _order_ = classdict.pop('_order_', None)\n\n        # check for illegal enum names (any others?)\n        invalid_names = set(enum_members) & {'mro', ''}\n        if invalid_names:\n            raise ValueError('Invalid enum member name: {0}'.format(\n                ','.join(invalid_names)))\n\n        # create a default docstring if one has not been provided\n        if '__doc__' not in classdict:\n            classdict['__doc__'] = 'An enumeration.'\n\n        enum_class = super().__new__(metacls, cls, bases, classdict, **kwds)\n        enum_class._member_names_ = []               # names in definition order\n        enum_class._member_map_ = {}                 # name->value map\n        enum_class._member_type_ = member_type\n\n        # save DynamicClassAttribute attributes from super classes so we know\n        # if we can take the shortcut of storing members in the class dict\n        dynamic_attributes = {\n                k for c in enum_class.mro()\n                for k, v in c.__dict__.items()\n                if isinstance(v, DynamicClassAttribute)\n                }\n\n        # Reverse value->name map for hashable values.\n        enum_class._value2member_map_ = {}\n\n        # If a custom type is mixed into the Enum, and it does not know how\n        # to pickle itself, pickle.dumps will succeed but pickle.loads will\n        # fail.  Rather than have the error show up later and possibly far\n        # from the source, sabotage the pickle protocol for this class so\n        # that pickle.dumps also fails.\n        #\n        # However, if the new class implements its own __reduce_ex__, do not\n        # sabotage -- it's on them to make sure it works correctly.  We use\n        # __reduce_ex__ instead of any of the others as it is preferred by\n        # pickle over __reduce__, and it handles all pickle protocols.\n        if '__reduce_ex__' not in classdict:\n            if member_type is not object:\n                methods = ('__getnewargs_ex__', '__getnewargs__',\n                        '__reduce_ex__', '__reduce__')\n                if not any(m in member_type.__dict__ for m in methods):\n                    if '__new__' in classdict:\n                        # too late, sabotage\n                        _make_class_unpicklable(enum_class)\n                    else:\n                        # final attempt to verify that pickling would work:\n                        # travel mro until __new__ is found, checking for\n                        # __reduce__ and friends along the way -- if any of them\n                        # are found before/when __new__ is found, pickling should\n                        # work\n                        sabotage = None\n                        for chain in bases:\n                            for base in chain.__mro__:\n                                if base is object:\n                                    continue\n                                elif any(m in base.__dict__ for m in methods):\n                                    # found one, we're good\n                                    sabotage = False\n                                    break\n                                elif '__new__' in base.__dict__:\n                                    # not good\n                                    sabotage = True\n                                    break\n                            if sabotage is not None:\n                                break\n                        if sabotage:\n                            _make_class_unpicklable(enum_class)\n        # instantiate them, checking for duplicates as we go\n        # we instantiate first instead of checking for duplicates first in case\n        # a custom __new__ is doing something funky with the values -- such as\n        # auto-numbering ;)\n        for member_name in classdict._member_names:\n            value = enum_members[member_name]\n            if not isinstance(value, tuple):\n                args = (value, )\n            else:\n                args = value\n            if member_type is tuple:   # special case for tuple enums\n                args = (args, )     # wrap it one more time\n            if not use_args:\n                enum_member = __new__(enum_class)\n                if not hasattr(enum_member, '_value_'):\n                    enum_member._value_ = value\n            else:\n                enum_member = __new__(enum_class, *args)\n                if not hasattr(enum_member, '_value_'):\n                    if member_type is object:\n                        enum_member._value_ = value\n                    else:\n                        enum_member._value_ = member_type(*args)\n            value = enum_member._value_\n            enum_member._name_ = member_name\n            enum_member.__objclass__ = enum_class\n            enum_member.__init__(*args)\n            # If another member with the same value was already defined, the\n            # new member becomes an alias to the existing one.\n            for name, canonical_member in enum_class._member_map_.items():\n                if canonical_member._value_ == enum_member._value_:\n                    enum_member = canonical_member\n                    break\n            else:\n                # Aliases don't appear in member names (only in __members__).\n                enum_class._member_names_.append(member_name)\n            # performance boost for any member that would not shadow\n            # a DynamicClassAttribute\n            if member_name not in dynamic_attributes:\n                setattr(enum_class, member_name, enum_member)\n            # now add to _member_map_\n            enum_class._member_map_[member_name] = enum_member\n            try:\n                # This may fail if value is not hashable. We can't add the value\n                # to the map, and by-value lookups for this value will be\n                # linear.\n                enum_class._value2member_map_[value] = enum_member\n            except TypeError:\n                pass\n\n        # double check that repr and friends are not the mixin's or various\n        # things break (such as pickle)\n        # however, if the method is defined in the Enum itself, don't replace\n        # it\n        for name in ('__repr__', '__str__', '__format__', '__reduce_ex__'):\n            if name in classdict:\n                continue\n            class_method = getattr(enum_class, name)\n            obj_method = getattr(member_type, name, None)\n            enum_method = getattr(first_enum, name, None)\n            if obj_method is not None and obj_method is class_method:\n                setattr(enum_class, name, enum_method)\n\n        # replace any other __new__ with our own (as long as Enum is not None,\n        # anyway) -- again, this is to support pickle\n        if Enum is not None:\n            # if the user defined their own __new__, save it before it gets\n            # clobbered in case they subclass later\n            if save_new:\n                enum_class.__new_member__ = __new__\n            enum_class.__new__ = Enum.__new__\n\n        # py3 support for definition order (helps keep py2/py3 code in sync)\n        if _order_ is not None:\n            if isinstance(_order_, str):\n                _order_ = _order_.replace(',', ' ').split()\n            if _order_ != enum_class._member_names_:\n                raise TypeError('member order does not match _order_')\n\n        return enum_class\n\n    def __bool__(self):\n        \"\"\"\n        classes/types should always be True.\n        \"\"\"\n        return True\n\n    def __call__(cls, value, names=None, *, module=None, qualname=None, type=None, start=1):\n        \"\"\"\n        Either returns an existing member, or creates a new enum class.\n\n        This method is used both when an enum class is given a value to match\n        to an enumeration member (i.e. Color(3)) and for the functional API\n        (i.e. Color = Enum('Color', names='RED GREEN BLUE')).\n\n        When used for the functional API:\n\n        `value` will be the name of the new class.\n\n        `names` should be either a string of white-space/comma delimited names\n        (values will start at `start`), or an iterator/mapping of name, value pairs.\n\n        `module` should be set to the module this class is being created in;\n        if it is not set, an attempt to find that module will be made, but if\n        it fails the class will not be picklable.\n\n        `qualname` should be set to the actual location this class can be found\n        at in its module; by default it is set to the global scope.  If this is\n        not correct, unpickling will fail in some circumstances.\n\n        `type`, if set, will be mixed in as the first base class.\n        \"\"\"\n        if names is None:  # simple value lookup\n            return cls.__new__(cls, value)\n        # otherwise, functional API: we're creating a new Enum type\n        return cls._create_(\n                value,\n                names,\n                module=module,\n                qualname=qualname,\n                type=type,\n                start=start,\n                )\n\n    def __contains__(cls, obj):\n        if not isinstance(obj, Enum):\n            import warnings\n            warnings.warn(\n                    \"in 3.12 __contains__ will no longer raise TypeError, but will return True if\\n\"\n                    \"obj is a member or a member's value\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                    )\n            raise TypeError(\n                \"unsupported operand type(s) for 'in': '%s' and '%s'\" % (\n                    type(obj).__qualname__, cls.__class__.__qualname__))\n        return isinstance(obj, cls) and obj._name_ in cls._member_map_\n\n    def __delattr__(cls, attr):\n        # nicer error message when someone tries to delete an attribute\n        # (see issue19025).\n        if attr in cls._member_map_:\n            raise AttributeError(\"%s: cannot delete Enum member.\" % cls.__name__)\n        super().__delattr__(attr)\n\n    def __dir__(self):\n        return (\n                ['__class__', '__doc__', '__members__', '__module__']\n                + self._member_names_\n                )\n\n    def __getattr__(cls, name):\n        \"\"\"\n        Return the enum member matching `name`\n\n        We use __getattr__ instead of descriptors or inserting into the enum\n        class' __dict__ in order to support `name` and `value` being both\n        properties for enum members (which live in the class' __dict__) and\n        enum members themselves.\n        \"\"\"\n        if _is_dunder(name):\n            raise AttributeError(name)\n        try:\n            return cls._member_map_[name]\n        except KeyError:\n            raise AttributeError(name) from None\n\n    def __getitem__(cls, name):\n        return cls._member_map_[name]\n\n    def __iter__(cls):\n        \"\"\"\n        Returns members in definition order.\n        \"\"\"\n        return (cls._member_map_[name] for name in cls._member_names_)\n\n    def __len__(cls):\n        return len(cls._member_names_)\n\n    @property\n    def __members__(cls):\n        \"\"\"\n        Returns a mapping of member name->value.\n\n        This mapping lists all enum members, including aliases. Note that this\n        is a read-only view of the internal mapping.\n        \"\"\"\n        return MappingProxyType(cls._member_map_)\n\n    def __repr__(cls):\n        return \"<enum %r>\" % cls.__name__\n\n    def __reversed__(cls):\n        \"\"\"\n        Returns members in reverse definition order.\n        \"\"\"\n        return (cls._member_map_[name] for name in reversed(cls._member_names_))\n\n    def __setattr__(cls, name, value):\n        \"\"\"\n        Block attempts to reassign Enum members.\n\n        A simple assignment to the class namespace only changes one of the\n        several possible ways to get an Enum member from the Enum class,\n        resulting in an inconsistent Enumeration.\n        \"\"\"\n        member_map = cls.__dict__.get('_member_map_', {})\n        if name in member_map:\n            raise AttributeError('Cannot reassign members.')\n        super().__setattr__(name, value)\n\n    def _create_(cls, class_name, names, *, module=None, qualname=None, type=None, start=1):\n        \"\"\"\n        Convenience method to create a new Enum class.\n\n        `names` can be:\n\n        * A string containing member names, separated either with spaces or\n          commas.  Values are incremented by 1 from `start`.\n        * An iterable of member names.  Values are incremented by 1 from `start`.\n        * An iterable of (member name, value) pairs.\n        * A mapping of member name -> value pairs.\n        \"\"\"\n        metacls = cls.__class__\n        bases = (cls, ) if type is None else (type, cls)\n        _, first_enum = cls._get_mixins_(cls, bases)\n        classdict = metacls.__prepare__(class_name, bases)\n\n        # special processing needed for names?\n        if isinstance(names, str):\n            names = names.replace(',', ' ').split()\n        if isinstance(names, (tuple, list)) and names and isinstance(names[0], str):\n            original_names, names = names, []\n            last_values = []\n            for count, name in enumerate(original_names):\n                value = first_enum._generate_next_value_(name, start, count, last_values[:])\n                last_values.append(value)\n                names.append((name, value))\n\n        # Here, names is either an iterable of (name, value) or a mapping.\n        for item in names:\n            if isinstance(item, str):\n                member_name, member_value = item, names[item]\n            else:\n                member_name, member_value = item\n            classdict[member_name] = member_value\n        enum_class = metacls.__new__(metacls, class_name, bases, classdict)\n\n        # TODO: replace the frame hack if a blessed way to know the calling\n        # module is ever developed\n        if module is None:\n            try:\n                module = sys._getframe(2).f_globals['__name__']\n            except (AttributeError, ValueError, KeyError):\n                pass\n        if module is None:\n            _make_class_unpicklable(enum_class)\n        else:\n            enum_class.__module__ = module\n        if qualname is not None:\n            enum_class.__qualname__ = qualname\n\n        return enum_class\n\n    def _convert_(cls, name, module, filter, source=None):\n        \"\"\"\n        Create a new Enum subclass that replaces a collection of global constants\n        \"\"\"\n        # convert all constants from source (or module) that pass filter() to\n        # a new Enum called name, and export the enum and its members back to\n        # module;\n        # also, replace the __reduce_ex__ method so unpickling works in\n        # previous Python versions\n        module_globals = vars(sys.modules[module])\n        if source:\n            source = vars(source)\n        else:\n            source = module_globals\n        # _value2member_map_ is populated in the same order every time\n        # for a consistent reverse mapping of number to name when there\n        # are multiple names for the same number.\n        members = [\n                (name, value)\n                for name, value in source.items()\n                if filter(name)]\n        try:\n            # sort by value\n            members.sort(key=lambda t: (t[1], t[0]))\n        except TypeError:\n            # unless some values aren't comparable, in which case sort by name\n            members.sort(key=lambda t: t[0])\n        cls = cls(name, members, module=module)\n        cls.__reduce_ex__ = _reduce_ex_by_name\n        module_globals.update(cls.__members__)\n        module_globals[name] = cls\n        return cls\n\n    @staticmethod\n    def _check_for_existing_members(class_name, bases):\n        for chain in bases:\n            for base in chain.__mro__:\n                if issubclass(base, Enum) and base._member_names_:\n                    raise TypeError(\n                            \"%s: cannot extend enumeration %r\"\n                            % (class_name, base.__name__)\n                            )\n\n    @staticmethod\n    def _get_mixins_(class_name, bases):\n        \"\"\"\n        Returns the type for creating enum members, and the first inherited\n        enum class.\n\n        bases: the tuple of bases that was given to __new__\n        \"\"\"\n        if not bases:\n            return object, Enum\n\n        def _find_data_type(bases):\n            data_types = set()\n            for chain in bases:\n                candidate = None\n                for base in chain.__mro__:\n                    if base is object:\n                        continue\n                    elif issubclass(base, Enum):\n                        if base._member_type_ is not object:\n                            data_types.add(base._member_type_)\n                            break\n                    elif '__new__' in base.__dict__:\n                        if issubclass(base, Enum):\n                            continue\n                        data_types.add(candidate or base)\n                        break\n                    else:\n                        candidate = candidate or base\n            if len(data_types) > 1:\n                raise TypeError('%r: too many data types: %r' % (class_name, data_types))\n            elif data_types:\n                return data_types.pop()\n            else:\n                return None\n\n        # ensure final parent class is an Enum derivative, find any concrete\n        # data type, and check that Enum has no members\n        first_enum = bases[-1]\n        if not issubclass(first_enum, Enum):\n            raise TypeError(\"new enumerations should be created as \"\n                    \"`EnumName([mixin_type, ...] [data_type,] enum_type)`\")\n        member_type = _find_data_type(bases) or object\n        if first_enum._member_names_:\n            raise TypeError(\"Cannot extend enumerations\")\n        return member_type, first_enum\n\n    @staticmethod\n    def _find_new_(classdict, member_type, first_enum):\n        \"\"\"\n        Returns the __new__ to be used for creating the enum members.\n\n        classdict: the class dictionary given to __new__\n        member_type: the data type whose __new__ will be used by default\n        first_enum: enumeration to check for an overriding __new__\n        \"\"\"\n        # now find the correct __new__, checking to see of one was defined\n        # by the user; also check earlier enum classes in case a __new__ was\n        # saved as __new_member__\n        __new__ = classdict.get('__new__', None)\n\n        # should __new__ be saved as __new_member__ later?\n        save_new = __new__ is not None\n\n        if __new__ is None:\n            # check all possibles for __new_member__ before falling back to\n            # __new__\n            for method in ('__new_member__', '__new__'):\n                for possible in (member_type, first_enum):\n                    target = getattr(possible, method, None)\n                    if target not in {\n                            None,\n                            None.__new__,\n                            object.__new__,\n                            Enum.__new__,\n                            }:\n                        __new__ = target\n                        break\n                if __new__ is not None:\n                    break\n            else:\n                __new__ = object.__new__\n\n        # if a non-object.__new__ is used then whatever value/tuple was\n        # assigned to the enum member name will be passed to __new__ and to the\n        # new enum member's __init__\n        if __new__ is object.__new__:\n            use_args = False\n        else:\n            use_args = True\n        return __new__, save_new, use_args\n\n\nclass Enum(metaclass=EnumMeta):\n    \"\"\"\n    Generic enumeration.\n\n    Derive from this class to define new enumerations.\n    \"\"\"\n    def __new__(cls, value):\n        # all enum instances are actually created during class construction\n        # without calling this method; this method is called by the metaclass'\n        # __call__ (i.e. Color(3) ), and by pickle\n        if type(value) is cls:\n            # For lookups like Color(Color.RED)\n            return value\n        # by-value search for a matching enum member\n        # see if it's in the reverse mapping (for hashable values)\n        try:\n            return cls._value2member_map_[value]\n        except KeyError:\n            # Not found, no need to do long O(n) search\n            pass\n        except TypeError:\n            # not there, now do long search -- O(n) behavior\n            for member in cls._member_map_.values():\n                if member._value_ == value:\n                    return member\n        # still not found -- try _missing_ hook\n        try:\n            exc = None\n            result = cls._missing_(value)\n        except Exception as e:\n            exc = e\n            result = None\n        try:\n            if isinstance(result, cls):\n                return result\n            else:\n                ve_exc = ValueError(\"%r is not a valid %s\" % (value, cls.__qualname__))\n                if result is None and exc is None:\n                    raise ve_exc\n                elif exc is None:\n                    exc = TypeError(\n                            'error in %s._missing_: returned %r instead of None or a valid member'\n                            % (cls.__name__, result)\n                            )\n                if not isinstance(exc, ValueError):\n                    exc.__context__ = ve_exc\n                raise exc\n        finally:\n            # ensure all variables that could hold an exception are destroyed\n            exc = None\n            ve_exc = None\n\n    def _generate_next_value_(name, start, count, last_values):\n        \"\"\"\n        Generate the next value when not given.\n\n        name: the name of the member\n        start: the initial start value or None\n        count: the number of existing members\n        last_value: the last value assigned or None\n        \"\"\"\n        for last_value in reversed(last_values):\n            try:\n                return last_value + 1\n            except TypeError:\n                pass\n        else:\n            return start\n\n    @classmethod\n    def _missing_(cls, value):\n        return None\n\n    def __repr__(self):\n        return \"<%s.%s: %r>\" % (\n                self.__class__.__name__, self._name_, self._value_)\n\n    def __str__(self):\n        return \"%s.%s\" % (self.__class__.__name__, self._name_)\n\n    def __dir__(self):\n        \"\"\"\n        Returns all members and all public methods\n        \"\"\"\n        added_behavior = [\n                m\n                for cls in self.__class__.mro()\n                for m in cls.__dict__\n                if m[0] != '_' and m not in self._member_map_\n                ] + [m for m in self.__dict__ if m[0] != '_']\n        return (['__class__', '__doc__', '__module__'] + added_behavior)\n\n    def __format__(self, format_spec):\n        \"\"\"\n        Returns format using actual value type unless __str__ has been overridden.\n        \"\"\"\n        # mixed-in Enums should use the mixed-in type's __format__, otherwise\n        # we can get strange results with the Enum name showing up instead of\n        # the value\n\n        # pure Enum branch, or branch with __str__ explicitly overridden\n        str_overridden = type(self).__str__ not in (Enum.__str__, Flag.__str__)\n        if self._member_type_ is object or str_overridden:\n            cls = str\n            val = str(self)\n        # mix-in branch\n        else:\n            cls = self._member_type_\n            val = self._value_\n        return cls.__format__(val, format_spec)\n\n    def __hash__(self):\n        return hash(self._name_)\n\n    def __reduce_ex__(self, proto):\n        return self.__class__, (self._value_, )\n\n    # DynamicClassAttribute is used to provide access to the `name` and\n    # `value` properties of enum members while keeping some measure of\n    # protection from modification, while still allowing for an enumeration\n    # to have members named `name` and `value`.  This works because enumeration\n    # members are not set directly on the enum class -- __getattr__ is\n    # used to look them up.\n\n    @DynamicClassAttribute\n    def name(self):\n        \"\"\"The name of the Enum member.\"\"\"\n        return self._name_\n\n    @DynamicClassAttribute\n    def value(self):\n        \"\"\"The value of the Enum member.\"\"\"\n        return self._value_\n\n\nclass IntEnum(int, Enum):\n    \"\"\"Enum where members are also (and must be) ints\"\"\"\n\n\ndef _reduce_ex_by_name(self, proto):\n    return self.name\n\nclass Flag(Enum):\n    \"\"\"\n    Support for flags\n    \"\"\"\n\n    def _generate_next_value_(name, start, count, last_values):\n        \"\"\"\n        Generate the next value when not given.\n\n        name: the name of the member\n        start: the initial start value or None\n        count: the number of existing members\n        last_value: the last value assigned or None\n        \"\"\"\n        if not count:\n            return start if start is not None else 1\n        for last_value in reversed(last_values):\n            try:\n                high_bit = _high_bit(last_value)\n                break\n            except Exception:\n                raise TypeError('Invalid Flag value: %r' % last_value) from None\n        return 2 ** (high_bit+1)\n\n    @classmethod\n    def _missing_(cls, value):\n        \"\"\"\n        Returns member (possibly creating it) if one can be found for value.\n        \"\"\"\n        original_value = value\n        if value < 0:\n            value = ~value\n        possible_member = cls._create_pseudo_member_(value)\n        if original_value < 0:\n            possible_member = ~possible_member\n        return possible_member\n\n    @classmethod\n    def _create_pseudo_member_(cls, value):\n        \"\"\"\n        Create a composite member iff value contains only members.\n        \"\"\"\n        pseudo_member = cls._value2member_map_.get(value, None)\n        if pseudo_member is None:\n            # verify all bits are accounted for\n            _, extra_flags = _decompose(cls, value)\n            if extra_flags:\n                raise ValueError(\"%r is not a valid %s\" % (value, cls.__qualname__))\n            # construct a singleton enum pseudo-member\n            pseudo_member = object.__new__(cls)\n            pseudo_member._name_ = None\n            pseudo_member._value_ = value\n            # use setdefault in case another thread already created a composite\n            # with this value\n            pseudo_member = cls._value2member_map_.setdefault(value, pseudo_member)\n        return pseudo_member\n\n    def __contains__(self, other):\n        \"\"\"\n        Returns True if self has at least the same flags set as other.\n        \"\"\"\n        if not isinstance(other, self.__class__):\n            raise TypeError(\n                \"unsupported operand type(s) for 'in': '%s' and '%s'\" % (\n                    type(other).__qualname__, self.__class__.__qualname__))\n        return other._value_ & self._value_ == other._value_\n\n    def __repr__(self):\n        cls = self.__class__\n        if self._name_ is not None:\n            return '<%s.%s: %r>' % (cls.__name__, self._name_, self._value_)\n        members, uncovered = _decompose(cls, self._value_)\n        return '<%s.%s: %r>' % (\n                cls.__name__,\n                '|'.join([str(m._name_ or m._value_) for m in members]),\n                self._value_,\n                )\n\n    def __str__(self):\n        cls = self.__class__\n        if self._name_ is not None:\n            return '%s.%s' % (cls.__name__, self._name_)\n        members, uncovered = _decompose(cls, self._value_)\n        if len(members) == 1 and members[0]._name_ is None:\n            return '%s.%r' % (cls.__name__, members[0]._value_)\n        else:\n            return '%s.%s' % (\n                    cls.__name__,\n                    '|'.join([str(m._name_ or m._value_) for m in members]),\n                    )\n\n    def __bool__(self):\n        return bool(self._value_)\n\n    def __or__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return self.__class__(self._value_ | other._value_)\n\n    def __and__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return self.__class__(self._value_ & other._value_)\n\n    def __xor__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return self.__class__(self._value_ ^ other._value_)\n\n    def __invert__(self):\n        members, uncovered = _decompose(self.__class__, self._value_)\n        inverted = self.__class__(0)\n        for m in self.__class__:\n            if m not in members and not (m._value_ & self._value_):\n                inverted = inverted | m\n        return self.__class__(inverted)\n\n\nclass IntFlag(int, Flag):\n    \"\"\"\n    Support for integer-based Flags\n    \"\"\"\n\n    @classmethod\n    def _missing_(cls, value):\n        \"\"\"\n        Returns member (possibly creating it) if one can be found for value.\n        \"\"\"\n        if not isinstance(value, int):\n            raise ValueError(\"%r is not a valid %s\" % (value, cls.__qualname__))\n        new_member = cls._create_pseudo_member_(value)\n        return new_member\n\n    @classmethod\n    def _create_pseudo_member_(cls, value):\n        \"\"\"\n        Create a composite member iff value contains only members.\n        \"\"\"\n        pseudo_member = cls._value2member_map_.get(value, None)\n        if pseudo_member is None:\n            need_to_create = [value]\n            # get unaccounted for bits\n            _, extra_flags = _decompose(cls, value)\n            # timer = 10\n            while extra_flags:\n                # timer -= 1\n                bit = _high_bit(extra_flags)\n                flag_value = 2 ** bit\n                if (flag_value not in cls._value2member_map_ and\n                        flag_value not in need_to_create\n                        ):\n                    need_to_create.append(flag_value)\n                if extra_flags == -flag_value:\n                    extra_flags = 0\n                else:\n                    extra_flags ^= flag_value\n            for value in reversed(need_to_create):\n                # construct singleton pseudo-members\n                pseudo_member = int.__new__(cls, value)\n                pseudo_member._name_ = None\n                pseudo_member._value_ = value\n                # use setdefault in case another thread already created a composite\n                # with this value\n                pseudo_member = cls._value2member_map_.setdefault(value, pseudo_member)\n        return pseudo_member\n\n    def __or__(self, other):\n        if not isinstance(other, (self.__class__, int)):\n            return NotImplemented\n        result = self.__class__(self._value_ | self.__class__(other)._value_)\n        return result\n\n    def __and__(self, other):\n        if not isinstance(other, (self.__class__, int)):\n            return NotImplemented\n        return self.__class__(self._value_ & self.__class__(other)._value_)\n\n    def __xor__(self, other):\n        if not isinstance(other, (self.__class__, int)):\n            return NotImplemented\n        return self.__class__(self._value_ ^ self.__class__(other)._value_)\n\n    __ror__ = __or__\n    __rand__ = __and__\n    __rxor__ = __xor__\n\n    def __invert__(self):\n        result = self.__class__(~self._value_)\n        return result\n\n\ndef _high_bit(value):\n    \"\"\"\n    returns index of highest bit, or -1 if value is zero or negative\n    \"\"\"\n    return value.bit_length() - 1\n\ndef unique(enumeration):\n    \"\"\"\n    Class decorator for enumerations ensuring unique member values.\n    \"\"\"\n    duplicates = []\n    for name, member in enumeration.__members__.items():\n        if name != member.name:\n            duplicates.append((name, member.name))\n    if duplicates:\n        alias_details = ', '.join(\n                [\"%s -> %s\" % (alias, name) for (alias, name) in duplicates])\n        raise ValueError('duplicate values found in %r: %s' %\n                (enumeration, alias_details))\n    return enumeration\n\ndef _decompose(flag, value):\n    \"\"\"\n    Extract all members from the value.\n    \"\"\"\n    # _decompose is only called if the value is not named\n    not_covered = value\n    negative = value < 0\n    members = []\n    for member in flag:\n        member_value = member.value\n        if member_value and member_value & value == member_value:\n            members.append(member)\n            not_covered &= ~member_value\n    if not negative:\n        tmp = not_covered\n        while tmp:\n            flag_value = 2 ** _high_bit(tmp)\n            if flag_value in flag._value2member_map_:\n                members.append(flag._value2member_map_[flag_value])\n                not_covered &= ~flag_value\n            tmp &= ~flag_value\n    if not members and value in flag._value2member_map_:\n        members.append(flag._value2member_map_[value])\n    members.sort(key=lambda m: m._value_, reverse=True)\n    if len(members) > 1 and members[0].value == value:\n        # we have the breakdown, don't need the value member itself\n        members.pop(0)\n    return members, not_covered\n", 1053], "D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py": ["\"\"\"Event loop using a proactor and related classes.\n\nA proactor is a \"notify-on-completion\" multiplexer.  Currently a\nproactor is only implemented on Windows with IOCP.\n\"\"\"\n\n__all__ = 'BaseProactorEventLoop',\n\nimport io\nimport os\nimport socket\nimport warnings\nimport signal\nimport threading\nimport collections\n\nfrom . import base_events\nfrom . import constants\nfrom . import futures\nfrom . import exceptions\nfrom . import protocols\nfrom . import sslproto\nfrom . import transports\nfrom . import trsock\nfrom .log import logger\n\n\ndef _set_socket_extra(transport, sock):\n    transport._extra['socket'] = trsock.TransportSocket(sock)\n\n    try:\n        transport._extra['sockname'] = sock.getsockname()\n    except socket.error:\n        if transport._loop.get_debug():\n            logger.warning(\n                \"getsockname() failed on %r\", sock, exc_info=True)\n\n    if 'peername' not in transport._extra:\n        try:\n            transport._extra['peername'] = sock.getpeername()\n        except socket.error:\n            # UDP sockets may not have a peer name\n            transport._extra['peername'] = None\n\n\nclass _ProactorBasePipeTransport(transports._FlowControlMixin,\n                                 transports.BaseTransport):\n    \"\"\"Base class for pipe and socket transports.\"\"\"\n\n    def __init__(self, loop, sock, protocol, waiter=None,\n                 extra=None, server=None):\n        super().__init__(extra, loop)\n        self._set_extra(sock)\n        self._sock = sock\n        self.set_protocol(protocol)\n        self._server = server\n        self._buffer = None  # None or bytearray.\n        self._read_fut = None\n        self._write_fut = None\n        self._pending_write = 0\n        self._conn_lost = 0\n        self._closing = False  # Set when close() called.\n        self._called_connection_lost = False\n        self._eof_written = False\n        if self._server is not None:\n            self._server._attach()\n        self._loop.call_soon(self._protocol.connection_made, self)\n        if waiter is not None:\n            # only wake up the waiter when connection_made() has been called\n            self._loop.call_soon(futures._set_result_unless_cancelled,\n                                 waiter, None)\n\n    def __repr__(self):\n        info = [self.__class__.__name__]\n        if self._sock is None:\n            info.append('closed')\n        elif self._closing:\n            info.append('closing')\n        if self._sock is not None:\n            info.append(f'fd={self._sock.fileno()}')\n        if self._read_fut is not None:\n            info.append(f'read={self._read_fut!r}')\n        if self._write_fut is not None:\n            info.append(f'write={self._write_fut!r}')\n        if self._buffer:\n            info.append(f'write_bufsize={len(self._buffer)}')\n        if self._eof_written:\n            info.append('EOF written')\n        return '<{}>'.format(' '.join(info))\n\n    def _set_extra(self, sock):\n        self._extra['pipe'] = sock\n\n    def set_protocol(self, protocol):\n        self._protocol = protocol\n\n    def get_protocol(self):\n        return self._protocol\n\n    def is_closing(self):\n        return self._closing\n\n    def close(self):\n        if self._closing:\n            return\n        self._closing = True\n        self._conn_lost += 1\n        if not self._buffer and self._write_fut is None:\n            self._loop.call_soon(self._call_connection_lost, None)\n        if self._read_fut is not None:\n            self._read_fut.cancel()\n            self._read_fut = None\n\n    def __del__(self, _warn=warnings.warn):\n        if self._sock is not None:\n            _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n            self._sock.close()\n\n    def _fatal_error(self, exc, message='Fatal error on pipe transport'):\n        try:\n            if isinstance(exc, OSError):\n                if self._loop.get_debug():\n                    logger.debug(\"%r: %s\", self, message, exc_info=True)\n            else:\n                self._loop.call_exception_handler({\n                    'message': message,\n                    'exception': exc,\n                    'transport': self,\n                    'protocol': self._protocol,\n                })\n        finally:\n            self._force_close(exc)\n\n    def _force_close(self, exc):\n        if self._empty_waiter is not None and not self._empty_waiter.done():\n            if exc is None:\n                self._empty_waiter.set_result(None)\n            else:\n                self._empty_waiter.set_exception(exc)\n        if self._closing and self._called_connection_lost:\n            return\n        self._closing = True\n        self._conn_lost += 1\n        if self._write_fut:\n            self._write_fut.cancel()\n            self._write_fut = None\n        if self._read_fut:\n            self._read_fut.cancel()\n            self._read_fut = None\n        self._pending_write = 0\n        self._buffer = None\n        self._loop.call_soon(self._call_connection_lost, exc)\n\n    def _call_connection_lost(self, exc):\n        if self._called_connection_lost:\n            return\n        try:\n            self._protocol.connection_lost(exc)\n        finally:\n            # XXX If there is a pending overlapped read on the other\n            # end then it may fail with ERROR_NETNAME_DELETED if we\n            # just close our end.  First calling shutdown() seems to\n            # cure it, but maybe using DisconnectEx() would be better.\n            if hasattr(self._sock, 'shutdown') and self._sock.fileno() != -1:\n                self._sock.shutdown(socket.SHUT_RDWR)\n            self._sock.close()\n            self._sock = None\n            server = self._server\n            if server is not None:\n                server._detach()\n                self._server = None\n            self._called_connection_lost = True\n\n    def get_write_buffer_size(self):\n        size = self._pending_write\n        if self._buffer is not None:\n            size += len(self._buffer)\n        return size\n\n\nclass _ProactorReadPipeTransport(_ProactorBasePipeTransport,\n                                 transports.ReadTransport):\n    \"\"\"Transport for read pipes.\"\"\"\n\n    def __init__(self, loop, sock, protocol, waiter=None,\n                 extra=None, server=None, buffer_size=65536):\n        self._pending_data_length = -1\n        self._paused = True\n        super().__init__(loop, sock, protocol, waiter, extra, server)\n\n        self._data = bytearray(buffer_size)\n        self._loop.call_soon(self._loop_reading)\n        self._paused = False\n\n    def is_reading(self):\n        return not self._paused and not self._closing\n\n    def pause_reading(self):\n        if self._closing or self._paused:\n            return\n        self._paused = True\n\n        # bpo-33694: Don't cancel self._read_fut because cancelling an\n        # overlapped WSASend() loss silently data with the current proactor\n        # implementation.\n        #\n        # If CancelIoEx() fails with ERROR_NOT_FOUND, it means that WSASend()\n        # completed (even if HasOverlappedIoCompleted() returns 0), but\n        # Overlapped.cancel() currently silently ignores the ERROR_NOT_FOUND\n        # error. Once the overlapped is ignored, the IOCP loop will ignores the\n        # completion I/O event and so not read the result of the overlapped\n        # WSARecv().\n\n        if self._loop.get_debug():\n            logger.debug(\"%r pauses reading\", self)\n\n    def resume_reading(self):\n        if self._closing or not self._paused:\n            return\n\n        self._paused = False\n        if self._read_fut is None:\n            self._loop.call_soon(self._loop_reading, None)\n\n        length = self._pending_data_length\n        self._pending_data_length = -1\n        if length > -1:\n            # Call the protocol methode after calling _loop_reading(),\n            # since the protocol can decide to pause reading again.\n            self._loop.call_soon(self._data_received, self._data[:length], length)\n\n        if self._loop.get_debug():\n            logger.debug(\"%r resumes reading\", self)\n\n    def _eof_received(self):\n        if self._loop.get_debug():\n            logger.debug(\"%r received EOF\", self)\n\n        try:\n            keep_open = self._protocol.eof_received()\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self._fatal_error(\n                exc, 'Fatal error: protocol.eof_received() call failed.')\n            return\n\n        if not keep_open:\n            self.close()\n\n    def _data_received(self, data, length):\n        if self._paused:\n            # Don't call any protocol method while reading is paused.\n            # The protocol will be called on resume_reading().\n            assert self._pending_data_length == -1\n            self._pending_data_length = length\n            return\n\n        if length == 0:\n            self._eof_received()\n            return\n\n        if isinstance(self._protocol, protocols.BufferedProtocol):\n            try:\n                protocols._feed_data_to_buffered_proto(self._protocol, data)\n            except (SystemExit, KeyboardInterrupt):\n                raise\n            except BaseException as exc:\n                self._fatal_error(exc,\n                                  'Fatal error: protocol.buffer_updated() '\n                                  'call failed.')\n                return\n        else:\n            self._protocol.data_received(data)\n\n    def _loop_reading(self, fut=None):\n        length = -1\n        data = None\n        try:\n            if fut is not None:\n                assert self._read_fut is fut or (self._read_fut is None and\n                                                 self._closing)\n                self._read_fut = None\n                if fut.done():\n                    # deliver data later in \"finally\" clause\n                    length = fut.result()\n                    if length == 0:\n                        # we got end-of-file so no need to reschedule a new read\n                        return\n\n                    data = self._data[:length]\n                else:\n                    # the future will be replaced by next proactor.recv call\n                    fut.cancel()\n\n            if self._closing:\n                # since close() has been called we ignore any read data\n                return\n\n            # bpo-33694: buffer_updated() has currently no fast path because of\n            # a data loss issue caused by overlapped WSASend() cancellation.\n\n            if not self._paused:\n                # reschedule a new read\n                self._read_fut = self._loop._proactor.recv_into(self._sock, self._data)\n        except ConnectionAbortedError as exc:\n            if not self._closing:\n                self._fatal_error(exc, 'Fatal read error on pipe transport')\n            elif self._loop.get_debug():\n                logger.debug(\"Read error on pipe transport while closing\",\n                             exc_info=True)\n        except ConnectionResetError as exc:\n            self._force_close(exc)\n        except OSError as exc:\n            self._fatal_error(exc, 'Fatal read error on pipe transport')\n        except exceptions.CancelledError:\n            if not self._closing:\n                raise\n        else:\n            if not self._paused:\n                self._read_fut.add_done_callback(self._loop_reading)\n        finally:\n            if length > -1:\n                self._data_received(data, length)\n\n\nclass _ProactorBaseWritePipeTransport(_ProactorBasePipeTransport,\n                                      transports.WriteTransport):\n    \"\"\"Transport for write pipes.\"\"\"\n\n    _start_tls_compatible = True\n\n    def __init__(self, *args, **kw):\n        super().__init__(*args, **kw)\n        self._empty_waiter = None\n\n    def write(self, data):\n        if not isinstance(data, (bytes, bytearray, memoryview)):\n            raise TypeError(\n                f\"data argument must be a bytes-like object, \"\n                f\"not {type(data).__name__}\")\n        if self._eof_written:\n            raise RuntimeError('write_eof() already called')\n        if self._empty_waiter is not None:\n            raise RuntimeError('unable to write; sendfile is in progress')\n\n        if not data:\n            return\n\n        if self._conn_lost:\n            if self._conn_lost >= constants.LOG_THRESHOLD_FOR_CONNLOST_WRITES:\n                logger.warning('socket.send() raised exception.')\n            self._conn_lost += 1\n            return\n\n        # Observable states:\n        # 1. IDLE: _write_fut and _buffer both None\n        # 2. WRITING: _write_fut set; _buffer None\n        # 3. BACKED UP: _write_fut set; _buffer a bytearray\n        # We always copy the data, so the caller can't modify it\n        # while we're still waiting for the I/O to happen.\n        if self._write_fut is None:  # IDLE -> WRITING\n            assert self._buffer is None\n            # Pass a copy, except if it's already immutable.\n            self._loop_writing(data=bytes(data))\n        elif not self._buffer:  # WRITING -> BACKED UP\n            # Make a mutable copy which we can extend.\n            self._buffer = bytearray(data)\n            self._maybe_pause_protocol()\n        else:  # BACKED UP\n            # Append to buffer (also copies).\n            self._buffer.extend(data)\n            self._maybe_pause_protocol()\n\n    def _loop_writing(self, f=None, data=None):\n        try:\n            if f is not None and self._write_fut is None and self._closing:\n                # XXX most likely self._force_close() has been called, and\n                # it has set self._write_fut to None.\n                return\n            assert f is self._write_fut\n            self._write_fut = None\n            self._pending_write = 0\n            if f:\n                f.result()\n            if data is None:\n                data = self._buffer\n                self._buffer = None\n            if not data:\n                if self._closing:\n                    self._loop.call_soon(self._call_connection_lost, None)\n                if self._eof_written:\n                    self._sock.shutdown(socket.SHUT_WR)\n                # Now that we've reduced the buffer size, tell the\n                # protocol to resume writing if it was paused.  Note that\n                # we do this last since the callback is called immediately\n                # and it may add more data to the buffer (even causing the\n                # protocol to be paused again).\n                self._maybe_resume_protocol()\n            else:\n                self._write_fut = self._loop._proactor.send(self._sock, data)\n                if not self._write_fut.done():\n                    assert self._pending_write == 0\n                    self._pending_write = len(data)\n                    self._write_fut.add_done_callback(self._loop_writing)\n                    self._maybe_pause_protocol()\n                else:\n                    self._write_fut.add_done_callback(self._loop_writing)\n            if self._empty_waiter is not None and self._write_fut is None:\n                self._empty_waiter.set_result(None)\n        except ConnectionResetError as exc:\n            self._force_close(exc)\n        except OSError as exc:\n            self._fatal_error(exc, 'Fatal write error on pipe transport')\n\n    def can_write_eof(self):\n        return True\n\n    def write_eof(self):\n        self.close()\n\n    def abort(self):\n        self._force_close(None)\n\n    def _make_empty_waiter(self):\n        if self._empty_waiter is not None:\n            raise RuntimeError(\"Empty waiter is already set\")\n        self._empty_waiter = self._loop.create_future()\n        if self._write_fut is None:\n            self._empty_waiter.set_result(None)\n        return self._empty_waiter\n\n    def _reset_empty_waiter(self):\n        self._empty_waiter = None\n\n\nclass _ProactorWritePipeTransport(_ProactorBaseWritePipeTransport):\n    def __init__(self, *args, **kw):\n        super().__init__(*args, **kw)\n        self._read_fut = self._loop._proactor.recv(self._sock, 16)\n        self._read_fut.add_done_callback(self._pipe_closed)\n\n    def _pipe_closed(self, fut):\n        if fut.cancelled():\n            # the transport has been closed\n            return\n        assert fut.result() == b''\n        if self._closing:\n            assert self._read_fut is None\n            return\n        assert fut is self._read_fut, (fut, self._read_fut)\n        self._read_fut = None\n        if self._write_fut is not None:\n            self._force_close(BrokenPipeError())\n        else:\n            self.close()\n\n\nclass _ProactorDatagramTransport(_ProactorBasePipeTransport,\n                                 transports.DatagramTransport):\n    max_size = 256 * 1024\n    def __init__(self, loop, sock, protocol, address=None,\n                 waiter=None, extra=None):\n        self._address = address\n        self._empty_waiter = None\n        # We don't need to call _protocol.connection_made() since our base\n        # constructor does it for us.\n        super().__init__(loop, sock, protocol, waiter=waiter, extra=extra)\n\n        # The base constructor sets _buffer = None, so we set it here\n        self._buffer = collections.deque()\n        self._loop.call_soon(self._loop_reading)\n\n    def _set_extra(self, sock):\n        _set_socket_extra(self, sock)\n\n    def get_write_buffer_size(self):\n        return sum(len(data) for data, _ in self._buffer)\n\n    def abort(self):\n        self._force_close(None)\n\n    def sendto(self, data, addr=None):\n        if not isinstance(data, (bytes, bytearray, memoryview)):\n            raise TypeError('data argument must be bytes-like object (%r)',\n                            type(data))\n\n        if not data:\n            return\n\n        if self._address is not None and addr not in (None, self._address):\n            raise ValueError(\n                f'Invalid address: must be None or {self._address}')\n\n        if self._conn_lost and self._address:\n            if self._conn_lost >= constants.LOG_THRESHOLD_FOR_CONNLOST_WRITES:\n                logger.warning('socket.sendto() raised exception.')\n            self._conn_lost += 1\n            return\n\n        # Ensure that what we buffer is immutable.\n        self._buffer.append((bytes(data), addr))\n\n        if self._write_fut is None:\n            # No current write operations are active, kick one off\n            self._loop_writing()\n        # else: A write operation is already kicked off\n\n        self._maybe_pause_protocol()\n\n    def _loop_writing(self, fut=None):\n        try:\n            if self._conn_lost:\n                return\n\n            assert fut is self._write_fut\n            self._write_fut = None\n            if fut:\n                # We are in a _loop_writing() done callback, get the result\n                fut.result()\n\n            if not self._buffer or (self._conn_lost and self._address):\n                # The connection has been closed\n                if self._closing:\n                    self._loop.call_soon(self._call_connection_lost, None)\n                return\n\n            data, addr = self._buffer.popleft()\n            if self._address is not None:\n                self._write_fut = self._loop._proactor.send(self._sock,\n                                                            data)\n            else:\n                self._write_fut = self._loop._proactor.sendto(self._sock,\n                                                              data,\n                                                              addr=addr)\n        except OSError as exc:\n            self._protocol.error_received(exc)\n        except Exception as exc:\n            self._fatal_error(exc, 'Fatal write error on datagram transport')\n        else:\n            self._write_fut.add_done_callback(self._loop_writing)\n            self._maybe_resume_protocol()\n\n    def _loop_reading(self, fut=None):\n        data = None\n        try:\n            if self._conn_lost:\n                return\n\n            assert self._read_fut is fut or (self._read_fut is None and\n                                             self._closing)\n\n            self._read_fut = None\n            if fut is not None:\n                res = fut.result()\n\n                if self._closing:\n                    # since close() has been called we ignore any read data\n                    data = None\n                    return\n\n                if self._address is not None:\n                    data, addr = res, self._address\n                else:\n                    data, addr = res\n\n            if self._conn_lost:\n                return\n            if self._address is not None:\n                self._read_fut = self._loop._proactor.recv(self._sock,\n                                                           self.max_size)\n            else:\n                self._read_fut = self._loop._proactor.recvfrom(self._sock,\n                                                               self.max_size)\n        except OSError as exc:\n            self._protocol.error_received(exc)\n        except exceptions.CancelledError:\n            if not self._closing:\n                raise\n        else:\n            if self._read_fut is not None:\n                self._read_fut.add_done_callback(self._loop_reading)\n        finally:\n            if data:\n                self._protocol.datagram_received(data, addr)\n\n\nclass _ProactorDuplexPipeTransport(_ProactorReadPipeTransport,\n                                   _ProactorBaseWritePipeTransport,\n                                   transports.Transport):\n    \"\"\"Transport for duplex pipes.\"\"\"\n\n    def can_write_eof(self):\n        return False\n\n    def write_eof(self):\n        raise NotImplementedError\n\n\nclass _ProactorSocketTransport(_ProactorReadPipeTransport,\n                               _ProactorBaseWritePipeTransport,\n                               transports.Transport):\n    \"\"\"Transport for connected sockets.\"\"\"\n\n    _sendfile_compatible = constants._SendfileMode.TRY_NATIVE\n\n    def __init__(self, loop, sock, protocol, waiter=None,\n                 extra=None, server=None):\n        super().__init__(loop, sock, protocol, waiter, extra, server)\n        base_events._set_nodelay(sock)\n\n    def _set_extra(self, sock):\n        _set_socket_extra(self, sock)\n\n    def can_write_eof(self):\n        return True\n\n    def write_eof(self):\n        if self._closing or self._eof_written:\n            return\n        self._eof_written = True\n        if self._write_fut is None:\n            self._sock.shutdown(socket.SHUT_WR)\n\n\nclass BaseProactorEventLoop(base_events.BaseEventLoop):\n\n    def __init__(self, proactor):\n        super().__init__()\n        logger.debug('Using proactor: %s', proactor.__class__.__name__)\n        self._proactor = proactor\n        self._selector = proactor   # convenient alias\n        self._self_reading_future = None\n        self._accept_futures = {}   # socket file descriptor => Future\n        proactor.set_loop(self)\n        self._make_self_pipe()\n        if threading.current_thread() is threading.main_thread():\n            # wakeup fd can only be installed to a file descriptor from the main thread\n            signal.set_wakeup_fd(self._csock.fileno())\n\n    def _make_socket_transport(self, sock, protocol, waiter=None,\n                               extra=None, server=None):\n        return _ProactorSocketTransport(self, sock, protocol, waiter,\n                                        extra, server)\n\n    def _make_ssl_transport(\n            self, rawsock, protocol, sslcontext, waiter=None,\n            *, server_side=False, server_hostname=None,\n            extra=None, server=None,\n            ssl_handshake_timeout=None):\n        ssl_protocol = sslproto.SSLProtocol(\n                self, protocol, sslcontext, waiter,\n                server_side, server_hostname,\n                ssl_handshake_timeout=ssl_handshake_timeout)\n        _ProactorSocketTransport(self, rawsock, ssl_protocol,\n                                 extra=extra, server=server)\n        return ssl_protocol._app_transport\n\n    def _make_datagram_transport(self, sock, protocol,\n                                 address=None, waiter=None, extra=None):\n        return _ProactorDatagramTransport(self, sock, protocol, address,\n                                          waiter, extra)\n\n    def _make_duplex_pipe_transport(self, sock, protocol, waiter=None,\n                                    extra=None):\n        return _ProactorDuplexPipeTransport(self,\n                                            sock, protocol, waiter, extra)\n\n    def _make_read_pipe_transport(self, sock, protocol, waiter=None,\n                                  extra=None):\n        return _ProactorReadPipeTransport(self, sock, protocol, waiter, extra)\n\n    def _make_write_pipe_transport(self, sock, protocol, waiter=None,\n                                   extra=None):\n        # We want connection_lost() to be called when other end closes\n        return _ProactorWritePipeTransport(self,\n                                           sock, protocol, waiter, extra)\n\n    def close(self):\n        if self.is_running():\n            raise RuntimeError(\"Cannot close a running event loop\")\n        if self.is_closed():\n            return\n\n        if threading.current_thread() is threading.main_thread():\n            signal.set_wakeup_fd(-1)\n        # Call these methods before closing the event loop (before calling\n        # BaseEventLoop.close), because they can schedule callbacks with\n        # call_soon(), which is forbidden when the event loop is closed.\n        self._stop_accept_futures()\n        self._close_self_pipe()\n        self._proactor.close()\n        self._proactor = None\n        self._selector = None\n\n        # Close the event loop\n        super().close()\n\n    async def sock_recv(self, sock, n):\n        return await self._proactor.recv(sock, n)\n\n    async def sock_recv_into(self, sock, buf):\n        return await self._proactor.recv_into(sock, buf)\n\n    async def sock_sendall(self, sock, data):\n        return await self._proactor.send(sock, data)\n\n    async def sock_connect(self, sock, address):\n        return await self._proactor.connect(sock, address)\n\n    async def sock_accept(self, sock):\n        return await self._proactor.accept(sock)\n\n    async def _sock_sendfile_native(self, sock, file, offset, count):\n        try:\n            fileno = file.fileno()\n        except (AttributeError, io.UnsupportedOperation) as err:\n            raise exceptions.SendfileNotAvailableError(\"not a regular file\")\n        try:\n            fsize = os.fstat(fileno).st_size\n        except OSError:\n            raise exceptions.SendfileNotAvailableError(\"not a regular file\")\n        blocksize = count if count else fsize\n        if not blocksize:\n            return 0  # empty file\n\n        blocksize = min(blocksize, 0xffff_ffff)\n        end_pos = min(offset + count, fsize) if count else fsize\n        offset = min(offset, fsize)\n        total_sent = 0\n        try:\n            while True:\n                blocksize = min(end_pos - offset, blocksize)\n                if blocksize <= 0:\n                    return total_sent\n                await self._proactor.sendfile(sock, file, offset, blocksize)\n                offset += blocksize\n                total_sent += blocksize\n        finally:\n            if total_sent > 0:\n                file.seek(offset)\n\n    async def _sendfile_native(self, transp, file, offset, count):\n        resume_reading = transp.is_reading()\n        transp.pause_reading()\n        await transp._make_empty_waiter()\n        try:\n            return await self.sock_sendfile(transp._sock, file, offset, count,\n                                            fallback=False)\n        finally:\n            transp._reset_empty_waiter()\n            if resume_reading:\n                transp.resume_reading()\n\n    def _close_self_pipe(self):\n        if self._self_reading_future is not None:\n            self._self_reading_future.cancel()\n            self._self_reading_future = None\n        self._ssock.close()\n        self._ssock = None\n        self._csock.close()\n        self._csock = None\n        self._internal_fds -= 1\n\n    def _make_self_pipe(self):\n        # A self-socket, really. :-)\n        self._ssock, self._csock = socket.socketpair()\n        self._ssock.setblocking(False)\n        self._csock.setblocking(False)\n        self._internal_fds += 1\n\n    def _loop_self_reading(self, f=None):\n        try:\n            if f is not None:\n                f.result()  # may raise\n            if self._self_reading_future is not f:\n                # When we scheduled this Future, we assigned it to\n                # _self_reading_future. If it's not there now, something has\n                # tried to cancel the loop while this callback was still in the\n                # queue (see windows_events.ProactorEventLoop.run_forever). In\n                # that case stop here instead of continuing to schedule a new\n                # iteration.\n                return\n            f = self._proactor.recv(self._ssock, 4096)\n        except exceptions.CancelledError:\n            # _close_self_pipe() has been called, stop waiting for data\n            return\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            self.call_exception_handler({\n                'message': 'Error on reading from the event loop self pipe',\n                'exception': exc,\n                'loop': self,\n            })\n        else:\n            self._self_reading_future = f\n            f.add_done_callback(self._loop_self_reading)\n\n    def _write_to_self(self):\n        # This may be called from a different thread, possibly after\n        # _close_self_pipe() has been called or even while it is\n        # running.  Guard for self._csock being None or closed.  When\n        # a socket is closed, send() raises OSError (with errno set to\n        # EBADF, but let's not rely on the exact error code).\n        csock = self._csock\n        if csock is None:\n            return\n\n        try:\n            csock.send(b'\\0')\n        except OSError:\n            if self._debug:\n                logger.debug(\"Fail to write a null byte into the \"\n                             \"self-pipe socket\",\n                             exc_info=True)\n\n    def _start_serving(self, protocol_factory, sock,\n                       sslcontext=None, server=None, backlog=100,\n                       ssl_handshake_timeout=None):\n\n        def loop(f=None):\n            try:\n                if f is not None:\n                    conn, addr = f.result()\n                    if self._debug:\n                        logger.debug(\"%r got a new connection from %r: %r\",\n                                     server, addr, conn)\n                    protocol = protocol_factory()\n                    if sslcontext is not None:\n                        self._make_ssl_transport(\n                            conn, protocol, sslcontext, server_side=True,\n                            extra={'peername': addr}, server=server,\n                            ssl_handshake_timeout=ssl_handshake_timeout)\n                    else:\n                        self._make_socket_transport(\n                            conn, protocol,\n                            extra={'peername': addr}, server=server)\n                if self.is_closed():\n                    return\n                f = self._proactor.accept(sock)\n            except OSError as exc:\n                if sock.fileno() != -1:\n                    self.call_exception_handler({\n                        'message': 'Accept failed on a socket',\n                        'exception': exc,\n                        'socket': trsock.TransportSocket(sock),\n                    })\n                    sock.close()\n                elif self._debug:\n                    logger.debug(\"Accept failed on socket %r\",\n                                 sock, exc_info=True)\n            except exceptions.CancelledError:\n                sock.close()\n            else:\n                self._accept_futures[sock.fileno()] = f\n                f.add_done_callback(loop)\n\n        self.call_soon(loop)\n\n    def _process_events(self, event_list):\n        # Events are processed in the IocpProactor._poll() method\n        pass\n\n    def _stop_accept_futures(self):\n        for future in self._accept_futures.values():\n            future.cancel()\n        self._accept_futures.clear()\n\n    def _stop_serving(self, sock):\n        future = self._accept_futures.pop(sock.fileno(), None)\n        if future:\n            future.cancel()\n        self._proactor._stop_serving(sock)\n        sock.close()\n", 875], "D:\\Program\\anaconda3\\lib\\threading.py": ["\"\"\"Thread module emulating a subset of Java's threading model.\"\"\"\n\nimport os as _os\nimport sys as _sys\nimport _thread\nimport functools\n\nfrom time import monotonic as _time\nfrom _weakrefset import WeakSet\nfrom itertools import islice as _islice, count as _count\ntry:\n    from _collections import deque as _deque\nexcept ImportError:\n    from collections import deque as _deque\n\n# Note regarding PEP 8 compliant names\n#  This threading model was originally inspired by Java, and inherited\n# the convention of camelCase function and method names from that\n# language. Those original names are not in any imminent danger of\n# being deprecated (even for Py3k),so this module provides them as an\n# alias for the PEP 8 compliant names\n# Note that using the new PEP 8 compliant names facilitates substitution\n# with the multiprocessing module, which doesn't provide the old\n# Java inspired names.\n\n__all__ = ['get_ident', 'active_count', 'Condition', 'current_thread',\n           'enumerate', 'main_thread', 'TIMEOUT_MAX',\n           'Event', 'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore', 'Thread',\n           'Barrier', 'BrokenBarrierError', 'Timer', 'ThreadError',\n           'setprofile', 'settrace', 'local', 'stack_size',\n           'excepthook', 'ExceptHookArgs', 'gettrace', 'getprofile']\n\n# Rename some stuff so \"from threading import *\" is safe\n_start_new_thread = _thread.start_new_thread\n_allocate_lock = _thread.allocate_lock\n_set_sentinel = _thread._set_sentinel\nget_ident = _thread.get_ident\ntry:\n    get_native_id = _thread.get_native_id\n    _HAVE_THREAD_NATIVE_ID = True\n    __all__.append('get_native_id')\nexcept AttributeError:\n    _HAVE_THREAD_NATIVE_ID = False\nThreadError = _thread.error\ntry:\n    _CRLock = _thread.RLock\nexcept AttributeError:\n    _CRLock = None\nTIMEOUT_MAX = _thread.TIMEOUT_MAX\ndel _thread\n\n\n# Support for profile and trace hooks\n\n_profile_hook = None\n_trace_hook = None\n\ndef setprofile(func):\n    \"\"\"Set a profile function for all threads started from the threading module.\n\n    The func will be passed to sys.setprofile() for each thread, before its\n    run() method is called.\n\n    \"\"\"\n    global _profile_hook\n    _profile_hook = func\n\ndef getprofile():\n    \"\"\"Get the profiler function as set by threading.setprofile().\"\"\"\n    return _profile_hook\n\ndef settrace(func):\n    \"\"\"Set a trace function for all threads started from the threading module.\n\n    The func will be passed to sys.settrace() for each thread, before its run()\n    method is called.\n\n    \"\"\"\n    global _trace_hook\n    _trace_hook = func\n\ndef gettrace():\n    \"\"\"Get the trace function as set by threading.settrace().\"\"\"\n    return _trace_hook\n\n# Synchronization classes\n\nLock = _allocate_lock\n\ndef RLock(*args, **kwargs):\n    \"\"\"Factory function that returns a new reentrant lock.\n\n    A reentrant lock must be released by the thread that acquired it. Once a\n    thread has acquired a reentrant lock, the same thread may acquire it again\n    without blocking; the thread must release it once for each time it has\n    acquired it.\n\n    \"\"\"\n    if _CRLock is None:\n        return _PyRLock(*args, **kwargs)\n    return _CRLock(*args, **kwargs)\n\nclass _RLock:\n    \"\"\"This class implements reentrant lock objects.\n\n    A reentrant lock must be released by the thread that acquired it. Once a\n    thread has acquired a reentrant lock, the same thread may acquire it\n    again without blocking; the thread must release it once for each time it\n    has acquired it.\n\n    \"\"\"\n\n    def __init__(self):\n        self._block = _allocate_lock()\n        self._owner = None\n        self._count = 0\n\n    def __repr__(self):\n        owner = self._owner\n        try:\n            owner = _active[owner].name\n        except KeyError:\n            pass\n        return \"<%s %s.%s object owner=%r count=%d at %s>\" % (\n            \"locked\" if self._block.locked() else \"unlocked\",\n            self.__class__.__module__,\n            self.__class__.__qualname__,\n            owner,\n            self._count,\n            hex(id(self))\n        )\n\n    def _at_fork_reinit(self):\n        self._block._at_fork_reinit()\n        self._owner = None\n        self._count = 0\n\n    def acquire(self, blocking=True, timeout=-1):\n        \"\"\"Acquire a lock, blocking or non-blocking.\n\n        When invoked without arguments: if this thread already owns the lock,\n        increment the recursion level by one, and return immediately. Otherwise,\n        if another thread owns the lock, block until the lock is unlocked. Once\n        the lock is unlocked (not owned by any thread), then grab ownership, set\n        the recursion level to one, and return. If more than one thread is\n        blocked waiting until the lock is unlocked, only one at a time will be\n        able to grab ownership of the lock. There is no return value in this\n        case.\n\n        When invoked with the blocking argument set to true, do the same thing\n        as when called without arguments, and return true.\n\n        When invoked with the blocking argument set to false, do not block. If a\n        call without an argument would block, return false immediately;\n        otherwise, do the same thing as when called without arguments, and\n        return true.\n\n        When invoked with the floating-point timeout argument set to a positive\n        value, block for at most the number of seconds specified by timeout\n        and as long as the lock cannot be acquired.  Return true if the lock has\n        been acquired, false if the timeout has elapsed.\n\n        \"\"\"\n        me = get_ident()\n        if self._owner == me:\n            self._count += 1\n            return 1\n        rc = self._block.acquire(blocking, timeout)\n        if rc:\n            self._owner = me\n            self._count = 1\n        return rc\n\n    __enter__ = acquire\n\n    def release(self):\n        \"\"\"Release a lock, decrementing the recursion level.\n\n        If after the decrement it is zero, reset the lock to unlocked (not owned\n        by any thread), and if any other threads are blocked waiting for the\n        lock to become unlocked, allow exactly one of them to proceed. If after\n        the decrement the recursion level is still nonzero, the lock remains\n        locked and owned by the calling thread.\n\n        Only call this method when the calling thread owns the lock. A\n        RuntimeError is raised if this method is called when the lock is\n        unlocked.\n\n        There is no return value.\n\n        \"\"\"\n        if self._owner != get_ident():\n            raise RuntimeError(\"cannot release un-acquired lock\")\n        self._count = count = self._count - 1\n        if not count:\n            self._owner = None\n            self._block.release()\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n    # Internal methods used by condition variables\n\n    def _acquire_restore(self, state):\n        self._block.acquire()\n        self._count, self._owner = state\n\n    def _release_save(self):\n        if self._count == 0:\n            raise RuntimeError(\"cannot release un-acquired lock\")\n        count = self._count\n        self._count = 0\n        owner = self._owner\n        self._owner = None\n        self._block.release()\n        return (count, owner)\n\n    def _is_owned(self):\n        return self._owner == get_ident()\n\n_PyRLock = _RLock\n\n\nclass Condition:\n    \"\"\"Class that implements a condition variable.\n\n    A condition variable allows one or more threads to wait until they are\n    notified by another thread.\n\n    If the lock argument is given and not None, it must be a Lock or RLock\n    object, and it is used as the underlying lock. Otherwise, a new RLock object\n    is created and used as the underlying lock.\n\n    \"\"\"\n\n    def __init__(self, lock=None):\n        if lock is None:\n            lock = RLock()\n        self._lock = lock\n        # Export the lock's acquire() and release() methods\n        self.acquire = lock.acquire\n        self.release = lock.release\n        # If the lock defines _release_save() and/or _acquire_restore(),\n        # these override the default implementations (which just call\n        # release() and acquire() on the lock).  Ditto for _is_owned().\n        try:\n            self._release_save = lock._release_save\n        except AttributeError:\n            pass\n        try:\n            self._acquire_restore = lock._acquire_restore\n        except AttributeError:\n            pass\n        try:\n            self._is_owned = lock._is_owned\n        except AttributeError:\n            pass\n        self._waiters = _deque()\n\n    def _at_fork_reinit(self):\n        self._lock._at_fork_reinit()\n        self._waiters.clear()\n\n    def __enter__(self):\n        return self._lock.__enter__()\n\n    def __exit__(self, *args):\n        return self._lock.__exit__(*args)\n\n    def __repr__(self):\n        return \"<Condition(%s, %d)>\" % (self._lock, len(self._waiters))\n\n    def _release_save(self):\n        self._lock.release()           # No state to save\n\n    def _acquire_restore(self, x):\n        self._lock.acquire()           # Ignore saved state\n\n    def _is_owned(self):\n        # Return True if lock is owned by current_thread.\n        # This method is called only if _lock doesn't have _is_owned().\n        if self._lock.acquire(False):\n            self._lock.release()\n            return False\n        else:\n            return True\n\n    def wait(self, timeout=None):\n        \"\"\"Wait until notified or until a timeout occurs.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method releases the underlying lock, and then blocks until it is\n        awakened by a notify() or notify_all() call for the same condition\n        variable in another thread, or until the optional timeout occurs. Once\n        awakened or timed out, it re-acquires the lock and returns.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        When the underlying lock is an RLock, it is not released using its\n        release() method, since this may not actually unlock the lock when it\n        was acquired multiple times recursively. Instead, an internal interface\n        of the RLock class is used, which really unlocks it even when it has\n        been recursively acquired several times. Another internal interface is\n        then used to restore the recursion level when the lock is reacquired.\n\n        \"\"\"\n        if not self._is_owned():\n            raise RuntimeError(\"cannot wait on un-acquired lock\")\n        waiter = _allocate_lock()\n        waiter.acquire()\n        self._waiters.append(waiter)\n        saved_state = self._release_save()\n        gotit = False\n        try:    # restore state no matter what (e.g., KeyboardInterrupt)\n            if timeout is None:\n                waiter.acquire()\n                gotit = True\n            else:\n                if timeout > 0:\n                    gotit = waiter.acquire(True, timeout)\n                else:\n                    gotit = waiter.acquire(False)\n            return gotit\n        finally:\n            self._acquire_restore(saved_state)\n            if not gotit:\n                try:\n                    self._waiters.remove(waiter)\n                except ValueError:\n                    pass\n\n    def wait_for(self, predicate, timeout=None):\n        \"\"\"Wait until a condition evaluates to True.\n\n        predicate should be a callable which result will be interpreted as a\n        boolean value.  A timeout may be provided giving the maximum time to\n        wait.\n\n        \"\"\"\n        endtime = None\n        waittime = timeout\n        result = predicate()\n        while not result:\n            if waittime is not None:\n                if endtime is None:\n                    endtime = _time() + waittime\n                else:\n                    waittime = endtime - _time()\n                    if waittime <= 0:\n                        break\n            self.wait(waittime)\n            result = predicate()\n        return result\n\n    def notify(self, n=1):\n        \"\"\"Wake up one or more threads waiting on this condition, if any.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method wakes up at most n of the threads waiting for the condition\n        variable; it is a no-op if no threads are waiting.\n\n        \"\"\"\n        if not self._is_owned():\n            raise RuntimeError(\"cannot notify on un-acquired lock\")\n        waiters = self._waiters\n        while waiters and n > 0:\n            waiter = waiters[0]\n            try:\n                waiter.release()\n            except RuntimeError:\n                # gh-92530: The previous call of notify() released the lock,\n                # but was interrupted before removing it from the queue.\n                # It can happen if a signal handler raises an exception,\n                # like CTRL+C which raises KeyboardInterrupt.\n                pass\n            else:\n                n -= 1\n            try:\n                waiters.remove(waiter)\n            except ValueError:\n                pass\n\n    def notify_all(self):\n        \"\"\"Wake up all threads waiting on this condition.\n\n        If the calling thread has not acquired the lock when this method\n        is called, a RuntimeError is raised.\n\n        \"\"\"\n        self.notify(len(self._waiters))\n\n    def notifyAll(self):\n        \"\"\"Wake up all threads waiting on this condition.\n\n        This method is deprecated, use notify_all() instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('notifyAll() is deprecated, use notify_all() instead',\n                      DeprecationWarning, stacklevel=2)\n        self.notify_all()\n\n\nclass Semaphore:\n    \"\"\"This class implements semaphore objects.\n\n    Semaphores manage a counter representing the number of release() calls minus\n    the number of acquire() calls, plus an initial value. The acquire() method\n    blocks if necessary until it can return without making the counter\n    negative. If not given, value defaults to 1.\n\n    \"\"\"\n\n    # After Tim Peters' semaphore class, but not quite the same (no maximum)\n\n    def __init__(self, value=1):\n        if value < 0:\n            raise ValueError(\"semaphore initial value must be >= 0\")\n        self._cond = Condition(Lock())\n        self._value = value\n\n    def acquire(self, blocking=True, timeout=None):\n        \"\"\"Acquire a semaphore, decrementing the internal counter by one.\n\n        When invoked without arguments: if the internal counter is larger than\n        zero on entry, decrement it by one and return immediately. If it is zero\n        on entry, block, waiting until some other thread has called release() to\n        make it larger than zero. This is done with proper interlocking so that\n        if multiple acquire() calls are blocked, release() will wake exactly one\n        of them up. The implementation may pick one at random, so the order in\n        which blocked threads are awakened should not be relied on. There is no\n        return value in this case.\n\n        When invoked with blocking set to true, do the same thing as when called\n        without arguments, and return true.\n\n        When invoked with blocking set to false, do not block. If a call without\n        an argument would block, return false immediately; otherwise, do the\n        same thing as when called without arguments, and return true.\n\n        When invoked with a timeout other than None, it will block for at\n        most timeout seconds.  If acquire does not complete successfully in\n        that interval, return false.  Return true otherwise.\n\n        \"\"\"\n        if not blocking and timeout is not None:\n            raise ValueError(\"can't specify timeout for non-blocking acquire\")\n        rc = False\n        endtime = None\n        with self._cond:\n            while self._value == 0:\n                if not blocking:\n                    break\n                if timeout is not None:\n                    if endtime is None:\n                        endtime = _time() + timeout\n                    else:\n                        timeout = endtime - _time()\n                        if timeout <= 0:\n                            break\n                self._cond.wait(timeout)\n            else:\n                self._value -= 1\n                rc = True\n        return rc\n\n    __enter__ = acquire\n\n    def release(self, n=1):\n        \"\"\"Release a semaphore, incrementing the internal counter by one or more.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        \"\"\"\n        if n < 1:\n            raise ValueError('n must be one or more')\n        with self._cond:\n            self._value += n\n            for i in range(n):\n                self._cond.notify()\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n\nclass BoundedSemaphore(Semaphore):\n    \"\"\"Implements a bounded semaphore.\n\n    A bounded semaphore checks to make sure its current value doesn't exceed its\n    initial value. If it does, ValueError is raised. In most situations\n    semaphores are used to guard resources with limited capacity.\n\n    If the semaphore is released too many times it's a sign of a bug. If not\n    given, value defaults to 1.\n\n    Like regular semaphores, bounded semaphores manage a counter representing\n    the number of release() calls minus the number of acquire() calls, plus an\n    initial value. The acquire() method blocks if necessary until it can return\n    without making the counter negative. If not given, value defaults to 1.\n\n    \"\"\"\n\n    def __init__(self, value=1):\n        Semaphore.__init__(self, value)\n        self._initial_value = value\n\n    def release(self, n=1):\n        \"\"\"Release a semaphore, incrementing the internal counter by one or more.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        If the number of releases exceeds the number of acquires,\n        raise a ValueError.\n\n        \"\"\"\n        if n < 1:\n            raise ValueError('n must be one or more')\n        with self._cond:\n            if self._value + n > self._initial_value:\n                raise ValueError(\"Semaphore released too many times\")\n            self._value += n\n            for i in range(n):\n                self._cond.notify()\n\n\nclass Event:\n    \"\"\"Class implementing event objects.\n\n    Events manage a flag that can be set to true with the set() method and reset\n    to false with the clear() method. The wait() method blocks until the flag is\n    true.  The flag is initially false.\n\n    \"\"\"\n\n    # After Tim Peters' event class (without is_posted())\n\n    def __init__(self):\n        self._cond = Condition(Lock())\n        self._flag = False\n\n    def _at_fork_reinit(self):\n        # Private method called by Thread._reset_internal_locks()\n        self._cond._at_fork_reinit()\n\n    def is_set(self):\n        \"\"\"Return true if and only if the internal flag is true.\"\"\"\n        return self._flag\n\n    def isSet(self):\n        \"\"\"Return true if and only if the internal flag is true.\n\n        This method is deprecated, use is_set() instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('isSet() is deprecated, use is_set() instead',\n                      DeprecationWarning, stacklevel=2)\n        return self.is_set()\n\n    def set(self):\n        \"\"\"Set the internal flag to true.\n\n        All threads waiting for it to become true are awakened. Threads\n        that call wait() once the flag is true will not block at all.\n\n        \"\"\"\n        with self._cond:\n            self._flag = True\n            self._cond.notify_all()\n\n    def clear(self):\n        \"\"\"Reset the internal flag to false.\n\n        Subsequently, threads calling wait() will block until set() is called to\n        set the internal flag to true again.\n\n        \"\"\"\n        with self._cond:\n            self._flag = False\n\n    def wait(self, timeout=None):\n        \"\"\"Block until the internal flag is true.\n\n        If the internal flag is true on entry, return immediately. Otherwise,\n        block until another thread calls set() to set the flag to true, or until\n        the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        This method returns the internal flag on exit, so it will always return\n        True except if a timeout is given and the operation times out.\n\n        \"\"\"\n        with self._cond:\n            signaled = self._flag\n            if not signaled:\n                signaled = self._cond.wait(timeout)\n            return signaled\n\n\n# A barrier class.  Inspired in part by the pthread_barrier_* api and\n# the CyclicBarrier class from Java.  See\n# http://sourceware.org/pthreads-win32/manual/pthread_barrier_init.html and\n# http://java.sun.com/j2se/1.5.0/docs/api/java/util/concurrent/\n#        CyclicBarrier.html\n# for information.\n# We maintain two main states, 'filling' and 'draining' enabling the barrier\n# to be cyclic.  Threads are not allowed into it until it has fully drained\n# since the previous cycle.  In addition, a 'resetting' state exists which is\n# similar to 'draining' except that threads leave with a BrokenBarrierError,\n# and a 'broken' state in which all threads get the exception.\nclass Barrier:\n    \"\"\"Implements a Barrier.\n\n    Useful for synchronizing a fixed number of threads at known synchronization\n    points.  Threads block on 'wait()' and are simultaneously awoken once they\n    have all made that call.\n\n    \"\"\"\n\n    def __init__(self, parties, action=None, timeout=None):\n        \"\"\"Create a barrier, initialised to 'parties' threads.\n\n        'action' is a callable which, when supplied, will be called by one of\n        the threads after they have all entered the barrier and just prior to\n        releasing them all. If a 'timeout' is provided, it is used as the\n        default for all subsequent 'wait()' calls.\n\n        \"\"\"\n        self._cond = Condition(Lock())\n        self._action = action\n        self._timeout = timeout\n        self._parties = parties\n        self._state = 0  # 0 filling, 1 draining, -1 resetting, -2 broken\n        self._count = 0\n\n    def wait(self, timeout=None):\n        \"\"\"Wait for the barrier.\n\n        When the specified number of threads have started waiting, they are all\n        simultaneously awoken. If an 'action' was provided for the barrier, one\n        of the threads will have executed that callback prior to returning.\n        Returns an individual index number from 0 to 'parties-1'.\n\n        \"\"\"\n        if timeout is None:\n            timeout = self._timeout\n        with self._cond:\n            self._enter() # Block while the barrier drains.\n            index = self._count\n            self._count += 1\n            try:\n                if index + 1 == self._parties:\n                    # We release the barrier\n                    self._release()\n                else:\n                    # We wait until someone releases us\n                    self._wait(timeout)\n                return index\n            finally:\n                self._count -= 1\n                # Wake up any threads waiting for barrier to drain.\n                self._exit()\n\n    # Block until the barrier is ready for us, or raise an exception\n    # if it is broken.\n    def _enter(self):\n        while self._state in (-1, 1):\n            # It is draining or resetting, wait until done\n            self._cond.wait()\n        #see if the barrier is in a broken state\n        if self._state < 0:\n            raise BrokenBarrierError\n        assert self._state == 0\n\n    # Optionally run the 'action' and release the threads waiting\n    # in the barrier.\n    def _release(self):\n        try:\n            if self._action:\n                self._action()\n            # enter draining state\n            self._state = 1\n            self._cond.notify_all()\n        except:\n            #an exception during the _action handler.  Break and reraise\n            self._break()\n            raise\n\n    # Wait in the barrier until we are released.  Raise an exception\n    # if the barrier is reset or broken.\n    def _wait(self, timeout):\n        if not self._cond.wait_for(lambda : self._state != 0, timeout):\n            #timed out.  Break the barrier\n            self._break()\n            raise BrokenBarrierError\n        if self._state < 0:\n            raise BrokenBarrierError\n        assert self._state == 1\n\n    # If we are the last thread to exit the barrier, signal any threads\n    # waiting for the barrier to drain.\n    def _exit(self):\n        if self._count == 0:\n            if self._state in (-1, 1):\n                #resetting or draining\n                self._state = 0\n                self._cond.notify_all()\n\n    def reset(self):\n        \"\"\"Reset the barrier to the initial state.\n\n        Any threads currently waiting will get the BrokenBarrier exception\n        raised.\n\n        \"\"\"\n        with self._cond:\n            if self._count > 0:\n                if self._state == 0:\n                    #reset the barrier, waking up threads\n                    self._state = -1\n                elif self._state == -2:\n                    #was broken, set it to reset state\n                    #which clears when the last thread exits\n                    self._state = -1\n            else:\n                self._state = 0\n            self._cond.notify_all()\n\n    def abort(self):\n        \"\"\"Place the barrier into a 'broken' state.\n\n        Useful in case of error.  Any currently waiting threads and threads\n        attempting to 'wait()' will have BrokenBarrierError raised.\n\n        \"\"\"\n        with self._cond:\n            self._break()\n\n    def _break(self):\n        # An internal error was detected.  The barrier is set to\n        # a broken state all parties awakened.\n        self._state = -2\n        self._cond.notify_all()\n\n    @property\n    def parties(self):\n        \"\"\"Return the number of threads required to trip the barrier.\"\"\"\n        return self._parties\n\n    @property\n    def n_waiting(self):\n        \"\"\"Return the number of threads currently waiting at the barrier.\"\"\"\n        # We don't need synchronization here since this is an ephemeral result\n        # anyway.  It returns the correct value in the steady state.\n        if self._state == 0:\n            return self._count\n        return 0\n\n    @property\n    def broken(self):\n        \"\"\"Return True if the barrier is in a broken state.\"\"\"\n        return self._state == -2\n\n# exception raised by the Barrier class\nclass BrokenBarrierError(RuntimeError):\n    pass\n\n\n# Helper to generate new thread names\n_counter = _count(1).__next__\ndef _newname(name_template):\n    return name_template % _counter()\n\n# Active thread administration.\n#\n# bpo-44422: Use a reentrant lock to allow reentrant calls to functions like\n# threading.enumerate().\n_active_limbo_lock = RLock()\n_active = {}    # maps thread id to Thread object\n_limbo = {}\n_dangling = WeakSet()\n\n# Set of Thread._tstate_lock locks of non-daemon threads used by _shutdown()\n# to wait until all Python thread states get deleted:\n# see Thread._set_tstate_lock().\n_shutdown_locks_lock = _allocate_lock()\n_shutdown_locks = set()\n\ndef _maintain_shutdown_locks():\n    \"\"\"\n    Drop any shutdown locks that don't correspond to running threads anymore.\n\n    Calling this from time to time avoids an ever-growing _shutdown_locks\n    set when Thread objects are not joined explicitly. See bpo-37788.\n\n    This must be called with _shutdown_locks_lock acquired.\n    \"\"\"\n    # If a lock was released, the corresponding thread has exited\n    to_remove = [lock for lock in _shutdown_locks if not lock.locked()]\n    _shutdown_locks.difference_update(to_remove)\n\n\n# Main class for threads\n\nclass Thread:\n    \"\"\"A class that represents a thread of control.\n\n    This class can be safely subclassed in a limited fashion. There are two ways\n    to specify the activity: by passing a callable object to the constructor, or\n    by overriding the run() method in a subclass.\n\n    \"\"\"\n\n    _initialized = False\n\n    def __init__(self, group=None, target=None, name=None,\n                 args=(), kwargs=None, *, daemon=None):\n        \"\"\"This constructor should always be called with keyword arguments. Arguments are:\n\n        *group* should be None; reserved for future extension when a ThreadGroup\n        class is implemented.\n\n        *target* is the callable object to be invoked by the run()\n        method. Defaults to None, meaning nothing is called.\n\n        *name* is the thread name. By default, a unique name is constructed of\n        the form \"Thread-N\" where N is a small decimal number.\n\n        *args* is the argument tuple for the target invocation. Defaults to ().\n\n        *kwargs* is a dictionary of keyword arguments for the target\n        invocation. Defaults to {}.\n\n        If a subclass overrides the constructor, it must make sure to invoke\n        the base class constructor (Thread.__init__()) before doing anything\n        else to the thread.\n\n        \"\"\"\n        assert group is None, \"group argument must be None for now\"\n        if kwargs is None:\n            kwargs = {}\n        if name:\n            name = str(name)\n        else:\n            name = _newname(\"Thread-%d\")\n            if target is not None:\n                try:\n                    target_name = target.__name__\n                    name += f\" ({target_name})\"\n                except AttributeError:\n                    pass\n\n        self._target = target\n        self._name = name\n        self._args = args\n        self._kwargs = kwargs\n        if daemon is not None:\n            self._daemonic = daemon\n        else:\n            self._daemonic = current_thread().daemon\n        self._ident = None\n        if _HAVE_THREAD_NATIVE_ID:\n            self._native_id = None\n        self._tstate_lock = None\n        self._started = Event()\n        self._is_stopped = False\n        self._initialized = True\n        # Copy of sys.stderr used by self._invoke_excepthook()\n        self._stderr = _sys.stderr\n        self._invoke_excepthook = _make_invoke_excepthook()\n        # For debugging and _after_fork()\n        _dangling.add(self)\n\n    def _reset_internal_locks(self, is_alive):\n        # private!  Called by _after_fork() to reset our internal locks as\n        # they may be in an invalid state leading to a deadlock or crash.\n        self._started._at_fork_reinit()\n        if is_alive:\n            # bpo-42350: If the fork happens when the thread is already stopped\n            # (ex: after threading._shutdown() has been called), _tstate_lock\n            # is None. Do nothing in this case.\n            if self._tstate_lock is not None:\n                self._tstate_lock._at_fork_reinit()\n                self._tstate_lock.acquire()\n        else:\n            # The thread isn't alive after fork: it doesn't have a tstate\n            # anymore.\n            self._is_stopped = True\n            self._tstate_lock = None\n\n    def __repr__(self):\n        assert self._initialized, \"Thread.__init__() was not called\"\n        status = \"initial\"\n        if self._started.is_set():\n            status = \"started\"\n        self.is_alive() # easy way to get ._is_stopped set when appropriate\n        if self._is_stopped:\n            status = \"stopped\"\n        if self._daemonic:\n            status += \" daemon\"\n        if self._ident is not None:\n            status += \" %s\" % self._ident\n        return \"<%s(%s, %s)>\" % (self.__class__.__name__, self._name, status)\n\n    def start(self):\n        \"\"\"Start the thread's activity.\n\n        It must be called at most once per thread object. It arranges for the\n        object's run() method to be invoked in a separate thread of control.\n\n        This method will raise a RuntimeError if called more than once on the\n        same thread object.\n\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"thread.__init__() not called\")\n\n        if self._started.is_set():\n            raise RuntimeError(\"threads can only be started once\")\n\n        with _active_limbo_lock:\n            _limbo[self] = self\n        try:\n            _start_new_thread(self._bootstrap, ())\n        except Exception:\n            with _active_limbo_lock:\n                del _limbo[self]\n            raise\n        self._started.wait()\n\n    def run(self):\n        \"\"\"Method representing the thread's activity.\n\n        You may override this method in a subclass. The standard run() method\n        invokes the callable object passed to the object's constructor as the\n        target argument, if any, with sequential and keyword arguments taken\n        from the args and kwargs arguments, respectively.\n\n        \"\"\"\n        try:\n            if self._target is not None:\n                self._target(*self._args, **self._kwargs)\n        finally:\n            # Avoid a refcycle if the thread is running a function with\n            # an argument that has a member that points to the thread.\n            del self._target, self._args, self._kwargs\n\n    def _bootstrap(self):\n        # Wrapper around the real bootstrap code that ignores\n        # exceptions during interpreter cleanup.  Those typically\n        # happen when a daemon thread wakes up at an unfortunate\n        # moment, finds the world around it destroyed, and raises some\n        # random exception *** while trying to report the exception in\n        # _bootstrap_inner() below ***.  Those random exceptions\n        # don't help anybody, and they confuse users, so we suppress\n        # them.  We suppress them only when it appears that the world\n        # indeed has already been destroyed, so that exceptions in\n        # _bootstrap_inner() during normal business hours are properly\n        # reported.  Also, we only suppress them for daemonic threads;\n        # if a non-daemonic encounters this, something else is wrong.\n        try:\n            self._bootstrap_inner()\n        except:\n            if self._daemonic and _sys is None:\n                return\n            raise\n\n    def _set_ident(self):\n        self._ident = get_ident()\n\n    if _HAVE_THREAD_NATIVE_ID:\n        def _set_native_id(self):\n            self._native_id = get_native_id()\n\n    def _set_tstate_lock(self):\n        \"\"\"\n        Set a lock object which will be released by the interpreter when\n        the underlying thread state (see pystate.h) gets deleted.\n        \"\"\"\n        self._tstate_lock = _set_sentinel()\n        self._tstate_lock.acquire()\n\n        if not self.daemon:\n            with _shutdown_locks_lock:\n                _maintain_shutdown_locks()\n                _shutdown_locks.add(self._tstate_lock)\n\n    def _bootstrap_inner(self):\n        try:\n            self._set_ident()\n            self._set_tstate_lock()\n            if _HAVE_THREAD_NATIVE_ID:\n                self._set_native_id()\n            self._started.set()\n            with _active_limbo_lock:\n                _active[self._ident] = self\n                del _limbo[self]\n\n            if _trace_hook:\n                _sys.settrace(_trace_hook)\n            if _profile_hook:\n                _sys.setprofile(_profile_hook)\n\n            try:\n                self.run()\n            except:\n                self._invoke_excepthook(self)\n        finally:\n            with _active_limbo_lock:\n                try:\n                    # We don't call self._delete() because it also\n                    # grabs _active_limbo_lock.\n                    del _active[get_ident()]\n                except:\n                    pass\n\n    def _stop(self):\n        # After calling ._stop(), .is_alive() returns False and .join() returns\n        # immediately.  ._tstate_lock must be released before calling ._stop().\n        #\n        # Normal case:  C code at the end of the thread's life\n        # (release_sentinel in _threadmodule.c) releases ._tstate_lock, and\n        # that's detected by our ._wait_for_tstate_lock(), called by .join()\n        # and .is_alive().  Any number of threads _may_ call ._stop()\n        # simultaneously (for example, if multiple threads are blocked in\n        # .join() calls), and they're not serialized.  That's harmless -\n        # they'll just make redundant rebindings of ._is_stopped and\n        # ._tstate_lock.  Obscure:  we rebind ._tstate_lock last so that the\n        # \"assert self._is_stopped\" in ._wait_for_tstate_lock() always works\n        # (the assert is executed only if ._tstate_lock is None).\n        #\n        # Special case:  _main_thread releases ._tstate_lock via this\n        # module's _shutdown() function.\n        lock = self._tstate_lock\n        if lock is not None:\n            assert not lock.locked()\n        self._is_stopped = True\n        self._tstate_lock = None\n        if not self.daemon:\n            with _shutdown_locks_lock:\n                # Remove our lock and other released locks from _shutdown_locks\n                _maintain_shutdown_locks()\n\n    def _delete(self):\n        \"Remove current thread from the dict of currently running threads.\"\n        with _active_limbo_lock:\n            del _active[get_ident()]\n            # There must not be any python code between the previous line\n            # and after the lock is released.  Otherwise a tracing function\n            # could try to acquire the lock again in the same thread, (in\n            # current_thread()), and would block.\n\n    def join(self, timeout=None):\n        \"\"\"Wait until the thread terminates.\n\n        This blocks the calling thread until the thread whose join() method is\n        called terminates -- either normally or through an unhandled exception\n        or until the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof). As join() always returns None, you must call\n        is_alive() after join() to decide whether a timeout happened -- if the\n        thread is still alive, the join() call timed out.\n\n        When the timeout argument is not present or None, the operation will\n        block until the thread terminates.\n\n        A thread can be join()ed many times.\n\n        join() raises a RuntimeError if an attempt is made to join the current\n        thread as that would cause a deadlock. It is also an error to join() a\n        thread before it has been started and attempts to do so raises the same\n        exception.\n\n        \"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if not self._started.is_set():\n            raise RuntimeError(\"cannot join thread before it is started\")\n        if self is current_thread():\n            raise RuntimeError(\"cannot join current thread\")\n\n        if timeout is None:\n            self._wait_for_tstate_lock()\n        else:\n            # the behavior of a negative timeout isn't documented, but\n            # historically .join(timeout=x) for x<0 has acted as if timeout=0\n            self._wait_for_tstate_lock(timeout=max(timeout, 0))\n\n    def _wait_for_tstate_lock(self, block=True, timeout=-1):\n        # Issue #18808: wait for the thread state to be gone.\n        # At the end of the thread's life, after all knowledge of the thread\n        # is removed from C data structures, C code releases our _tstate_lock.\n        # This method passes its arguments to _tstate_lock.acquire().\n        # If the lock is acquired, the C code is done, and self._stop() is\n        # called.  That sets ._is_stopped to True, and ._tstate_lock to None.\n        lock = self._tstate_lock\n        if lock is None:\n            # already determined that the C code is done\n            assert self._is_stopped\n            return\n\n        try:\n            if lock.acquire(block, timeout):\n                lock.release()\n                self._stop()\n        except:\n            if lock.locked():\n                # bpo-45274: lock.acquire() acquired the lock, but the function\n                # was interrupted with an exception before reaching the\n                # lock.release(). It can happen if a signal handler raises an\n                # exception, like CTRL+C which raises KeyboardInterrupt.\n                lock.release()\n                self._stop()\n            raise\n\n    @property\n    def name(self):\n        \"\"\"A string used for identification purposes only.\n\n        It has no semantics. Multiple threads may be given the same name. The\n        initial name is set by the constructor.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        assert self._initialized, \"Thread.__init__() not called\"\n        self._name = str(name)\n\n    @property\n    def ident(self):\n        \"\"\"Thread identifier of this thread or None if it has not been started.\n\n        This is a nonzero integer. See the get_ident() function. Thread\n        identifiers may be recycled when a thread exits and another thread is\n        created. The identifier is available even after the thread has exited.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._ident\n\n    if _HAVE_THREAD_NATIVE_ID:\n        @property\n        def native_id(self):\n            \"\"\"Native integral thread ID of this thread, or None if it has not been started.\n\n            This is a non-negative integer. See the get_native_id() function.\n            This represents the Thread ID as reported by the kernel.\n\n            \"\"\"\n            assert self._initialized, \"Thread.__init__() not called\"\n            return self._native_id\n\n    def is_alive(self):\n        \"\"\"Return whether the thread is alive.\n\n        This method returns True just before the run() method starts until just\n        after the run() method terminates. See also the module function\n        enumerate().\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        if self._is_stopped or not self._started.is_set():\n            return False\n        self._wait_for_tstate_lock(False)\n        return not self._is_stopped\n\n    @property\n    def daemon(self):\n        \"\"\"A boolean value indicating whether this thread is a daemon thread.\n\n        This must be set before start() is called, otherwise RuntimeError is\n        raised. Its initial value is inherited from the creating thread; the\n        main thread is not a daemon thread and therefore all threads created in\n        the main thread default to daemon = False.\n\n        The entire Python program exits when only daemon threads are left.\n\n        \"\"\"\n        assert self._initialized, \"Thread.__init__() not called\"\n        return self._daemonic\n\n    @daemon.setter\n    def daemon(self, daemonic):\n        if not self._initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if self._started.is_set():\n            raise RuntimeError(\"cannot set daemon status of active thread\")\n        self._daemonic = daemonic\n\n    def isDaemon(self):\n        \"\"\"Return whether this thread is a daemon.\n\n        This method is deprecated, use the daemon attribute instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('isDaemon() is deprecated, get the daemon attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        return self.daemon\n\n    def setDaemon(self, daemonic):\n        \"\"\"Set whether this thread is a daemon.\n\n        This method is deprecated, use the .daemon property instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('setDaemon() is deprecated, set the daemon attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        self.daemon = daemonic\n\n    def getName(self):\n        \"\"\"Return a string used for identification purposes only.\n\n        This method is deprecated, use the name attribute instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('getName() is deprecated, get the name attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        return self.name\n\n    def setName(self, name):\n        \"\"\"Set the name string for this thread.\n\n        This method is deprecated, use the name attribute instead.\n\n        \"\"\"\n        import warnings\n        warnings.warn('setName() is deprecated, set the name attribute instead',\n                      DeprecationWarning, stacklevel=2)\n        self.name = name\n\n\ntry:\n    from _thread import (_excepthook as excepthook,\n                         _ExceptHookArgs as ExceptHookArgs)\nexcept ImportError:\n    # Simple Python implementation if _thread._excepthook() is not available\n    from traceback import print_exception as _print_exception\n    from collections import namedtuple\n\n    _ExceptHookArgs = namedtuple(\n        'ExceptHookArgs',\n        'exc_type exc_value exc_traceback thread')\n\n    def ExceptHookArgs(args):\n        return _ExceptHookArgs(*args)\n\n    def excepthook(args, /):\n        \"\"\"\n        Handle uncaught Thread.run() exception.\n        \"\"\"\n        if args.exc_type == SystemExit:\n            # silently ignore SystemExit\n            return\n\n        if _sys is not None and _sys.stderr is not None:\n            stderr = _sys.stderr\n        elif args.thread is not None:\n            stderr = args.thread._stderr\n            if stderr is None:\n                # do nothing if sys.stderr is None and sys.stderr was None\n                # when the thread was created\n                return\n        else:\n            # do nothing if sys.stderr is None and args.thread is None\n            return\n\n        if args.thread is not None:\n            name = args.thread.name\n        else:\n            name = get_ident()\n        print(f\"Exception in thread {name}:\",\n              file=stderr, flush=True)\n        _print_exception(args.exc_type, args.exc_value, args.exc_traceback,\n                         file=stderr)\n        stderr.flush()\n\n\n# Original value of threading.excepthook\n__excepthook__ = excepthook\n\n\ndef _make_invoke_excepthook():\n    # Create a local namespace to ensure that variables remain alive\n    # when _invoke_excepthook() is called, even if it is called late during\n    # Python shutdown. It is mostly needed for daemon threads.\n\n    old_excepthook = excepthook\n    old_sys_excepthook = _sys.excepthook\n    if old_excepthook is None:\n        raise RuntimeError(\"threading.excepthook is None\")\n    if old_sys_excepthook is None:\n        raise RuntimeError(\"sys.excepthook is None\")\n\n    sys_exc_info = _sys.exc_info\n    local_print = print\n    local_sys = _sys\n\n    def invoke_excepthook(thread):\n        global excepthook\n        try:\n            hook = excepthook\n            if hook is None:\n                hook = old_excepthook\n\n            args = ExceptHookArgs([*sys_exc_info(), thread])\n\n            hook(args)\n        except Exception as exc:\n            exc.__suppress_context__ = True\n            del exc\n\n            if local_sys is not None and local_sys.stderr is not None:\n                stderr = local_sys.stderr\n            else:\n                stderr = thread._stderr\n\n            local_print(\"Exception in threading.excepthook:\",\n                        file=stderr, flush=True)\n\n            if local_sys is not None and local_sys.excepthook is not None:\n                sys_excepthook = local_sys.excepthook\n            else:\n                sys_excepthook = old_sys_excepthook\n\n            sys_excepthook(*sys_exc_info())\n        finally:\n            # Break reference cycle (exception stored in a variable)\n            args = None\n\n    return invoke_excepthook\n\n\n# The timer class was contributed by Itamar Shtull-Trauring\n\nclass Timer(Thread):\n    \"\"\"Call a function after a specified number of seconds:\n\n            t = Timer(30.0, f, args=None, kwargs=None)\n            t.start()\n            t.cancel()     # stop the timer's action if it's still waiting\n\n    \"\"\"\n\n    def __init__(self, interval, function, args=None, kwargs=None):\n        Thread.__init__(self)\n        self.interval = interval\n        self.function = function\n        self.args = args if args is not None else []\n        self.kwargs = kwargs if kwargs is not None else {}\n        self.finished = Event()\n\n    def cancel(self):\n        \"\"\"Stop the timer if it hasn't finished yet.\"\"\"\n        self.finished.set()\n\n    def run(self):\n        self.finished.wait(self.interval)\n        if not self.finished.is_set():\n            self.function(*self.args, **self.kwargs)\n        self.finished.set()\n\n\n# Special thread class to represent the main thread\n\nclass _MainThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=\"MainThread\", daemon=False)\n        self._set_tstate_lock()\n        self._started.set()\n        self._set_ident()\n        if _HAVE_THREAD_NATIVE_ID:\n            self._set_native_id()\n        with _active_limbo_lock:\n            _active[self._ident] = self\n\n\n# Dummy thread class to represent threads not started here.\n# These aren't garbage collected when they die, nor can they be waited for.\n# If they invoke anything in threading.py that calls current_thread(), they\n# leave an entry in the _active dict forever after.\n# Their purpose is to return *something* from current_thread().\n# They are marked as daemon threads so we won't wait for them\n# when we exit (conform previous semantics).\n\nclass _DummyThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=_newname(\"Dummy-%d\"), daemon=True)\n\n        self._started.set()\n        self._set_ident()\n        if _HAVE_THREAD_NATIVE_ID:\n            self._set_native_id()\n        with _active_limbo_lock:\n            _active[self._ident] = self\n\n    def _stop(self):\n        pass\n\n    def is_alive(self):\n        assert not self._is_stopped and self._started.is_set()\n        return True\n\n    def join(self, timeout=None):\n        assert False, \"cannot join a dummy thread\"\n\n\n# Global API functions\n\ndef current_thread():\n    \"\"\"Return the current Thread object, corresponding to the caller's thread of control.\n\n    If the caller's thread of control was not created through the threading\n    module, a dummy thread object with limited functionality is returned.\n\n    \"\"\"\n    try:\n        return _active[get_ident()]\n    except KeyError:\n        return _DummyThread()\n\ndef currentThread():\n    \"\"\"Return the current Thread object, corresponding to the caller's thread of control.\n\n    This function is deprecated, use current_thread() instead.\n\n    \"\"\"\n    import warnings\n    warnings.warn('currentThread() is deprecated, use current_thread() instead',\n                  DeprecationWarning, stacklevel=2)\n    return current_thread()\n\ndef active_count():\n    \"\"\"Return the number of Thread objects currently alive.\n\n    The returned count is equal to the length of the list returned by\n    enumerate().\n\n    \"\"\"\n    with _active_limbo_lock:\n        return len(_active) + len(_limbo)\n\ndef activeCount():\n    \"\"\"Return the number of Thread objects currently alive.\n\n    This function is deprecated, use active_count() instead.\n\n    \"\"\"\n    import warnings\n    warnings.warn('activeCount() is deprecated, use active_count() instead',\n                  DeprecationWarning, stacklevel=2)\n    return active_count()\n\ndef _enumerate():\n    # Same as enumerate(), but without the lock. Internal use only.\n    return list(_active.values()) + list(_limbo.values())\n\ndef enumerate():\n    \"\"\"Return a list of all Thread objects currently alive.\n\n    The list includes daemonic threads, dummy thread objects created by\n    current_thread(), and the main thread. It excludes terminated threads and\n    threads that have not yet been started.\n\n    \"\"\"\n    with _active_limbo_lock:\n        return list(_active.values()) + list(_limbo.values())\n\n\n_threading_atexits = []\n_SHUTTING_DOWN = False\n\ndef _register_atexit(func, *arg, **kwargs):\n    \"\"\"CPython internal: register *func* to be called before joining threads.\n\n    The registered *func* is called with its arguments just before all\n    non-daemon threads are joined in `_shutdown()`. It provides a similar\n    purpose to `atexit.register()`, but its functions are called prior to\n    threading shutdown instead of interpreter shutdown.\n\n    For similarity to atexit, the registered functions are called in reverse.\n    \"\"\"\n    if _SHUTTING_DOWN:\n        raise RuntimeError(\"can't register atexit after shutdown\")\n\n    call = functools.partial(func, *arg, **kwargs)\n    _threading_atexits.append(call)\n\n\nfrom _thread import stack_size\n\n# Create the main thread object,\n# and make it available for the interpreter\n# (Py_Main) as threading._shutdown.\n\n_main_thread = _MainThread()\n\ndef _shutdown():\n    \"\"\"\n    Wait until the Python thread state of all non-daemon threads get deleted.\n    \"\"\"\n    # Obscure:  other threads may be waiting to join _main_thread.  That's\n    # dubious, but some code does it.  We can't wait for C code to release\n    # the main thread's tstate_lock - that won't happen until the interpreter\n    # is nearly dead.  So we release it here.  Note that just calling _stop()\n    # isn't enough:  other threads may already be waiting on _tstate_lock.\n    if _main_thread._is_stopped:\n        # _shutdown() was already called\n        return\n\n    global _SHUTTING_DOWN\n    _SHUTTING_DOWN = True\n\n    # Call registered threading atexit functions before threads are joined.\n    # Order is reversed, similar to atexit.\n    for atexit_call in reversed(_threading_atexits):\n        atexit_call()\n\n    # Main thread\n    if _main_thread.ident == get_ident():\n        tlock = _main_thread._tstate_lock\n        # The main thread isn't finished yet, so its thread state lock can't\n        # have been released.\n        assert tlock is not None\n        assert tlock.locked()\n        tlock.release()\n        _main_thread._stop()\n    else:\n        # bpo-1596321: _shutdown() must be called in the main thread.\n        # If the threading module was not imported by the main thread,\n        # _main_thread is the thread which imported the threading module.\n        # In this case, ignore _main_thread, similar behavior than for threads\n        # spawned by C libraries or using _thread.start_new_thread().\n        pass\n\n    # Join all non-deamon threads\n    while True:\n        with _shutdown_locks_lock:\n            locks = list(_shutdown_locks)\n            _shutdown_locks.clear()\n\n        if not locks:\n            break\n\n        for lock in locks:\n            # mimic Thread.join()\n            lock.acquire()\n            lock.release()\n\n        # new threads can be spawned while we were waiting for the other\n        # threads to complete\n\n\ndef main_thread():\n    \"\"\"Return the main thread object.\n\n    In normal conditions, the main thread is the thread from which the\n    Python interpreter was started.\n    \"\"\"\n    return _main_thread\n\n# get thread-local implementation, either from the thread\n# module, or from the python fallback\n\ntry:\n    from _thread import _local as local\nexcept ImportError:\n    from _threading_local import local\n\n\ndef _after_fork():\n    \"\"\"\n    Cleanup threading module state that should not exist after a fork.\n    \"\"\"\n    # Reset _active_limbo_lock, in case we forked while the lock was held\n    # by another (non-forked) thread.  http://bugs.python.org/issue874900\n    global _active_limbo_lock, _main_thread\n    global _shutdown_locks_lock, _shutdown_locks\n    _active_limbo_lock = RLock()\n\n    # fork() only copied the current thread; clear references to others.\n    new_active = {}\n\n    try:\n        current = _active[get_ident()]\n    except KeyError:\n        # fork() was called in a thread which was not spawned\n        # by threading.Thread. For example, a thread spawned\n        # by thread.start_new_thread().\n        current = _MainThread()\n\n    _main_thread = current\n\n    # reset _shutdown() locks: threads re-register their _tstate_lock below\n    _shutdown_locks_lock = _allocate_lock()\n    _shutdown_locks = set()\n\n    with _active_limbo_lock:\n        # Dangling thread instances must still have their locks reset,\n        # because someone may join() them.\n        threads = set(_enumerate())\n        threads.update(_dangling)\n        for thread in threads:\n            # Any lock/condition variable may be currently locked or in an\n            # invalid state, so we reinitialize them.\n            if thread is current:\n                # There is only one active thread. We reset the ident to\n                # its new value since it can have changed.\n                thread._reset_internal_locks(True)\n                ident = get_ident()\n                thread._ident = ident\n                new_active[ident] = thread\n            else:\n                # All the others are already stopped.\n                thread._reset_internal_locks(False)\n                thread._stop()\n\n        _limbo.clear()\n        _active.clear()\n        _active.update(new_active)\n        assert len(_active) == 1\n\n\nif hasattr(_os, \"register_at_fork\"):\n    _os.register_at_fork(after_in_child=_after_fork)\n", 1645], "D:\\Program\\anaconda3\\lib\\asyncio\\base_futures.py": ["__all__ = ()\n\nimport reprlib\nfrom _thread import get_ident\n\nfrom . import format_helpers\n\n# States for Future.\n_PENDING = 'PENDING'\n_CANCELLED = 'CANCELLED'\n_FINISHED = 'FINISHED'\n\n\ndef isfuture(obj):\n    \"\"\"Check for a Future.\n\n    This returns True when obj is a Future instance or is advertising\n    itself as duck-type compatible by setting _asyncio_future_blocking.\n    See comment in Future for more details.\n    \"\"\"\n    return (hasattr(obj.__class__, '_asyncio_future_blocking') and\n            obj._asyncio_future_blocking is not None)\n\n\ndef _format_callbacks(cb):\n    \"\"\"helper function for Future.__repr__\"\"\"\n    size = len(cb)\n    if not size:\n        cb = ''\n\n    def format_cb(callback):\n        return format_helpers._format_callback_source(callback, ())\n\n    if size == 1:\n        cb = format_cb(cb[0][0])\n    elif size == 2:\n        cb = '{}, {}'.format(format_cb(cb[0][0]), format_cb(cb[1][0]))\n    elif size > 2:\n        cb = '{}, <{} more>, {}'.format(format_cb(cb[0][0]),\n                                        size - 2,\n                                        format_cb(cb[-1][0]))\n    return f'cb=[{cb}]'\n\n\n# bpo-42183: _repr_running is needed for repr protection\n# when a Future or Task result contains itself directly or indirectly.\n# The logic is borrowed from @reprlib.recursive_repr decorator.\n# Unfortunately, the direct decorator usage is impossible because of\n# AttributeError: '_asyncio.Task' object has no attribute '__module__' error.\n#\n# After fixing this thing we can return to the decorator based approach.\n_repr_running = set()\n\n\ndef _future_repr_info(future):\n    # (Future) -> str\n    \"\"\"helper function for Future.__repr__\"\"\"\n    info = [future._state.lower()]\n    if future._state == _FINISHED:\n        if future._exception is not None:\n            info.append(f'exception={future._exception!r}')\n        else:\n            key = id(future), get_ident()\n            if key in _repr_running:\n                result = '...'\n            else:\n                _repr_running.add(key)\n                try:\n                    # use reprlib to limit the length of the output, especially\n                    # for very long strings\n                    result = reprlib.repr(future._result)\n                finally:\n                    _repr_running.discard(key)\n            info.append(f'result={result}')\n    if future._callbacks:\n        info.append(_format_callbacks(future._callbacks))\n    if future._source_traceback:\n        frame = future._source_traceback[-1]\n        info.append(f'created at {frame[0]}:{frame[1]}')\n    return info\n", 80], "D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py": ["\"\"\"Support for tasks, coroutines and the scheduler.\"\"\"\n\n__all__ = (\n    'Task', 'create_task',\n    'FIRST_COMPLETED', 'FIRST_EXCEPTION', 'ALL_COMPLETED',\n    'wait', 'wait_for', 'as_completed', 'sleep',\n    'gather', 'shield', 'ensure_future', 'run_coroutine_threadsafe',\n    'current_task', 'all_tasks',\n    '_register_task', '_unregister_task', '_enter_task', '_leave_task',\n)\n\nimport concurrent.futures\nimport contextvars\nimport functools\nimport inspect\nimport itertools\nimport types\nimport warnings\nimport weakref\nfrom types import GenericAlias\n\nfrom . import base_tasks\nfrom . import coroutines\nfrom . import events\nfrom . import exceptions\nfrom . import futures\nfrom .coroutines import _is_coroutine\n\n# Helper to generate new task names\n# This uses itertools.count() instead of a \"+= 1\" operation because the latter\n# is not thread safe. See bpo-11866 for a longer explanation.\n_task_name_counter = itertools.count(1).__next__\n\n\ndef current_task(loop=None):\n    \"\"\"Return a currently executed task.\"\"\"\n    if loop is None:\n        loop = events.get_running_loop()\n    return _current_tasks.get(loop)\n\n\ndef all_tasks(loop=None):\n    \"\"\"Return a set of all tasks for the loop.\"\"\"\n    if loop is None:\n        loop = events.get_running_loop()\n    # Looping over a WeakSet (_all_tasks) isn't safe as it can be updated from another\n    # thread while we do so. Therefore we cast it to list prior to filtering. The list\n    # cast itself requires iteration, so we repeat it several times ignoring\n    # RuntimeErrors (which are not very likely to occur). See issues 34970 and 36607 for\n    # details.\n    i = 0\n    while True:\n        try:\n            tasks = list(_all_tasks)\n        except RuntimeError:\n            i += 1\n            if i >= 1000:\n                raise\n        else:\n            break\n    return {t for t in tasks\n            if futures._get_loop(t) is loop and not t.done()}\n\n\ndef _set_task_name(task, name):\n    if name is not None:\n        try:\n            set_name = task.set_name\n        except AttributeError:\n            pass\n        else:\n            set_name(name)\n\n\nclass Task(futures._PyFuture):  # Inherit Python Task implementation\n                                # from a Python Future implementation.\n\n    \"\"\"A coroutine wrapped in a Future.\"\"\"\n\n    # An important invariant maintained while a Task not done:\n    #\n    # - Either _fut_waiter is None, and _step() is scheduled;\n    # - or _fut_waiter is some Future, and _step() is *not* scheduled.\n    #\n    # The only transition from the latter to the former is through\n    # _wakeup().  When _fut_waiter is not None, one of its callbacks\n    # must be _wakeup().\n\n    # If False, don't log a message if the task is destroyed whereas its\n    # status is still pending\n    _log_destroy_pending = True\n\n    def __init__(self, coro, *, loop=None, name=None):\n        super().__init__(loop=loop)\n        if self._source_traceback:\n            del self._source_traceback[-1]\n        if not coroutines.iscoroutine(coro):\n            # raise after Future.__init__(), attrs are required for __del__\n            # prevent logging for pending task in __del__\n            self._log_destroy_pending = False\n            raise TypeError(f\"a coroutine was expected, got {coro!r}\")\n\n        if name is None:\n            self._name = f'Task-{_task_name_counter()}'\n        else:\n            self._name = str(name)\n\n        self._must_cancel = False\n        self._fut_waiter = None\n        self._coro = coro\n        self._context = contextvars.copy_context()\n\n        self._loop.call_soon(self.__step, context=self._context)\n        _register_task(self)\n\n    def __del__(self):\n        if self._state == futures._PENDING and self._log_destroy_pending:\n            context = {\n                'task': self,\n                'message': 'Task was destroyed but it is pending!',\n            }\n            if self._source_traceback:\n                context['source_traceback'] = self._source_traceback\n            self._loop.call_exception_handler(context)\n        super().__del__()\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n    def _repr_info(self):\n        return base_tasks._task_repr_info(self)\n\n    def get_coro(self):\n        return self._coro\n\n    def get_name(self):\n        return self._name\n\n    def set_name(self, value):\n        self._name = str(value)\n\n    def set_result(self, result):\n        raise RuntimeError('Task does not support set_result operation')\n\n    def set_exception(self, exception):\n        raise RuntimeError('Task does not support set_exception operation')\n\n    def get_stack(self, *, limit=None):\n        \"\"\"Return the list of stack frames for this task's coroutine.\n\n        If the coroutine is not done, this returns the stack where it is\n        suspended.  If the coroutine has completed successfully or was\n        cancelled, this returns an empty list.  If the coroutine was\n        terminated by an exception, this returns the list of traceback\n        frames.\n\n        The frames are always ordered from oldest to newest.\n\n        The optional limit gives the maximum number of frames to\n        return; by default all available frames are returned.  Its\n        meaning differs depending on whether a stack or a traceback is\n        returned: the newest frames of a stack are returned, but the\n        oldest frames of a traceback are returned.  (This matches the\n        behavior of the traceback module.)\n\n        For reasons beyond our control, only one stack frame is\n        returned for a suspended coroutine.\n        \"\"\"\n        return base_tasks._task_get_stack(self, limit)\n\n    def print_stack(self, *, limit=None, file=None):\n        \"\"\"Print the stack or traceback for this task's coroutine.\n\n        This produces output similar to that of the traceback module,\n        for the frames retrieved by get_stack().  The limit argument\n        is passed to get_stack().  The file argument is an I/O stream\n        to which the output is written; by default output is written\n        to sys.stderr.\n        \"\"\"\n        return base_tasks._task_print_stack(self, limit, file)\n\n    def cancel(self, msg=None):\n        \"\"\"Request that this task cancel itself.\n\n        This arranges for a CancelledError to be thrown into the\n        wrapped coroutine on the next cycle through the event loop.\n        The coroutine then has a chance to clean up or even deny\n        the request using try/except/finally.\n\n        Unlike Future.cancel, this does not guarantee that the\n        task will be cancelled: the exception might be caught and\n        acted upon, delaying cancellation of the task or preventing\n        cancellation completely.  The task may also return a value or\n        raise a different exception.\n\n        Immediately after this method is called, Task.cancelled() will\n        not return True (unless the task was already cancelled).  A\n        task will be marked as cancelled when the wrapped coroutine\n        terminates with a CancelledError exception (even if cancel()\n        was not called).\n        \"\"\"\n        self._log_traceback = False\n        if self.done():\n            return False\n        if self._fut_waiter is not None:\n            if self._fut_waiter.cancel(msg=msg):\n                # Leave self._fut_waiter; it may be a Task that\n                # catches and ignores the cancellation so we may have\n                # to cancel it again later.\n                return True\n        # It must be the case that self.__step is already scheduled.\n        self._must_cancel = True\n        self._cancel_message = msg\n        return True\n\n    def __step(self, exc=None):\n        if self.done():\n            raise exceptions.InvalidStateError(\n                f'_step(): already done: {self!r}, {exc!r}')\n        if self._must_cancel:\n            if not isinstance(exc, exceptions.CancelledError):\n                exc = self._make_cancelled_error()\n            self._must_cancel = False\n        coro = self._coro\n        self._fut_waiter = None\n\n        _enter_task(self._loop, self)\n        # Call either coro.throw(exc) or coro.send(None).\n        try:\n            if exc is None:\n                # We use the `send` method directly, because coroutines\n                # don't have `__iter__` and `__next__` methods.\n                result = coro.send(None)\n            else:\n                result = coro.throw(exc)\n        except StopIteration as exc:\n            if self._must_cancel:\n                # Task is cancelled right before coro stops.\n                self._must_cancel = False\n                super().cancel(msg=self._cancel_message)\n            else:\n                super().set_result(exc.value)\n        except exceptions.CancelledError as exc:\n            # Save the original exception so we can chain it later.\n            self._cancelled_exc = exc\n            super().cancel()  # I.e., Future.cancel(self).\n        except (KeyboardInterrupt, SystemExit) as exc:\n            super().set_exception(exc)\n            raise\n        except BaseException as exc:\n            super().set_exception(exc)\n        else:\n            blocking = getattr(result, '_asyncio_future_blocking', None)\n            if blocking is not None:\n                # Yielded Future must come from Future.__iter__().\n                if futures._get_loop(result) is not self._loop:\n                    new_exc = RuntimeError(\n                        f'Task {self!r} got Future '\n                        f'{result!r} attached to a different loop')\n                    self._loop.call_soon(\n                        self.__step, new_exc, context=self._context)\n                elif blocking:\n                    if result is self:\n                        new_exc = RuntimeError(\n                            f'Task cannot await on itself: {self!r}')\n                        self._loop.call_soon(\n                            self.__step, new_exc, context=self._context)\n                    else:\n                        result._asyncio_future_blocking = False\n                        result.add_done_callback(\n                            self.__wakeup, context=self._context)\n                        self._fut_waiter = result\n                        if self._must_cancel:\n                            if self._fut_waiter.cancel(\n                                    msg=self._cancel_message):\n                                self._must_cancel = False\n                else:\n                    new_exc = RuntimeError(\n                        f'yield was used instead of yield from '\n                        f'in task {self!r} with {result!r}')\n                    self._loop.call_soon(\n                        self.__step, new_exc, context=self._context)\n\n            elif result is None:\n                # Bare yield relinquishes control for one event loop iteration.\n                self._loop.call_soon(self.__step, context=self._context)\n            elif inspect.isgenerator(result):\n                # Yielding a generator is just wrong.\n                new_exc = RuntimeError(\n                    f'yield was used instead of yield from for '\n                    f'generator in task {self!r} with {result!r}')\n                self._loop.call_soon(\n                    self.__step, new_exc, context=self._context)\n            else:\n                # Yielding something else is an error.\n                new_exc = RuntimeError(f'Task got bad yield: {result!r}')\n                self._loop.call_soon(\n                    self.__step, new_exc, context=self._context)\n        finally:\n            _leave_task(self._loop, self)\n            self = None  # Needed to break cycles when an exception occurs.\n\n    def __wakeup(self, future):\n        try:\n            future.result()\n        except BaseException as exc:\n            # This may also be a cancellation.\n            self.__step(exc)\n        else:\n            # Don't pass the value of `future.result()` explicitly,\n            # as `Future.__iter__` and `Future.__await__` don't need it.\n            # If we call `_step(value, None)` instead of `_step()`,\n            # Python eval loop would use `.send(value)` method call,\n            # instead of `__next__()`, which is slower for futures\n            # that return non-generator iterators from their `__iter__`.\n            self.__step()\n        self = None  # Needed to break cycles when an exception occurs.\n\n\n_PyTask = Task\n\n\ntry:\n    import _asyncio\nexcept ImportError:\n    pass\nelse:\n    # _CTask is needed for tests.\n    Task = _CTask = _asyncio.Task\n\n\ndef create_task(coro, *, name=None):\n    \"\"\"Schedule the execution of a coroutine object in a spawn task.\n\n    Return a Task object.\n    \"\"\"\n    loop = events.get_running_loop()\n    task = loop.create_task(coro)\n    _set_task_name(task, name)\n    return task\n\n\n# wait() and as_completed() similar to those in PEP 3148.\n\nFIRST_COMPLETED = concurrent.futures.FIRST_COMPLETED\nFIRST_EXCEPTION = concurrent.futures.FIRST_EXCEPTION\nALL_COMPLETED = concurrent.futures.ALL_COMPLETED\n\n\nasync def wait(fs, *, timeout=None, return_when=ALL_COMPLETED):\n    \"\"\"Wait for the Futures and coroutines given by fs to complete.\n\n    The fs iterable must not be empty.\n\n    Coroutines will be wrapped in Tasks.\n\n    Returns two sets of Future: (done, pending).\n\n    Usage:\n\n        done, pending = await asyncio.wait(fs)\n\n    Note: This does not raise TimeoutError! Futures that aren't done\n    when the timeout occurs are returned in the second set.\n    \"\"\"\n    if futures.isfuture(fs) or coroutines.iscoroutine(fs):\n        raise TypeError(f\"expect a list of futures, not {type(fs).__name__}\")\n    if not fs:\n        raise ValueError('Set of coroutines/Futures is empty.')\n    if return_when not in (FIRST_COMPLETED, FIRST_EXCEPTION, ALL_COMPLETED):\n        raise ValueError(f'Invalid return_when value: {return_when}')\n\n    loop = events.get_running_loop()\n\n    fs = set(fs)\n\n    if any(coroutines.iscoroutine(f) for f in fs):\n        warnings.warn(\"The explicit passing of coroutine objects to \"\n                      \"asyncio.wait() is deprecated since Python 3.8, and \"\n                      \"scheduled for removal in Python 3.11.\",\n                      DeprecationWarning, stacklevel=2)\n\n    fs = {ensure_future(f, loop=loop) for f in fs}\n\n    return await _wait(fs, timeout, return_when, loop)\n\n\ndef _release_waiter(waiter, *args):\n    if not waiter.done():\n        waiter.set_result(None)\n\n\nasync def wait_for(fut, timeout):\n    \"\"\"Wait for the single Future or coroutine to complete, with timeout.\n\n    Coroutine will be wrapped in Task.\n\n    Returns result of the Future or coroutine.  When a timeout occurs,\n    it cancels the task and raises TimeoutError.  To avoid the task\n    cancellation, wrap it in shield().\n\n    If the wait is cancelled, the task is also cancelled.\n\n    This function is a coroutine.\n    \"\"\"\n    loop = events.get_running_loop()\n\n    if timeout is None:\n        return await fut\n\n    if timeout <= 0:\n        fut = ensure_future(fut, loop=loop)\n\n        if fut.done():\n            return fut.result()\n\n        await _cancel_and_wait(fut, loop=loop)\n        try:\n            return fut.result()\n        except exceptions.CancelledError as exc:\n            raise exceptions.TimeoutError() from exc\n\n    waiter = loop.create_future()\n    timeout_handle = loop.call_later(timeout, _release_waiter, waiter)\n    cb = functools.partial(_release_waiter, waiter)\n\n    fut = ensure_future(fut, loop=loop)\n    fut.add_done_callback(cb)\n\n    try:\n        # wait until the future completes or the timeout\n        try:\n            await waiter\n        except exceptions.CancelledError:\n            if fut.done():\n                return fut.result()\n            else:\n                fut.remove_done_callback(cb)\n                # We must ensure that the task is not running\n                # after wait_for() returns.\n                # See https://bugs.python.org/issue32751\n                await _cancel_and_wait(fut, loop=loop)\n                raise\n\n        if fut.done():\n            return fut.result()\n        else:\n            fut.remove_done_callback(cb)\n            # We must ensure that the task is not running\n            # after wait_for() returns.\n            # See https://bugs.python.org/issue32751\n            await _cancel_and_wait(fut, loop=loop)\n            # In case task cancellation failed with some\n            # exception, we should re-raise it\n            # See https://bugs.python.org/issue40607\n            try:\n                return fut.result()\n            except exceptions.CancelledError as exc:\n                raise exceptions.TimeoutError() from exc\n    finally:\n        timeout_handle.cancel()\n\n\nasync def _wait(fs, timeout, return_when, loop):\n    \"\"\"Internal helper for wait().\n\n    The fs argument must be a collection of Futures.\n    \"\"\"\n    assert fs, 'Set of Futures is empty.'\n    waiter = loop.create_future()\n    timeout_handle = None\n    if timeout is not None:\n        timeout_handle = loop.call_later(timeout, _release_waiter, waiter)\n    counter = len(fs)\n\n    def _on_completion(f):\n        nonlocal counter\n        counter -= 1\n        if (counter <= 0 or\n            return_when == FIRST_COMPLETED or\n            return_when == FIRST_EXCEPTION and (not f.cancelled() and\n                                                f.exception() is not None)):\n            if timeout_handle is not None:\n                timeout_handle.cancel()\n            if not waiter.done():\n                waiter.set_result(None)\n\n    for f in fs:\n        f.add_done_callback(_on_completion)\n\n    try:\n        await waiter\n    finally:\n        if timeout_handle is not None:\n            timeout_handle.cancel()\n        for f in fs:\n            f.remove_done_callback(_on_completion)\n\n    done, pending = set(), set()\n    for f in fs:\n        if f.done():\n            done.add(f)\n        else:\n            pending.add(f)\n    return done, pending\n\n\nasync def _cancel_and_wait(fut, loop):\n    \"\"\"Cancel the *fut* future or task and wait until it completes.\"\"\"\n\n    waiter = loop.create_future()\n    cb = functools.partial(_release_waiter, waiter)\n    fut.add_done_callback(cb)\n\n    try:\n        fut.cancel()\n        # We cannot wait on *fut* directly to make\n        # sure _cancel_and_wait itself is reliably cancellable.\n        await waiter\n    finally:\n        fut.remove_done_callback(cb)\n\n\n# This is *not* a @coroutine!  It is just an iterator (yielding Futures).\ndef as_completed(fs, *, timeout=None):\n    \"\"\"Return an iterator whose values are coroutines.\n\n    When waiting for the yielded coroutines you'll get the results (or\n    exceptions!) of the original Futures (or coroutines), in the order\n    in which and as soon as they complete.\n\n    This differs from PEP 3148; the proper way to use this is:\n\n        for f in as_completed(fs):\n            result = await f  # The 'await' may raise.\n            # Use result.\n\n    If a timeout is specified, the 'await' will raise\n    TimeoutError when the timeout occurs before all Futures are done.\n\n    Note: The futures 'f' are not necessarily members of fs.\n    \"\"\"\n    if futures.isfuture(fs) or coroutines.iscoroutine(fs):\n        raise TypeError(f\"expect an iterable of futures, not {type(fs).__name__}\")\n\n    from .queues import Queue  # Import here to avoid circular import problem.\n    done = Queue()\n\n    loop = events._get_event_loop()\n    todo = {ensure_future(f, loop=loop) for f in set(fs)}\n    timeout_handle = None\n\n    def _on_timeout():\n        for f in todo:\n            f.remove_done_callback(_on_completion)\n            done.put_nowait(None)  # Queue a dummy value for _wait_for_one().\n        todo.clear()  # Can't do todo.remove(f) in the loop.\n\n    def _on_completion(f):\n        if not todo:\n            return  # _on_timeout() was here first.\n        todo.remove(f)\n        done.put_nowait(f)\n        if not todo and timeout_handle is not None:\n            timeout_handle.cancel()\n\n    async def _wait_for_one():\n        f = await done.get()\n        if f is None:\n            # Dummy value from _on_timeout().\n            raise exceptions.TimeoutError\n        return f.result()  # May raise f.exception().\n\n    for f in todo:\n        f.add_done_callback(_on_completion)\n    if todo and timeout is not None:\n        timeout_handle = loop.call_later(timeout, _on_timeout)\n    for _ in range(len(todo)):\n        yield _wait_for_one()\n\n\n@types.coroutine\ndef __sleep0():\n    \"\"\"Skip one event loop run cycle.\n\n    This is a private helper for 'asyncio.sleep()', used\n    when the 'delay' is set to 0.  It uses a bare 'yield'\n    expression (which Task.__step knows how to handle)\n    instead of creating a Future object.\n    \"\"\"\n    yield\n\n\nasync def sleep(delay, result=None):\n    \"\"\"Coroutine that completes after a given time (in seconds).\"\"\"\n    if delay <= 0:\n        await __sleep0()\n        return result\n\n    loop = events.get_running_loop()\n    future = loop.create_future()\n    h = loop.call_later(delay,\n                        futures._set_result_unless_cancelled,\n                        future, result)\n    try:\n        return await future\n    finally:\n        h.cancel()\n\n\ndef ensure_future(coro_or_future, *, loop=None):\n    \"\"\"Wrap a coroutine or an awaitable in a future.\n\n    If the argument is a Future, it is returned directly.\n    \"\"\"\n    return _ensure_future(coro_or_future, loop=loop)\n\n\ndef _ensure_future(coro_or_future, *, loop=None):\n    if futures.isfuture(coro_or_future):\n        if loop is not None and loop is not futures._get_loop(coro_or_future):\n            raise ValueError('The future belongs to a different loop than '\n                            'the one specified as the loop argument')\n        return coro_or_future\n    called_wrap_awaitable = False\n    if not coroutines.iscoroutine(coro_or_future):\n        if inspect.isawaitable(coro_or_future):\n            coro_or_future = _wrap_awaitable(coro_or_future)\n            called_wrap_awaitable = True\n        else:\n            raise TypeError('An asyncio.Future, a coroutine or an awaitable '\n                            'is required')\n\n    if loop is None:\n        loop = events._get_event_loop(stacklevel=4)\n    try:\n        return loop.create_task(coro_or_future)\n    except RuntimeError: \n        if not called_wrap_awaitable:\n            coro_or_future.close()\n        raise\n\n\n@types.coroutine\ndef _wrap_awaitable(awaitable):\n    \"\"\"Helper for asyncio.ensure_future().\n\n    Wraps awaitable (an object with __await__) into a coroutine\n    that will later be wrapped in a Task by ensure_future().\n    \"\"\"\n    return (yield from awaitable.__await__())\n\n_wrap_awaitable._is_coroutine = _is_coroutine\n\n\nclass _GatheringFuture(futures.Future):\n    \"\"\"Helper for gather().\n\n    This overrides cancel() to cancel all the children and act more\n    like Task.cancel(), which doesn't immediately mark itself as\n    cancelled.\n    \"\"\"\n\n    def __init__(self, children, *, loop):\n        assert loop is not None\n        super().__init__(loop=loop)\n        self._children = children\n        self._cancel_requested = False\n\n    def cancel(self, msg=None):\n        if self.done():\n            return False\n        ret = False\n        for child in self._children:\n            if child.cancel(msg=msg):\n                ret = True\n        if ret:\n            # If any child tasks were actually cancelled, we should\n            # propagate the cancellation request regardless of\n            # *return_exceptions* argument.  See issue 32684.\n            self._cancel_requested = True\n        return ret\n\n\ndef gather(*coros_or_futures, return_exceptions=False):\n    \"\"\"Return a future aggregating results from the given coroutines/futures.\n\n    Coroutines will be wrapped in a future and scheduled in the event\n    loop. They will not necessarily be scheduled in the same order as\n    passed in.\n\n    All futures must share the same event loop.  If all the tasks are\n    done successfully, the returned future's result is the list of\n    results (in the order of the original sequence, not necessarily\n    the order of results arrival).  If *return_exceptions* is True,\n    exceptions in the tasks are treated the same as successful\n    results, and gathered in the result list; otherwise, the first\n    raised exception will be immediately propagated to the returned\n    future.\n\n    Cancellation: if the outer Future is cancelled, all children (that\n    have not completed yet) are also cancelled.  If any child is\n    cancelled, this is treated as if it raised CancelledError --\n    the outer Future is *not* cancelled in this case.  (This is to\n    prevent the cancellation of one child to cause other children to\n    be cancelled.)\n\n    If *return_exceptions* is False, cancelling gather() after it\n    has been marked done won't cancel any submitted awaitables.\n    For instance, gather can be marked done after propagating an\n    exception to the caller, therefore, calling ``gather.cancel()``\n    after catching an exception (raised by one of the awaitables) from\n    gather won't cancel any other awaitables.\n    \"\"\"\n    if not coros_or_futures:\n        loop = events._get_event_loop()\n        outer = loop.create_future()\n        outer.set_result([])\n        return outer\n\n    def _done_callback(fut):\n        nonlocal nfinished\n        nfinished += 1\n\n        if outer is None or outer.done():\n            if not fut.cancelled():\n                # Mark exception retrieved.\n                fut.exception()\n            return\n\n        if not return_exceptions:\n            if fut.cancelled():\n                # Check if 'fut' is cancelled first, as\n                # 'fut.exception()' will *raise* a CancelledError\n                # instead of returning it.\n                exc = fut._make_cancelled_error()\n                outer.set_exception(exc)\n                return\n            else:\n                exc = fut.exception()\n                if exc is not None:\n                    outer.set_exception(exc)\n                    return\n\n        if nfinished == nfuts:\n            # All futures are done; create a list of results\n            # and set it to the 'outer' future.\n            results = []\n\n            for fut in children:\n                if fut.cancelled():\n                    # Check if 'fut' is cancelled first, as 'fut.exception()'\n                    # will *raise* a CancelledError instead of returning it.\n                    # Also, since we're adding the exception return value\n                    # to 'results' instead of raising it, don't bother\n                    # setting __context__.  This also lets us preserve\n                    # calling '_make_cancelled_error()' at most once.\n                    res = exceptions.CancelledError(\n                        '' if fut._cancel_message is None else\n                        fut._cancel_message)\n                else:\n                    res = fut.exception()\n                    if res is None:\n                        res = fut.result()\n                results.append(res)\n\n            if outer._cancel_requested:\n                # If gather is being cancelled we must propagate the\n                # cancellation regardless of *return_exceptions* argument.\n                # See issue 32684.\n                exc = fut._make_cancelled_error()\n                outer.set_exception(exc)\n            else:\n                outer.set_result(results)\n\n    arg_to_fut = {}\n    children = []\n    nfuts = 0\n    nfinished = 0\n    loop = None\n    outer = None  # bpo-46672\n    for arg in coros_or_futures:\n        if arg not in arg_to_fut:\n            fut = _ensure_future(arg, loop=loop)\n            if loop is None:\n                loop = futures._get_loop(fut)\n            if fut is not arg:\n                # 'arg' was not a Future, therefore, 'fut' is a new\n                # Future created specifically for 'arg'.  Since the caller\n                # can't control it, disable the \"destroy pending task\"\n                # warning.\n                fut._log_destroy_pending = False\n\n            nfuts += 1\n            arg_to_fut[arg] = fut\n            fut.add_done_callback(_done_callback)\n\n        else:\n            # There's a duplicate Future object in coros_or_futures.\n            fut = arg_to_fut[arg]\n\n        children.append(fut)\n\n    outer = _GatheringFuture(children, loop=loop)\n    return outer\n\n\ndef shield(arg):\n    \"\"\"Wait for a future, shielding it from cancellation.\n\n    The statement\n\n        task = asyncio.create_task(something())\n        res = await shield(task)\n\n    is exactly equivalent to the statement\n\n        res = await something()\n\n    *except* that if the coroutine containing it is cancelled, the\n    task running in something() is not cancelled.  From the POV of\n    something(), the cancellation did not happen.  But its caller is\n    still cancelled, so the yield-from expression still raises\n    CancelledError.  Note: If something() is cancelled by other means\n    this will still cancel shield().\n\n    If you want to completely ignore cancellation (not recommended)\n    you can combine shield() with a try/except clause, as follows:\n\n        task = asyncio.create_task(something())\n        try:\n            res = await shield(task)\n        except CancelledError:\n            res = None\n\n    Save a reference to tasks passed to this function, to avoid\n    a task disappearing mid-execution. The event loop only keeps\n    weak references to tasks. A task that isn't referenced elsewhere\n    may get garbage collected at any time, even before it's done.\n    \"\"\"\n    inner = _ensure_future(arg)\n    if inner.done():\n        # Shortcut.\n        return inner\n    loop = futures._get_loop(inner)\n    outer = loop.create_future()\n\n    def _inner_done_callback(inner):\n        if outer.cancelled():\n            if not inner.cancelled():\n                # Mark inner's result as retrieved.\n                inner.exception()\n            return\n\n        if inner.cancelled():\n            outer.cancel()\n        else:\n            exc = inner.exception()\n            if exc is not None:\n                outer.set_exception(exc)\n            else:\n                outer.set_result(inner.result())\n\n\n    def _outer_done_callback(outer):\n        if not inner.done():\n            inner.remove_done_callback(_inner_done_callback)\n\n    inner.add_done_callback(_inner_done_callback)\n    outer.add_done_callback(_outer_done_callback)\n    return outer\n\n\ndef run_coroutine_threadsafe(coro, loop):\n    \"\"\"Submit a coroutine object to a given event loop.\n\n    Return a concurrent.futures.Future to access the result.\n    \"\"\"\n    if not coroutines.iscoroutine(coro):\n        raise TypeError('A coroutine object is required')\n    future = concurrent.futures.Future()\n\n    def callback():\n        try:\n            futures._chain_future(ensure_future(coro, loop=loop), future)\n        except (SystemExit, KeyboardInterrupt):\n            raise\n        except BaseException as exc:\n            if future.set_running_or_notify_cancel():\n                future.set_exception(exc)\n            raise\n\n    loop.call_soon_threadsafe(callback)\n    return future\n\n\n# WeakSet containing all alive tasks.\n_all_tasks = weakref.WeakSet()\n\n# Dictionary containing tasks that are currently active in\n# all running event loops.  {EventLoop: Task}\n_current_tasks = {}\n\n\ndef _register_task(task):\n    \"\"\"Register a new task in asyncio as executed by loop.\"\"\"\n    _all_tasks.add(task)\n\n\ndef _enter_task(loop, task):\n    current_task = _current_tasks.get(loop)\n    if current_task is not None:\n        raise RuntimeError(f\"Cannot enter into task {task!r} while another \"\n                           f\"task {current_task!r} is being executed.\")\n    _current_tasks[loop] = task\n\n\ndef _leave_task(loop, task):\n    current_task = _current_tasks.get(loop)\n    if current_task is not task:\n        raise RuntimeError(f\"Leaving task {task!r} does not match \"\n                           f\"the current task {current_task!r}.\")\n    del _current_tasks[loop]\n\n\ndef _unregister_task(task):\n    \"\"\"Unregister a task.\"\"\"\n    _all_tasks.discard(task)\n\n\n_py_register_task = _register_task\n_py_unregister_task = _unregister_task\n_py_enter_task = _enter_task\n_py_leave_task = _leave_task\n\n\ntry:\n    from _asyncio import (_register_task, _unregister_task,\n                          _enter_task, _leave_task,\n                          _all_tasks, _current_tasks)\nexcept ImportError:\n    pass\nelse:\n    _c_register_task = _register_task\n    _c_unregister_task = _unregister_task\n    _c_enter_task = _enter_task\n    _c_leave_task = _leave_task\n", 946], "d:\\Program\\Code\\1_Programing_Language\\Python\\Learning\\Fluent_Python\\example2.py": ["import asyncio\nimport time\n\nasync def say_after(delay, what):\n    await asyncio.sleep(delay)\n    print(what)\n\n\nasync def main():\n    print(f\"started at {time.strftime('%X')}\")\n\n    await say_after(1, \"hello\")\n    await say_after(2, \"world\")\n\n    print(f\"finished at {time.strftime('%X')}\")\n\n\nasyncio.run(main())\n", 18], "D:\\Program\\anaconda3\\lib\\asyncio\\futures.py": ["\"\"\"A Future class similar to the one in PEP 3148.\"\"\"\n\n__all__ = (\n    'Future', 'wrap_future', 'isfuture',\n)\n\nimport concurrent.futures\nimport contextvars\nimport logging\nimport sys\nfrom types import GenericAlias\n\nfrom . import base_futures\nfrom . import events\nfrom . import exceptions\nfrom . import format_helpers\n\n\nisfuture = base_futures.isfuture\n\n\n_PENDING = base_futures._PENDING\n_CANCELLED = base_futures._CANCELLED\n_FINISHED = base_futures._FINISHED\n\n\nSTACK_DEBUG = logging.DEBUG - 1  # heavy-duty debugging\n\n\nclass Future:\n    \"\"\"This class is *almost* compatible with concurrent.futures.Future.\n\n    Differences:\n\n    - This class is not thread-safe.\n\n    - result() and exception() do not take a timeout argument and\n      raise an exception when the future isn't done yet.\n\n    - Callbacks registered with add_done_callback() are always called\n      via the event loop's call_soon().\n\n    - This class is not compatible with the wait() and as_completed()\n      methods in the concurrent.futures package.\n\n    (In Python 3.4 or later we may be able to unify the implementations.)\n    \"\"\"\n\n    # Class variables serving as defaults for instance variables.\n    _state = _PENDING\n    _result = None\n    _exception = None\n    _loop = None\n    _source_traceback = None\n    _cancel_message = None\n    # A saved CancelledError for later chaining as an exception context.\n    _cancelled_exc = None\n\n    # This field is used for a dual purpose:\n    # - Its presence is a marker to declare that a class implements\n    #   the Future protocol (i.e. is intended to be duck-type compatible).\n    #   The value must also be not-None, to enable a subclass to declare\n    #   that it is not compatible by setting this to None.\n    # - It is set by __iter__() below so that Task._step() can tell\n    #   the difference between\n    #   `await Future()` or`yield from Future()` (correct) vs.\n    #   `yield Future()` (incorrect).\n    _asyncio_future_blocking = False\n\n    __log_traceback = False\n\n    def __init__(self, *, loop=None):\n        \"\"\"Initialize the future.\n\n        The optional event_loop argument allows explicitly setting the event\n        loop object used by the future. If it's not provided, the future uses\n        the default event loop.\n        \"\"\"\n        if loop is None:\n            self._loop = events._get_event_loop()\n        else:\n            self._loop = loop\n        self._callbacks = []\n        if self._loop.get_debug():\n            self._source_traceback = format_helpers.extract_stack(\n                sys._getframe(1))\n\n    _repr_info = base_futures._future_repr_info\n\n    def __repr__(self):\n        return '<{} {}>'.format(self.__class__.__name__,\n                                ' '.join(self._repr_info()))\n\n    def __del__(self):\n        if not self.__log_traceback:\n            # set_exception() was not called, or result() or exception()\n            # has consumed the exception\n            return\n        exc = self._exception\n        context = {\n            'message':\n                f'{self.__class__.__name__} exception was never retrieved',\n            'exception': exc,\n            'future': self,\n        }\n        if self._source_traceback:\n            context['source_traceback'] = self._source_traceback\n        self._loop.call_exception_handler(context)\n\n    __class_getitem__ = classmethod(GenericAlias)\n\n    @property\n    def _log_traceback(self):\n        return self.__log_traceback\n\n    @_log_traceback.setter\n    def _log_traceback(self, val):\n        if val:\n            raise ValueError('_log_traceback can only be set to False')\n        self.__log_traceback = False\n\n    def get_loop(self):\n        \"\"\"Return the event loop the Future is bound to.\"\"\"\n        loop = self._loop\n        if loop is None:\n            raise RuntimeError(\"Future object is not initialized.\")\n        return loop\n\n    def _make_cancelled_error(self):\n        \"\"\"Create the CancelledError to raise if the Future is cancelled.\n\n        This should only be called once when handling a cancellation since\n        it erases the saved context exception value.\n        \"\"\"\n        if self._cancel_message is None:\n            exc = exceptions.CancelledError()\n        else:\n            exc = exceptions.CancelledError(self._cancel_message)\n        exc.__context__ = self._cancelled_exc\n        # Remove the reference since we don't need this anymore.\n        self._cancelled_exc = None\n        return exc\n\n    def cancel(self, msg=None):\n        \"\"\"Cancel the future and schedule callbacks.\n\n        If the future is already done or cancelled, return False.  Otherwise,\n        change the future's state to cancelled, schedule the callbacks and\n        return True.\n        \"\"\"\n        self.__log_traceback = False\n        if self._state != _PENDING:\n            return False\n        self._state = _CANCELLED\n        self._cancel_message = msg\n        self.__schedule_callbacks()\n        return True\n\n    def __schedule_callbacks(self):\n        \"\"\"Internal: Ask the event loop to call all callbacks.\n\n        The callbacks are scheduled to be called as soon as possible. Also\n        clears the callback list.\n        \"\"\"\n        callbacks = self._callbacks[:]\n        if not callbacks:\n            return\n\n        self._callbacks[:] = []\n        for callback, ctx in callbacks:\n            self._loop.call_soon(callback, self, context=ctx)\n\n    def cancelled(self):\n        \"\"\"Return True if the future was cancelled.\"\"\"\n        return self._state == _CANCELLED\n\n    # Don't implement running(); see http://bugs.python.org/issue18699\n\n    def done(self):\n        \"\"\"Return True if the future is done.\n\n        Done means either that a result / exception are available, or that the\n        future was cancelled.\n        \"\"\"\n        return self._state != _PENDING\n\n    def result(self):\n        \"\"\"Return the result this future represents.\n\n        If the future has been cancelled, raises CancelledError.  If the\n        future's result isn't yet available, raises InvalidStateError.  If\n        the future is done and has an exception set, this exception is raised.\n        \"\"\"\n        if self._state == _CANCELLED:\n            exc = self._make_cancelled_error()\n            raise exc\n        if self._state != _FINISHED:\n            raise exceptions.InvalidStateError('Result is not ready.')\n        self.__log_traceback = False\n        if self._exception is not None:\n            raise self._exception.with_traceback(self._exception_tb)\n        return self._result\n\n    def exception(self):\n        \"\"\"Return the exception that was set on this future.\n\n        The exception (or None if no exception was set) is returned only if\n        the future is done.  If the future has been cancelled, raises\n        CancelledError.  If the future isn't done yet, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state == _CANCELLED:\n            exc = self._make_cancelled_error()\n            raise exc\n        if self._state != _FINISHED:\n            raise exceptions.InvalidStateError('Exception is not set.')\n        self.__log_traceback = False\n        return self._exception\n\n    def add_done_callback(self, fn, *, context=None):\n        \"\"\"Add a callback to be run when the future becomes done.\n\n        The callback is called with a single argument - the future object. If\n        the future is already done when this is called, the callback is\n        scheduled with call_soon.\n        \"\"\"\n        if self._state != _PENDING:\n            self._loop.call_soon(fn, self, context=context)\n        else:\n            if context is None:\n                context = contextvars.copy_context()\n            self._callbacks.append((fn, context))\n\n    # New method not in PEP 3148.\n\n    def remove_done_callback(self, fn):\n        \"\"\"Remove all instances of a callback from the \"call when done\" list.\n\n        Returns the number of callbacks removed.\n        \"\"\"\n        filtered_callbacks = [(f, ctx)\n                              for (f, ctx) in self._callbacks\n                              if f != fn]\n        removed_count = len(self._callbacks) - len(filtered_callbacks)\n        if removed_count:\n            self._callbacks[:] = filtered_callbacks\n        return removed_count\n\n    # So-called internal methods (note: no set_running_or_notify_cancel()).\n\n    def set_result(self, result):\n        \"\"\"Mark the future done and set its result.\n\n        If the future is already done when this method is called, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state != _PENDING:\n            raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\n        self._result = result\n        self._state = _FINISHED\n        self.__schedule_callbacks()\n\n    def set_exception(self, exception):\n        \"\"\"Mark the future done and set an exception.\n\n        If the future is already done when this method is called, raises\n        InvalidStateError.\n        \"\"\"\n        if self._state != _PENDING:\n            raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\n        if isinstance(exception, type):\n            exception = exception()\n        if type(exception) is StopIteration:\n            raise TypeError(\"StopIteration interacts badly with generators \"\n                            \"and cannot be raised into a Future\")\n        self._exception = exception\n        self._exception_tb = exception.__traceback__\n        self._state = _FINISHED\n        self.__schedule_callbacks()\n        self.__log_traceback = True\n\n    def __await__(self):\n        if not self.done():\n            self._asyncio_future_blocking = True\n            yield self  # This tells Task to wait for completion.\n        if not self.done():\n            raise RuntimeError(\"await wasn't used with future\")\n        return self.result()  # May raise too.\n\n    __iter__ = __await__  # make compatible with 'yield from'.\n\n\n# Needed for testing purposes.\n_PyFuture = Future\n\n\ndef _get_loop(fut):\n    # Tries to call Future.get_loop() if it's available.\n    # Otherwise fallbacks to using the old '_loop' property.\n    try:\n        get_loop = fut.get_loop\n    except AttributeError:\n        pass\n    else:\n        return get_loop()\n    return fut._loop\n\n\ndef _set_result_unless_cancelled(fut, result):\n    \"\"\"Helper setting the result only if the future was not cancelled.\"\"\"\n    if fut.cancelled():\n        return\n    fut.set_result(result)\n\n\ndef _convert_future_exc(exc):\n    exc_class = type(exc)\n    if exc_class is concurrent.futures.CancelledError:\n        return exceptions.CancelledError(*exc.args)\n    elif exc_class is concurrent.futures.TimeoutError:\n        return exceptions.TimeoutError(*exc.args)\n    elif exc_class is concurrent.futures.InvalidStateError:\n        return exceptions.InvalidStateError(*exc.args)\n    else:\n        return exc\n\n\ndef _set_concurrent_future_state(concurrent, source):\n    \"\"\"Copy state from a future to a concurrent.futures.Future.\"\"\"\n    assert source.done()\n    if source.cancelled():\n        concurrent.cancel()\n    if not concurrent.set_running_or_notify_cancel():\n        return\n    exception = source.exception()\n    if exception is not None:\n        concurrent.set_exception(_convert_future_exc(exception))\n    else:\n        result = source.result()\n        concurrent.set_result(result)\n\n\ndef _copy_future_state(source, dest):\n    \"\"\"Internal helper to copy state from another Future.\n\n    The other Future may be a concurrent.futures.Future.\n    \"\"\"\n    assert source.done()\n    if dest.cancelled():\n        return\n    assert not dest.done()\n    if source.cancelled():\n        dest.cancel()\n    else:\n        exception = source.exception()\n        if exception is not None:\n            dest.set_exception(_convert_future_exc(exception))\n        else:\n            result = source.result()\n            dest.set_result(result)\n\n\ndef _chain_future(source, destination):\n    \"\"\"Chain two futures so that when one completes, so does the other.\n\n    The result (or exception) of source will be copied to destination.\n    If destination is cancelled, source gets cancelled too.\n    Compatible with both asyncio.Future and concurrent.futures.Future.\n    \"\"\"\n    if not isfuture(source) and not isinstance(source,\n                                               concurrent.futures.Future):\n        raise TypeError('A future is required for source argument')\n    if not isfuture(destination) and not isinstance(destination,\n                                                    concurrent.futures.Future):\n        raise TypeError('A future is required for destination argument')\n    source_loop = _get_loop(source) if isfuture(source) else None\n    dest_loop = _get_loop(destination) if isfuture(destination) else None\n\n    def _set_state(future, other):\n        if isfuture(future):\n            _copy_future_state(other, future)\n        else:\n            _set_concurrent_future_state(future, other)\n\n    def _call_check_cancel(destination):\n        if destination.cancelled():\n            if source_loop is None or source_loop is dest_loop:\n                source.cancel()\n            else:\n                source_loop.call_soon_threadsafe(source.cancel)\n\n    def _call_set_state(source):\n        if (destination.cancelled() and\n                dest_loop is not None and dest_loop.is_closed()):\n            return\n        if dest_loop is None or dest_loop is source_loop:\n            _set_state(destination, source)\n        else:\n            if dest_loop.is_closed():\n                return\n            dest_loop.call_soon_threadsafe(_set_state, destination, source)\n\n    destination.add_done_callback(_call_check_cancel)\n    source.add_done_callback(_call_set_state)\n\n\ndef wrap_future(future, *, loop=None):\n    \"\"\"Wrap concurrent.futures.Future object.\"\"\"\n    if isfuture(future):\n        return future\n    assert isinstance(future, concurrent.futures.Future), \\\n        f'concurrent.futures.Future is expected, got {future!r}'\n    if loop is None:\n        loop = events._get_event_loop()\n    new_future = loop.create_future()\n    _chain_future(future, new_future)\n    return new_future\n\n\ntry:\n    import _asyncio\nexcept ImportError:\n    pass\nelse:\n    # _CFuture is needed for tests.\n    Future = _CFuture = _asyncio.Future\n", 426], "D:\\Program\\anaconda3\\lib\\asyncio\\runners.py": ["__all__ = 'run',\n\nfrom . import coroutines\nfrom . import events\nfrom . import tasks\n\n\ndef run(main, *, debug=None):\n    \"\"\"Execute the coroutine and return the result.\n\n    This function runs the passed coroutine, taking care of\n    managing the asyncio event loop and finalizing asynchronous\n    generators.\n\n    This function cannot be called when another asyncio event loop is\n    running in the same thread.\n\n    If debug is True, the event loop will be run in debug mode.\n\n    This function always creates a new event loop and closes it at the end.\n    It should be used as a main entry point for asyncio programs, and should\n    ideally only be called once.\n\n    Example:\n\n        async def main():\n            await asyncio.sleep(1)\n            print('hello')\n\n        asyncio.run(main())\n    \"\"\"\n    if events._get_running_loop() is not None:\n        raise RuntimeError(\n            \"asyncio.run() cannot be called from a running event loop\")\n\n    if not coroutines.iscoroutine(main):\n        raise ValueError(\"a coroutine was expected, got {!r}\".format(main))\n\n    loop = events.new_event_loop()\n    try:\n        events.set_event_loop(loop)\n        if debug is not None:\n            loop.set_debug(debug)\n        return loop.run_until_complete(main)\n    finally:\n        try:\n            _cancel_all_tasks(loop)\n            loop.run_until_complete(loop.shutdown_asyncgens())\n            loop.run_until_complete(loop.shutdown_default_executor())\n        finally:\n            events.set_event_loop(None)\n            loop.close()\n\n\ndef _cancel_all_tasks(loop):\n    to_cancel = tasks.all_tasks(loop)\n    if not to_cancel:\n        return\n\n    for task in to_cancel:\n        task.cancel()\n\n    loop.run_until_complete(tasks.gather(*to_cancel, return_exceptions=True))\n\n    for task in to_cancel:\n        if task.cancelled():\n            continue\n        if task.exception() is not None:\n            loop.call_exception_handler({\n                'message': 'unhandled exception during asyncio.run() shutdown',\n                'exception': task.exception(),\n                'task': task,\n            })\n", 73]}, "functions": {"iscoroutine (D:\\Program\\anaconda3\\lib\\asyncio\\coroutines.py:177)": ["D:\\Program\\anaconda3\\lib\\asyncio\\coroutines.py", 177], "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:642)": ["D:\\Program\\anaconda3\\lib\\asyncio\\events.py", 642], "_init_event_loop_policy (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:743)": ["D:\\Program\\anaconda3\\lib\\asyncio\\events.py", 743], "get_event_loop_policy (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:751)": ["D:\\Program\\anaconda3\\lib\\asyncio\\events.py", 751], "__init__ (D:\\Program\\anaconda3\\lib\\_weakrefset.py:37)": ["D:\\Program\\anaconda3\\lib\\_weakrefset.py", 37], "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:413)": ["D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py", 413], "check_str (D:\\Program\\anaconda3\\lib\\os.py:741)": ["D:\\Program\\anaconda3\\lib\\os.py", 741], "encodekey (D:\\Program\\anaconda3\\lib\\os.py:747)": ["D:\\Program\\anaconda3\\lib\\os.py", 747], "__getitem__ (D:\\Program\\anaconda3\\lib\\os.py:675)": ["D:\\Program\\anaconda3\\lib\\os.py", 675], "get (D:\\Program\\anaconda3\\lib\\_collections_abc.py:821)": ["D:\\Program\\anaconda3\\lib\\_collections_abc.py", 821], "_is_debug_mode (D:\\Program\\anaconda3\\lib\\asyncio\\coroutines.py:18)": ["D:\\Program\\anaconda3\\lib\\asyncio\\coroutines.py", 18], "is_running (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:692)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 692], "set_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1927)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 1927], "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:391)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 391], "_acquireLock (D:\\Program\\anaconda3\\lib\\logging\\__init__.py:219)": ["D:\\Program\\anaconda3\\lib\\logging\\__init__.py", 219], "disable (D:\\Program\\anaconda3\\lib\\logging\\__init__.py:1307)": ["D:\\Program\\anaconda3\\lib\\logging\\__init__.py", 1307], "getEffectiveLevel (D:\\Program\\anaconda3\\lib\\logging\\__init__.py:1710)": ["D:\\Program\\anaconda3\\lib\\logging\\__init__.py", 1710], "_releaseLock (D:\\Program\\anaconda3\\lib\\logging\\__init__.py:228)": ["D:\\Program\\anaconda3\\lib\\logging\\__init__.py", 228], "isEnabledFor (D:\\Program\\anaconda3\\lib\\logging\\__init__.py:1724)": ["D:\\Program\\anaconda3\\lib\\logging\\__init__.py", 1724], "debug (D:\\Program\\anaconda3\\lib\\logging\\__init__.py:1455)": ["D:\\Program\\anaconda3\\lib\\logging\\__init__.py", 1455], "set_loop (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:434)": ["D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py", 434], "__init__ (D:\\Program\\anaconda3\\lib\\socket.py:220)": ["D:\\Program\\anaconda3\\lib\\socket.py", 220], "__new__ (D:\\Program\\anaconda3\\lib\\enum.py:678)": ["D:\\Program\\anaconda3\\lib\\enum.py", 678], "__call__ (D:\\Program\\anaconda3\\lib\\enum.py:359)": ["D:\\Program\\anaconda3\\lib\\enum.py", 359], "_intenum_converter (D:\\Program\\anaconda3\\lib\\socket.py:99)": ["D:\\Program\\anaconda3\\lib\\socket.py", 99], "family (D:\\Program\\anaconda3\\lib\\socket.py:514)": ["D:\\Program\\anaconda3\\lib\\socket.py", 514], "type (D:\\Program\\anaconda3\\lib\\socket.py:520)": ["D:\\Program\\anaconda3\\lib\\socket.py", 520], "accept (D:\\Program\\anaconda3\\lib\\socket.py:286)": ["D:\\Program\\anaconda3\\lib\\socket.py", 286], "_real_close (D:\\Program\\anaconda3\\lib\\socket.py:494)": ["D:\\Program\\anaconda3\\lib\\socket.py", 494], "close (D:\\Program\\anaconda3\\lib\\socket.py:498)": ["D:\\Program\\anaconda3\\lib\\socket.py", 498], "socketpair (D:\\Program\\anaconda3\\lib\\socket.py:615)": ["D:\\Program\\anaconda3\\lib\\socket.py", 615], "_make_self_pipe (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:765)": ["D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py", 765], "current_thread (D:\\Program\\anaconda3\\lib\\threading.py:1430)": ["D:\\Program\\anaconda3\\lib\\threading.py", 1430], "main_thread (D:\\Program\\anaconda3\\lib\\threading.py:1574)": ["D:\\Program\\anaconda3\\lib\\threading.py", 1574], "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:628)": ["D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py", 628], "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:312)": ["D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py", 312], "new_event_loop (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:682)": ["D:\\Program\\anaconda3\\lib\\asyncio\\events.py", 682], "new_event_loop (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:796)": ["D:\\Program\\anaconda3\\lib\\asyncio\\events.py", 796], "set_event_loop (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:676)": ["D:\\Program\\anaconda3\\lib\\asyncio\\events.py", 676], "set_event_loop (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:791)": ["D:\\Program\\anaconda3\\lib\\asyncio\\events.py", 791], "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:513)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 513], "_check_running (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:582)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 582], "isfuture (D:\\Program\\anaconda3\\lib\\asyncio\\base_futures.py:14)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_futures.py", 14], "get_debug (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1924)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 1924], "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:31)": ["D:\\Program\\anaconda3\\lib\\asyncio\\events.py", 31], "_call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:772)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 772], "call_soon (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:743)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 743], "add (D:\\Program\\anaconda3\\lib\\_weakrefset.py:86)": ["D:\\Program\\anaconda3\\lib\\_weakrefset.py", 86], "create_task (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:431)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 431], "_ensure_future (D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py:618)": ["D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py", 618], "ensure_future (D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py:610)": ["D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py", 610], "_set_coroutine_origin_tracking (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1909)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 1909], "_poll (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:779)": ["D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py", 779], "select (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:437)": ["D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py", 437], "_process_events (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:861)": ["D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py", 861], "time (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:696)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 696], "create_future (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:427)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 427], "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:103)": ["D:\\Program\\anaconda3\\lib\\asyncio\\events.py", 103], "call_at (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:727)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 727], "call_later (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:705)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 705], "sleep (D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py:593)": ["D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py", 593], "say_after (d:\\Program\\Code\\1_Programing_Language\\Python\\Learning\\Fluent_Python\\example2.py:4)": ["d:\\Program\\Code\\1_Programing_Language\\Python\\Learning\\Fluent_Python\\example2.py", 4], "main (d:\\Program\\Code\\1_Programing_Language\\Python\\Learning\\Fluent_Python\\example2.py:9)": ["d:\\Program\\Code\\1_Programing_Language\\Python\\Learning\\Fluent_Python\\example2.py", 9], "_run (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:78)": ["D:\\Program\\anaconda3\\lib\\asyncio\\events.py", 78], "__contains__ (D:\\Program\\anaconda3\\lib\\_weakrefset.py:75)": ["D:\\Program\\anaconda3\\lib\\_weakrefset.py", 75], "_register_with_iocp (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:722)": ["D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py", 722], "_check_closed (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:423)": ["D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py", 423], "__init__ (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:54)": ["D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py", 54], "_register (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:732)": ["D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py", 732], "recv (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:453)": ["D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py", 453], "_loop_self_reading (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:772)": ["D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py", 772], "_run_once (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1829)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 1829], "_set_result_unless_cancelled (D:\\Program\\anaconda3\\lib\\asyncio\\futures.py:309)": ["D:\\Program\\anaconda3\\lib\\asyncio\\futures.py", 309], "_timer_handle_cancelled (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:1824)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 1824], "cancel (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:64)": ["D:\\Program\\anaconda3\\lib\\asyncio\\events.py", 64], "cancel (D:\\Program\\anaconda3\\lib\\asyncio\\events.py:148)": ["D:\\Program\\anaconda3\\lib\\asyncio\\events.py", 148], "_get_loop (D:\\Program\\anaconda3\\lib\\asyncio\\futures.py:297)": ["D:\\Program\\anaconda3\\lib\\asyncio\\futures.py", 297], "stop (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:651)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 651], "_run_until_complete_cb (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:184)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 184], "run_forever (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:589)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 589], "_cancel_overlapped (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:67)": ["D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py", 67], "cancel (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:83)": ["D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py", 83], "_unregister (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:764)": ["D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py", 764], "run_forever (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:317)": ["D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py", 317], "run_until_complete (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:613)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 613], "_remove (D:\\Program\\anaconda3\\lib\\_weakrefset.py:39)": ["D:\\Program\\anaconda3\\lib\\_weakrefset.py", 39], "__len__ (D:\\Program\\anaconda3\\lib\\_weakrefset.py:72)": ["D:\\Program\\anaconda3\\lib\\_weakrefset.py", 72], "__init__ (D:\\Program\\anaconda3\\lib\\_weakrefset.py:17)": ["D:\\Program\\anaconda3\\lib\\_weakrefset.py", 17], "__enter__ (D:\\Program\\anaconda3\\lib\\_weakrefset.py:21)": ["D:\\Program\\anaconda3\\lib\\_weakrefset.py", 21], "_commit_removals (D:\\Program\\anaconda3\\lib\\_weakrefset.py:53)": ["D:\\Program\\anaconda3\\lib\\_weakrefset.py", 53], "__exit__ (D:\\Program\\anaconda3\\lib\\_weakrefset.py:27)": ["D:\\Program\\anaconda3\\lib\\_weakrefset.py", 27], "__iter__ (D:\\Program\\anaconda3\\lib\\_weakrefset.py:63)": ["D:\\Program\\anaconda3\\lib\\_weakrefset.py", 63], "<setcomp> (D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py:61)": ["D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py", 61], "all_tasks (D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py:42)": ["D:\\Program\\anaconda3\\lib\\asyncio\\tasks.py", 42], "_cancel_all_tasks (D:\\Program\\anaconda3\\lib\\asyncio\\runners.py:55)": ["D:\\Program\\anaconda3\\lib\\asyncio\\runners.py", 55], "shutdown_asyncgens (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:535)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 535], "shutdown_default_executor (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:560)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 560], "is_closed (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:682)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 682], "_stop_accept_futures (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:865)": ["D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py", 865], "_close_self_pipe (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:755)": ["D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py", 755], "close (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:842)": ["D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py", 842], "__del__ (D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py:889)": ["D:\\Program\\anaconda3\\lib\\asyncio\\windows_events.py", 889], "close (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:659)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 659], "close (D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py:679)": ["D:\\Program\\anaconda3\\lib\\asyncio\\proactor_events.py", 679], "run (D:\\Program\\anaconda3\\lib\\asyncio\\runners.py:8)": ["D:\\Program\\anaconda3\\lib\\asyncio\\runners.py", 8], "__del__ (D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py:686)": ["D:\\Program\\anaconda3\\lib\\asyncio\\base_events.py", 686], "<module> (d:\\Program\\Code\\1_Programing_Language\\Python\\Learning\\Fluent_Python\\example2.py:1)": ["d:\\Program\\Code\\1_Programing_Language\\Python\\Learning\\Fluent_Python\\example2.py", 1]}}}